[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numerical Solutions of Differential Equations",
    "section": "",
    "text": "Introduction\nThis unit will cover many numerical methods and how they can be implemented in MATLAB. The programming language MATLAB is an incredibly versatile and robust platform used in many industries including engineering, aeronautics, astronautics, manufacturing, telecommunication and many more.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01_Internal.html",
    "href": "01_Internal.html",
    "title": "1  Internal Workings of MATLAB",
    "section": "",
    "text": "1.1 Floating-Point Arithmetic\nSince computers have limited resources, only a finite strict subset \\(\\mathcal{F}\\) of the real numbers can be represented. This set of possible stored values is known as Floating-Point Numbers and these are characterised by properties that are different from those in \\(\\mathbb{R}\\), since any real number \\(x\\) is – in principle – truncated by the computer, giving rise to a new number denoted by \\(fl(x)\\), which does not necessarily coincide with the original number \\(x\\).\nA computer represents a real number \\(x\\) as a floating-point number in \\(\\mathcal{F}\\) as \\[ x = (-1)^s \\times (a_1a_2\\dots a_t) \\times \\beta^E \\tag{1.1}\\] where:\nThe set \\(\\mathcal{F}\\) is therefore fully characterised by the basis \\(\\beta\\), the number of significant digits \\(t\\) and the range of values that \\(E\\) can take which is \\(E \\in (L,U)\\) with \\(L &lt; 0\\) and \\(U &gt; 0\\).\nA computer typically uses binary representation, meaning that the base is \\(\\beta=2\\) with the available digits \\(\\{0,1\\}\\) (also known as bits) and each digit is the coefficient of a power of 2. Available platforms (like MATLAB and Python) typically use the IEEE754 double precision format for \\(\\mathcal{F}\\), which uses 64-bits as follows:\nThis gives a total of 64-bits with \\(L=-1022\\) and \\(U=1023\\). For 32-bit storage, the exponent is at most 7 and the mantissa has 23 digits. Note that 0 does not belong to \\(\\mathcal{F}\\) since it cannot be represented in the form shown in Equation 1.1 and it is therefore handled separately.\nIf a non-zero real number \\(x\\) is replaced by its floating-point representation \\(fl(x)\\in\\mathcal{F}\\), then there will inevitably be a round-off error which is given by \\[\\frac{|x-fl(x)|}{|x|}\\leq \\frac{1}{2}\\varepsilon_M\\] where \\(\\varepsilon_M=\\beta^{1-t}\\) is the machine epsilon and provides the distance between 1 and its closest floating-point number greater than 1. In the binary case, \\(\\varepsilon_M \\approx 2.2204\\cdot 10^{-16}\\) (this can be found using eps(1)). In other words, the number \\(u = \\frac{1}{2}\\varepsilon_M\\) is the maximum relative error that the computer can make while representing a real number by finite arithmetic.\nMoreover, since \\(L\\) and \\(U\\) are finite, one cannot represent numbers whose absolute value is either arbitrarily large or arbitrarily small. Precisely, the smallest and the largest positive real numbers of \\(\\mathcal{F}\\) are given by \\(x_{\\min} =\\beta^{L-1}\\) and \\(x_{\\max} = \\beta^{U}(1-\\beta^{-t})\\), respectively. A positive number smaller than \\(x_{\\min}\\) yields under-flow and a positive number greater than \\(x_{\\max}\\) yields over-flow. The elements in \\(\\mathcal{F}\\) are more dense near \\(x_{\\min}\\), and less dense while approaching \\(x_{\\max}\\). However, the relative distance is small in both cases. The largest and smallest values that can be represented as floating-point numbers in MATLAB can be found using the realmin and realmax commands.\nSince \\(\\mathcal{F}\\) is a strict subset of \\(\\mathbb{R}\\), elementary algebraic operations on floating-point numbers do not inherit all the properties of analogous operations on \\(\\mathbb{R}\\). Precisely, commutativity still holds for addition and multiplication, i.e. \\(fl(x + y) = fl(y + x)\\) and \\(fl(xy) = fl(yx)\\). Associativity is violated whenever a situation of overflow or underflow occurs or, similarly, whenever two numbers with opposite signs but similar absolute values are added, the result may be quite inexact and the situation is referred to as loss of significant digits.\nProperly handling floating point computations can be tricky sometimes and, if not correctly done, may have serious consequences. There are many webpages (and books) collecting examples of different disasters caused by a poor handling of computer arithmetic or a bad algorithmic implementation. See, for instance, see Software Bugs and the Patriot Missile Fail among others.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Internal Workings of MATLAB</span>"
    ]
  },
  {
    "objectID": "01_Internal.html#floating-point-arithmetic",
    "href": "01_Internal.html#floating-point-arithmetic",
    "title": "1  Internal Workings of MATLAB",
    "section": "",
    "text": "\\(s\\in\\{0,1\\}\\) determines the sign of the number;\n\\(\\beta\\geq 2\\) is the base;\n\\(E\\in\\mathbb{Z}\\) is the exponent.\n\\(a_1a_2\\dots a_t\\) is the mantissa (or significand). The mantissa has length \\(t\\) which is the maximum number of digits that can be stored. Each term in the mantissa must satisfy \\(0\\leq a_i\\leq\\beta-1\\) for all \\(i=1,2,\\dots,t\\) and \\(a_1 \\neq 0\\) (to ensure that the same number cannot have different representations). The digits \\(a_1a_2\\dots a_p\\) (with \\(p\\leq t\\)) are often called the \\(p\\) first significant digits of \\(x\\).\n\n\n\n\n1 bit for \\(s\\) (either 0 or 1) to determine the sign;\n11 bits for \\(E\\) (which can be \\(0,1,2,\\dots,10\\));\n52 bits for \\(a_2 a_3 \\dots a_{53}\\) (since \\(a_1\\neq 0\\), it has to be equal to 1).\n\n\n\n\n&gt;&gt; realmin\nans =\n     2.2251e-308\n&gt;&gt; realmax\nans =\n     1.7977e308",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Internal Workings of MATLAB</span>"
    ]
  },
  {
    "objectID": "01_Internal.html#computational-complexity",
    "href": "01_Internal.html#computational-complexity",
    "title": "1  Internal Workings of MATLAB",
    "section": "1.2 Computational Complexity",
    "text": "1.2 Computational Complexity\nThe Computational Complexity of a function can be defined as the relationship between the size of the input and the difficulty of running the function to completion. The size (or at least, an attribute of the size) of the input is usually denoted \\(n\\), for instance, for a 1-D array, \\(n\\) can be its length.\nThe difficulty of a problem can be measured in several ways. One suitable way to describe the difficulty of the problem is to count the number of basic, or Floating-Point Operations, such as additions, subtractions, multiplications, divisions and assignments. Floating-point operations, also called flops, usually measure the speed of a computer, measured as the maximum number of floating-point operations which the computer can execute in one second. Although each basic operation takes a different amount of time, the number of basic operations needed to complete a function is sufficiently related to the running time to be useful, and it is usually easy to count and less dependent on the specific machine (hardware) that is used to perform the computations.\nA common notation for complexity is the Big-O notation (denoted \\(\\mathcal{O}\\)), which establishes the relationship in the growth of the number of basic operations with respect to the size of the input as the input size becomes very large. In general, the basic operations grow in direct response to the increase in the size \\(n\\) of the input and, as \\(n\\) gets large, the highest power dominates. Therefore, only the highest power term is included in Big-O notation; moreover, coefficients are not required to characterise growth and are usually dropped (although this will also depend on the precision of the estimates).\nFormally, a function \\(f\\) behaves as \\(f(x) \\sim \\mathcal{O}\\left(p(x)\\right)\\) as \\(x\\) tends to infinity if \\[\\lim_{x \\to \\infty}\\frac{f(x)}{p(x)}=\\text{constant}.\\] For example, the polynomial \\(f(x)=x^4+2x^2+x+5\\) behaves like \\(x^4\\) as \\(x\\) tends to infinity since this term will be the fastest to grow. This can be written as \\(f(x) \\sim \\mathcal{O}\\left(x^4\\right)\\) as \\(x \\to \\infty\\).\n\n\n\n\n\n\nCouting flops\n\n\n\nLet \\(f:\\mathbb{N} \\to \\mathbb{N}\\) be given by \\[f(n)=\\left( \\sum\\limits_{j=1}^{n} j\\right)^2\\]\nThis function \\(f\\) can be coded as fun in MATLAB as follows:\nfunction [out]=fun(n)\n\nout = 0;\n\nfor i=1:1:n\n\n     for j=1:1:n\n\n          out = out + i*j;\n    \n     end\n  \nend\n\nend\nFor example, \\(f(3)\\) should perform the overall calculation \\[(1 \\times 1)+(1 \\times 2)+(1 \\times 3)+(2 \\times 1)+(2 \\times 2) +( 2 \\times 3) +(3 \\times 1)+(3 \\times 2)+(3 \\times 3),\\] so fun(3) should output out=36.\nThis code requires the following operations:\n\n\\(1+n+2n^2\\) assignments:\n\n1: out=0;\n\\(n\\): i=1:1:n;\n\\(n^2\\): for every i, j=1:1:n;\n\\(n^2\\): for every i, out=out+i*j;\n\n\\(n^2\\) multiplications: i*j;\n\\(n^2\\) additions: out=out+i*j.\n\nTherefore, for any \\(n\\), this code will need \\(4n^2+n+1\\) flops, meaning that the computational complexity is \\(\\mathcal{O}\\left(n^2\\right)\\), i.e. the code runs in polynomial time. It is not uncommon to find algorithms that run in exponential time \\(\\mathcal{O}\\left(c^n\\right)\\), like some recursive algorithms, or in logarithmic time \\(\\mathcal{O}\\left(\\log n\\right)\\).\n\n\nFor more complicated codes, it is important to see where most of the time is spent in a code and how execution can be improved. A rudimentary way of timing can be done by the toc toc:\n&gt;&gt; tic;\n&gt;&gt; Run code or code block\n&gt;&gt; toc;\nThis will produce a simple time in seconds that MATLAB took from tic until toc, so if toc has not been types, then the timer will continue.\nFor more advanced analysis, MATLAB uses a Code Profiler to analyse code which includes run times for each iteration, times a code has been called and a lot more.\n\n\n\n\n\n\nCode Profiling\n\n\n\nSuppose that a code needs to be written that finds the \\({N}^{\\mathrm{th}}\\) Fibonacci number starting the sequence with (1,1). This can be done in two ways:\n\nIteratively by having a self-contained code that generates all the terms of the sequence up to \\(N\\) and displays the last term.\n\nfunction [F]=Fib_Iter(N)\n\nS=ones(1,N);\n\nfor n=3:1:N\n\n     S(n)=S(n-1)+S(n-2);\n\nend\n\nF=S(end);\n\nend\n\nRecursively by have a self-referential code that keeps referring back to itself to generate last term in the sequence from the previous terms.\n\nfunction [F]=Fib_Rec(N)\n\nif N&lt;3\n\n     F=1;\n\nelse\n\n     F=Fib_Rec(N-1)+Fib_Rec(N-2);\n\nend\n\nend\nWhen running these codes for an input of \\(N=10\\), the times are very short, of the order of \\(10^{-5}\\) seconds but as \\(N\\) gets larger, the recursive code starts to take much longer. Suppose the code efficiency is to be analysed for the input \\(N=40\\), this can be done using the profiler as follows:\n&gt;&gt; profile on\n&gt;&gt; Fib_Iter(40);\n&gt;&gt; profile off\n&gt;&gt; profile viewer\nThis will give a full breakdown of how many times every line was run and how much time it took. For Fib_Iter(40), a total of 38 operations were performed, each taking such a short amount of time that it registers as “0 seconds”.\n\nHowever, performing the profiler for Fib_Rec(40) gives a dramatically different answer with the code taking nearly 247 seconds and having to call itself more than 102 million times.\n\nThis is why it is important to profile longer codes to see which parts take the longest time and which loops are the most time consuming.\n\n\n\n\n\n\n\n\nGood Practice\n\n\n\nTo reduce computational time in general, avoid self-referential codes because these tend to grow in usage exponentially. Another important practice is to use in-built MATLAB syntax, like using sum to add elements in a vector rather than manually hard coding it. This is where being familiar with a lot of the MATLAB syntax is important; MATLAB has a lot of built-in codes and syntaxes which can save a lot of time.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Internal Workings of MATLAB</span>"
    ]
  },
  {
    "objectID": "02_LinAlg.html",
    "href": "02_LinAlg.html",
    "title": "2  Linear Algebra",
    "section": "",
    "text": "2.1 Solving Linear Systems of Equations\nThe first part of linear algebra is solving the square linear system \\[A\\boldsymbol{x}=\\boldsymbol{b} \\quad \\text{where} \\quad A \\in \\mathbb{C}^{N \\times N}, \\boldsymbol{x} \\in \\mathbb{C}^N \\quad \\text{and} \\quad \\boldsymbol{b} \\in \\mathbb{C}^N.\\] This is a situation when the LHS forms a system of equations with a vector of unknowns \\(\\boldsymbol{x}\\) and the RHS is known.\nThere are many way in which this can be done, depending on the form of the matrix:",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "02_LinAlg.html#solving-linear-systems-of-equations",
    "href": "02_LinAlg.html#solving-linear-systems-of-equations",
    "title": "2  Linear Algebra",
    "section": "",
    "text": "Simple Example of Linear System\n\n\n\nLet \\(a, b\\) and \\(c\\) be integers such that: - their sum is equal to 20 - \\(a\\) is twice as large as \\(b\\) - \\(b\\) is bigger than \\(c\\) by 10.\nThese three relationships can be written in equation form as: \\[a+b+c=20\\] \\[a=2b\\] \\[b-c=10\\]\nThis can written in matrix form as: \\[\\underbrace{\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & -2 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}}_{A} \\underbrace{\\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}}_{\\boldsymbol{x}} = \\underbrace{\\begin{pmatrix} 20 \\\\ 0 \\\\ 10 \\end{pmatrix}}_{\\boldsymbol{b}}.\\]\n\n\n\n\nDirect Methods:\n\nDirect substitution for diagonal systems\nForward substitution for lower triangular systems\nBackward substitution for upper training systems\nTDMA for tridiagonal systems\nCramer’s Rule and Gaussian Elimination for more general matrix systems\n\nIterative Methods\n\nJacobi:\nGauss-Seidel\n\nIn-built Methods:\n\nBacksklash\n\n\n\n2.1.1 Computational Stability of Linear Systems\nBefore tackling any linear algebra techniques, it is important to understand Computational Stability.\nConsider the linear system \\[A\\boldsymbol{x}=\\boldsymbol{b} \\quad \\text{where} \\quad A \\in \\mathbb{C}^{N \\times N}, \\boldsymbol{x} \\in \\mathbb{C}^N \\quad \\text{and} \\quad \\boldsymbol{b} \\in \\mathbb{C}^N.\\] In real-life applications, the matrix \\(A\\) is usually fully known and often invertible while the vector \\(\\boldsymbol{b}\\) may not be known exactly and its measurement may often include rounding errors. Suppose that the vector \\(\\boldsymbol{b}\\) has a small error \\(\\delta \\boldsymbol{b}\\), then the solution \\(\\boldsymbol{x}\\) will also have a small error \\(\\delta \\boldsymbol{x}\\), meaning that the system will in fact be \\[A (\\boldsymbol{x} + \\delta \\boldsymbol{x}) = \\boldsymbol{b} + \\delta \\boldsymbol{b}. \\tag{2.1}\\] Subtracting \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) form Equation 2.1 gives \\(A \\delta \\boldsymbol{x} = \\delta \\boldsymbol{b}\\), therefore \\(\\delta \\boldsymbol{x}=A^{-1} \\delta \\boldsymbol{b}\\).\nFor \\(p \\in \\mathbb{N}\\), consider the ratio between the \\(p\\)-norm of the error \\(\\| \\delta \\boldsymbol{x} \\|_p\\) and the \\(p\\)-norm of the exact solution \\(\\| \\boldsymbol{x} \\|_p\\): \\[\\begin{align*}\n\\frac{\\| \\delta \\boldsymbol{x} \\|_p}{\\| \\boldsymbol{x} \\|_p} & = \\frac{ \\| A^{-1} \\delta \\boldsymbol{b} \\|_p}{\\| \\boldsymbol{x} \\|_p} && \\quad \\text{since} \\quad \\delta \\boldsymbol{x}=A^{-1} \\delta \\boldsymbol{b}\\\\\n& \\leq \\frac{ \\| A^{-1} \\|_p \\| \\delta \\boldsymbol{b} \\|_p}{\\| \\boldsymbol{x} \\|_p} && \\quad \\text{by the \\emph{Submultiplicative Property}} \\\\\n& = \\frac{ \\| A^{-1} \\|_p \\| \\delta \\boldsymbol{b} \\|_p}{\\| \\boldsymbol{x} \\|_p} \\times \\frac{\\| A \\|_p}{\\| A \\|_p} && \\quad \\text{multiplying by} \\quad 1=\\frac{\\| A \\|_p}{\\| A \\|_p} \\\\\n& =\\| A \\|_p \\| A^{-1} \\|_p \\frac{\\| \\delta \\boldsymbol{b} \\|_p}{\\| A \\|_p \\| \\boldsymbol{x} \\|_p} && \\quad \\text{rearranging} \\\\\n& \\leq \\| A \\|_p \\| A^{-1} \\|_p \\frac{\\| \\delta \\boldsymbol{b} \\|_p}{\\| \\boldsymbol{b} \\|_p} && \\quad \\text{since} \\; \\; \\boldsymbol{b}=A\\boldsymbol{x} \\; \\text{then} \\; \\; \\| \\boldsymbol{b} \\|_p \\leq \\| A \\|_p \\| \\boldsymbol{x} \\|_p \\\\\n&&& \\quad \\text{by the Submultiplicative Property,} \\\\\n&&& \\quad \\text{meaning that}\\; \\; \\frac{1}{\\| \\boldsymbol{b} \\|_p} \\geq \\frac{1}{\\| A \\|_p \\| \\boldsymbol{x} \\|_p}\n\\end{align*}\\]\n\n\n\n\n\n\nNote 2.1: Submultiplicative Property of Matrix Norms\n\n\n\nFor a matrix \\(A\\) and a vector \\(\\boldsymbol{x}\\), \\[\\| A\\boldsymbol{x} \\| \\leq \\| A \\| \\| \\boldsymbol{x} \\|.\\]\n\n\nLet \\(\\kappa_p(A)=\\| A^{-1} \\|_p \\| A \\|_p\\), then \\[\\frac{\\| \\delta \\boldsymbol{x} \\|_p}{\\| \\boldsymbol{x} \\|_p } \\leq \\kappa_p(A) \\frac{\\| \\delta \\boldsymbol{b} \\|_p}{\\| \\boldsymbol{b} \\|_p}\\]\nThe quantity \\(\\kappa_p(A)\\) is called the Condition Number1 and it can be regarded as a measure of how sensitive a matrix is to perturbations, in other words, it gives an indication as to the stability of the matrix system. A problem is Well-Conditioned if the condition number is small, and is Ill-Conditioned if the condition number is large (the terms “small” and “large” are somewhat subjective here and will depend on the context). Bear in mind that in practice, calculating the condition number may be computationally expensive since it requires inverting the matrix \\(A\\).\nThe condition number derived above follows the assumption that the error only occurs in \\(\\boldsymbol{b}\\) which then results in an error in \\(\\boldsymbol{x}\\). If an error \\(\\delta A\\) is also committed in \\(A\\), then for sufficiently small \\(\\delta  A\\), the error bound for the ratio is \\[\\frac{\\| \\delta \\boldsymbol{x} \\|_p}{\\| \\boldsymbol{x} \\|_p} \\leq \\frac{\\kappa_p(A)}{ 1 - \\kappa_p(A) \\frac{\\| \\delta A \\|_p}{\\| A \\|_p}} \\left( \\frac{\\| \\delta \\boldsymbol{b} \\|_p}{\\| \\boldsymbol{b} \\|_p} + \\frac{\\| \\delta A \\|_p}{\\| A \\|_p} \\right).\\]\nAn example for which \\(A\\) is large is a discretisation matrix of a PDE, in this case, the condition number of \\(A\\) can be very large and increases rapidly as the number of mesh points increases. For example, for a PDE with \\(N\\) mesh points in 2-dimensions, the condition number \\(\\kappa_2(A)\\) is of order \\(\\mathcal{O}\\left(N\\right)\\) and it is not uncommon to have \\(N\\) between \\(10^6\\) and \\(10^8\\). In this case, errors in \\(\\boldsymbol{b}\\) may be amplified enormously in the solution process. Thus, if \\(\\kappa_p(A)\\) is large, there may be difficulties in solving the system reliably, a problem which plagues calculations with partial differential equations.\nMoreover, if \\(A\\) is large, then the system \\(A\\boldsymbol{x}= \\boldsymbol{b}\\) may be solved using an iterative method which generate a sequence of approximations \\(\\boldsymbol{x}_n\\) to \\(\\boldsymbol{x}\\) while ensuring that each iteration is easy to perform and that \\(\\boldsymbol{x}_n\\) rapidly tends to \\(\\boldsymbol{x}\\), within a certain tolerance, as \\(n\\) tends to infinity. If \\(\\kappa_p(A)\\) is large, then the number of iterations to reach this tolerance increases rapidly as the size of \\(A\\) increases, often being proportional to \\(\\kappa_p(A)\\) or even to \\(\\kappa_p(A)^2\\). Thus not only do errors in \\(\\boldsymbol{x}\\) accumulate for large \\(\\kappa_p(A)\\), but the number of computation required to find \\(\\boldsymbol{x}\\) increases as well.\nIn MATLAB, the condition number can be calculated using the cond(A,p) command where A is the square matrix in question and p is the chosen norm which can only be equal to 1, 2, inf or 'Fro' (when using the Frobenius norm). Also note that cond(A) without the second argument p produces the condition number with the 2-norm by default.\n\nProperties of the Condition Number\nLet \\(A\\) and \\(B\\) be invertible matrices, \\(p \\in \\mathbb{N}\\) and \\(\\lambda \\in \\mathbb{R}\\). The condition number \\(\\kappa_p\\) has the following properties:\n\n\\(\\kappa_p(A) \\geq 1\\);\n\\(\\kappa_p(A)=1\\) if and only if \\(A\\) is an orthogonal matrix, i.e. \\(A^{-1}=A^{\\mathrm{T}}\\);\n\\(\\kappa_p({A}^{\\mathrm{T}})=\\kappa_p(A^{-1})=\\kappa_p(A)\\);\n\\(\\kappa_p(\\lambda A)=\\kappa_p(A)\\);\n\\(\\kappa_p(AB) \\leq \\kappa_p(A)\\kappa_p(B)\\).",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "02_LinAlg.html#sec-Dir",
    "href": "02_LinAlg.html#sec-Dir",
    "title": "2  Linear Algebra",
    "section": "2.2 Direct Methods",
    "text": "2.2 Direct Methods\nDirect methods can be used to solve matrix systems in a finite number of steps, although these steps could possibly be computationally expensive.\n\n2.2.1 Direct Substitution\nDirect substitution is the simplest direct method and requires the matrix \\(A\\) to be a diagonal with none of the diagonal terms being 0 (otherwise the matrix will not be invertible).\nConsider the matrix system \\(A \\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} a_1 \\\\ & a_2 \\\\ && \\ddots \\\\ &&& a_{N-1} \\\\ &&&& a_{N} \\end{pmatrix}, \\quad \\boldsymbol{x}=\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N-1} \\\\ x_N \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_{N-1} \\\\ b_N \\end{pmatrix}\\] and \\(a_1, a_2, \\dots, a_N \\neq 0\\). Direct substitution involves simple multiplication and division: \\[A\\boldsymbol{x}=\\boldsymbol{b} \\quad \\implies \\quad\\begin{pmatrix} a_1 \\\\ & a_2 \\\\ && \\ddots \\\\ &&& a_{N-1} \\\\ &&&& a_N \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N-1} \\\\ x_{N} \\end{pmatrix}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_{N-1} \\\\ b_N \\end{pmatrix}\\] \\[\\implies \\quad\\begin{matrix} a_1 x_1 = b_1 \\\\ a_2 x_2 = b_2 \\\\ \\vdots \\\\ a_{N-1}x_{N-1}=b_{N-1} \\\\ a_N x_N=b_N \\end{matrix} \\quad \\implies \\quad\\begin{matrix} x_1=\\frac{b_1}{a_1} \\\\ x_2=\\frac{b_2}{a_2} \\\\ \\vdots \\\\ x_{N-1}=\\frac{b_{N-1}}{a_{N-1}} \\\\ x_N=\\frac{b_N}{a_N}. \\end{matrix}\\]\nThe solution can be written explicitly as \\(x_n=\\frac{b_n}{a_n}\\) for all \\(n=1,2,\\dots,N\\). Every step can done independently, meaning that direct substitution lends itself well to parallel computing. In total, direct substitution requires exactly \\(N\\) computations (all being division).\n\n\n\n\n\n\nExample of Direct Substituion\n\n\n\nConsider the system \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}, \\quad \\boldsymbol{x}=\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} 4 \\\\ 2 \\\\ 4 \\end{pmatrix}.\\] Solving the system using direct substitution: \\[A\\boldsymbol{x}=\\boldsymbol{b} \\quad \\implies \\quad\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}=\\begin{pmatrix} 4 \\\\ 2 \\\\ 4 \\end{pmatrix}\\] \\[\\implies \\quad\\begin{pmatrix} x_1 \\\\ 2x_2 \\\\ -x_3 \\end{pmatrix}=\\begin{pmatrix} 4 \\\\ 2 \\\\ 4 \\end{pmatrix} \\quad \\implies \\quad\\begin{matrix} x_1=4 \\\\ x_2=1 \\\\ x_3=-4. \\end{matrix}\\]\n\n\n\n\n2.2.2 Forward/Backward Substitution\nForward/backward substitution require that the matrix \\(A\\) be lower/upper triangular.\nConsider the matrix system \\(A \\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} a_{11} & a_{12} & \\dots & a_{1,N-1} & a_{1N} \\\\ & a_{22} & \\dots & a_{2,N-1} & a_{2N} \\\\ && \\ddots & \\vdots \\\\ &&& a_{N-1,N-1} & a_{N-1,N} \\\\ &&&& a_{NN} \\end{pmatrix}, \\quad \\boldsymbol{x}=\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N-1} \\\\ x_N \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_{N-1} \\\\ b_N \\end{pmatrix}\\] and \\(a_1, a_2, \\dots, a_N \\neq 0\\) (so that the determinant is non-zero). The matrix \\(A\\) is upper triangular in this case and will require backwards substitution: \\[A\\boldsymbol{x}=\\boldsymbol{b} \\quad \\implies \\quad\\begin{pmatrix} a_{11} & a_{12} & \\dots & a_{1,N-1} & a_{1N} \\\\ & a_{22} & \\dots & a_{2,N-1} & a_{2N} \\\\ && \\ddots & \\vdots \\\\ &&& a_{N-1,N-1} & a_{N-1,N} \\\\ &&&& a_{NN} \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N-1} \\\\ x_N \\end{pmatrix}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_{N-1} \\\\ b_N. \\end{pmatrix}\\]\n\\[\\implies \\quad\\begin{matrix}\na_{11}x_1 &+& a_{12}x_2 &+& \\dots  &+& a_{1,N-1} x_{N-1}   &+& a_{1N}x_N    &= b_1     \\\\\n          & & a_{22}x_2 &+& \\dots  &+& a_{2,N-1} x_{N-1}   &+& a_{2N}x_N    &= b_2     \\\\\n          & &           & & \\vdots & &                     & &              &          \\\\\n          & &           & &        & & a_{N-1,N-1} x_{N-1} &+& a_{N-1,N}x_N &= b_{N-1} \\\\\n          & &           & &        & &                     & & a_{NN}x_N    &= b_N\n\\end{matrix}\\]\nBackward substitution involves using the solutions from the later equations to solve the earlier ones, this gives: \\[x_N=\\frac{b_N}{a_{NN}}\\] \\[x_{N-1}=\\frac{b_{N-1}-a_{N-1,N}x_N}{a_{N-1,N-1}}\\] \\[\\vdots\\] \\[x_2=\\frac{b_2-a_{2N}x_N-a_{2,N-1}x_{N-1}-\\dots-a_{23}x_3}{a_{22}}\\] \\[x_1=\\frac{b_1-a_{1N}x_N-a_{1,N-1}x_{N-1}-\\dots-a_{12}x_2}{a_{11}}.\\]\nThis can be written more explicitly as: \\[x_n=\\begin{cases}\n\\frac{1}{a_{nn}}\\left( b_n - \\sum_{i=n+1}^{N}{a_{ni}x_i} \\right) & \\quad \\text{for} \\quad n=1,2,\\dots,N-1 \\\\\n\\frac{b_N}{a_{NN}} & \\quad \\text{for} \\quad n=N.\n\\end{cases}\\] A similar version can be obtained for the forward substitution for lower triangular matrices. For any \\(n=1,2,\\dots,N-1\\), calculating it requires 1 division, \\(N-n\\) multiplications and \\(N-n\\) subtractions. Therefore cumulatively, \\(x_1, x_2, \\dots, x_{N-1}\\) require \\(N\\) divisions, \\(\\frac{1}{2}\\left( N^2-N \\right)\\) multiplications and \\(\\frac{1}{2}\\left( N^2-N \\right)\\) additions with one more division required for \\(x_N\\), meaning that in total, backward (and forward) substitution requires \\(N^2+1\\) computations.\n\n\n\n\n\n\nExample of Backward Substituion\n\n\n\nConsider the system \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & -1 & 4 \\\\ 0 & 0 & -1 \\end{pmatrix}, \\quad \\boldsymbol{x}=\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\\] This problem can be solved by suing backward substitution: \\[A\\boldsymbol{x}=\\boldsymbol{b} \\quad \\implies \\quad\\begin{pmatrix} 1 & 2 & 1 \\\\ 0 & -1 & 4 \\\\ 0 & 0 & -1 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} \\quad \\implies \\quad\\begin{matrix} x_1+2x_2+x_3=1 \\\\ -x_2+4x_3=0 \\\\ -x_3=1 \\end{matrix}\\] \\[-x_2+4x_3=0 \\quad \\underset{x_3=-1}{\\Rightarrow} \\quad -x_2-4=0 \\quad \\implies \\quad x_2=-4\\] \\[x_1+2x_2+x_3=1 \\quad \\underset{x_2=-4, \\; x_3=-1}{\\Rightarrow} \\quad x_1-8-1=1 \\quad \\implies \\quad x_1=10.\\]\n\n\n\n\n2.2.3 TDMA Algorithm\nThe TriDiagonal Matrix Algorithm, abbreviated as TDMA (also called the Thomas Algorithm) was developed by Llewellyn Thomas which solves tridiagonal matrix systems.\nConsider the matrix system \\(A \\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} m_1 & r_1 \\\\ l_2 & m_2 & r_2 \\\\ & \\ddots & \\ddots & \\ddots \\\\ && l_{N-1} & m_{N-1} & r_{N-1} \\\\ &&& l_N & m_N \\end{pmatrix}, \\quad \\boldsymbol{x}=\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{N-1} \\\\ x_N \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_{N-1} \\\\ b_N \\end{pmatrix}.\\] The \\(m\\) terms denote the diagonal elements, \\(l\\) denote subdiagonal elements (left of the diagonal terms) and \\(r\\) denote the superdiagonal elements (right of the diagonal terms). The TDMA algorithm has explicit formulas for the solution using a forward and backwards sweep that convert the tridiagonal system into an upper triangular system as follows: \\[R_n=\n\\begin{cases}\n\\frac{r_1}{m_1} & n=1 \\\\ \\frac{r_n}{m_n-l_n R_{n-1}} & n=2,3,\\dots,N-1\n\\end{cases}\\] \\[B_n=\n\\begin{cases}\n\\frac{b_1}{m_1} & n=1 \\\\ \\frac{b_n-l_n B_{n-1}}{m_n - l_n R_{n-1}} & n=2,3,\\dots,N\n\\end{cases}\\] \\[x_n=\n\\begin{cases}\nB_n-R_n x_{n+1} & n=1,2,3,\\dots,N-1 \\\\ B_N & n=N.\n\\end{cases}\\]\nThe computational complexity can be calculated as follows:\n\n\n\nTerm\n\\(\\times\\)\n\\(+\\)\n\\(\\div\\)\n\n\n\n\n\\(R_1\\)\n0\n0\n1\n\n\n\\(R_2\\)\n1\n1\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(R_{N-1}\\)\n1\n1\n1\n\n\n\\(B_1\\)\n0\n0\n1\n\n\n\\(B_2\\)\n2\n2\n1\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(B_{N-1}\\)\n2\n2\n1\n\n\n\\(B_N\\)\n2\n2\n1\n\n\n\\(x_1\\)\n1\n1\n0\n\n\n\\(x_2\\)\n1\n1\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_{N-1}\\)\n1\n1\n0\n\n\n\nThis gives a total of \\(3N-5\\) computations for \\(R\\), \\(5N-4\\) computations for \\(B\\) and \\(2N-2\\) computations for \\(x\\) giving a total of \\(10N-11\\) computations.\nThere are other similar algorithms to the TDMA algorithm, like those the solve pentadiagonal systems and those where the tridiagonal matrix starts or ends with a full row.\n\n\n2.2.4 Cramer’s Rule\nCramer’s Rule is a method that can be used to solve any system \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) (of course provided that \\(A\\) is non-singular).\nCramer’s rule states that the elements of the vector \\(\\boldsymbol{x}\\) are given by \\[x_n = \\frac{\\text{det}(A_n)}{\\text{det}(A)} \\quad \\text{for all} \\quad n = 1,2,\\dots,N\\] where \\(A_n\\) is the matrix obtained from \\(A\\) by replacing the \\({n}^{\\mathrm{th}}\\) column by \\(\\boldsymbol{b}\\). This method seems very simple to execute thanks to its very simple formula, but in practice, it can be very computationally expensive.\n\n\n\n\n\n\nExample of Cramer’s Rule\n\n\n\nConsider the system \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} 0 & 4 & 7 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} 14 \\\\ 1 \\\\ 7 \\end{pmatrix}.\\] The determinant of \\(A\\) is equal to 7. Using Cramer’s rule, the solution \\(\\boldsymbol{x}={\\left( x_1, \\; x_2, \\; x_3 \\right)}^{\\mathrm{T}}\\) can be calculated as: \\[x_1=\\frac{\\det(A_1)}{\\det(A)}=\\frac{\\det \\begin{pmatrix} 14 & 4 & 7 \\\\ 1 & 0 & 1 \\\\ 7 & 1 & 0 \\end{pmatrix}}{7}=\\frac{21}{7}=3.\\] \\[x_2=\\frac{\\det(A_1)}{\\det(A)}=\\frac{\\det \\begin{pmatrix} 0 & 14 & 7 \\\\ 1 & 1 & 1 \\\\ 0 & 7 & 0 \\end{pmatrix}}{7}=\\frac{49}{7}=7.\\] \\[x_3=\\frac{\\det(A_1)}{\\det(A)}=\\frac{\\det \\begin{pmatrix} 0 & 4 & 14 \\\\ 1 & 0 & 1 \\\\ 0 & 1 & 7 \\end{pmatrix}}{7}=\\frac{-14}{7}=-2.\\]\n\n\nGenerally, for a matrix of size \\(N \\times N\\), the determinant will require \\(\\mathcal{O}\\left(N!\\right)\\) computations (other matrix forms or methods may require fewer, of \\(\\mathcal{O}\\left(N^3\\right)\\) at least). Cramer’s rule requires calculating the determinants of \\(N+1\\) matrices each is size \\(N \\times N\\) and performing \\(N\\) divisions, therefore the computational complexity of Cramer’s rule is \\(\\mathcal{O}\\left(N+(N+1) \\times N!\\right)=\\mathcal{O}\\left(N+(N+1)!\\right)\\). This means that if a machine runs at 1 Gigaflops per second (\\(10^9\\) flops), then a matrix system of size \\(20 \\times 20\\) will require 1620 years to compute.\n\n\n2.2.5 Gaussian Elimination Method\nThe Gaussian Elimination Method is an algorithm that transforms the linear system \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) where \\(A \\in \\mathbb{C}^{N \\times N}\\) and \\(\\boldsymbol{b} \\in \\mathbb{C}^{N}\\) into an equivalent upper triangular system \\(U\\boldsymbol{x}=\\boldsymbol{g}\\) after \\(N-1\\) steps, where \\(U \\in \\mathbb{C}^{N \\times N}\\) is an upper triangular matrix and \\(\\boldsymbol{g} \\in \\mathbb{C}^{N}\\). This uses Elementary Row Operations (swapping rows, multiplying a row by a constant, adding two rows), after which point, the system \\(U\\boldsymbol{x}=\\boldsymbol{g}\\) can solved by the backward substitution. Note that this method is possible when the elementary row operations are performed on both \\(A\\) and \\(\\boldsymbol{b}\\) simultaneously, so if rows \\(i\\) and \\(j\\) are swapped in \\(A\\), the rows \\(i\\) and \\(j\\) must also be swapped in \\(\\boldsymbol{b}\\), simialry for the other operations.\nThe Gaussian elimination method can be performed as follows (the superscripts in brackets will be the step number):\n\n\n\n\n\n\nParallel Example\n\n\n\nThe algorithm will be explained and an example will be done in parallel to explain the steps with the matrix system \\(A\\boldsymbol{x}=\\boldsymbol{b}\\) where \\[A=\\begin{pmatrix} 2 & -1 & 1 \\\\ -1 & 1 & 2 \\\\ 1 & 2 & -1 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}=\\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix}.\\]\n\n\n\nEstablish the starting matrix: If \\(a_{11} \\neq 0\\), then set \\(A^{(1)}=A\\) and \\(\\boldsymbol{b}^{(1)}=\\boldsymbol{b}\\) as \\[A^{(1)}=\\begin{pmatrix} a_{11}^{(1)} & a_{12}^{(1)} & \\dots & a_{1j}^{(1)} & \\dots & a_{1N}^{(1)} \\\\ a_{21}^{(1)} & a_{22}^{(1)} & \\dots & a_{2j}^{(1)} & \\dots & a_{2N}^{(1)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ a_{j1}^{(1)} & a_{j2}^{(1)} & \\dots & a_{jj}^{(1)} & \\dots & a_{jN}^{(1)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ a_{N1}^{(1)} & a_{N2}^{(1)} & \\dots & a_{Nj}^{(1)} & \\dots & a_{NN}^{(1)} \\end{pmatrix} \\in \\mathbb{R}^{N \\times N} \\quad \\text{where} \\quad a_{11}^{(1)} \\neq 0\\] \\[\\text{and} \\quad \\boldsymbol{b}^{(1)}=\\begin{pmatrix} b_1^{(1)} \\\\ b_2^{(1)} \\\\ \\vdots \\\\ b_j^{(1)} \\\\ \\vdots \\\\ b_N^{(1)} \\end{pmatrix}.\\] If \\(a_{11} = 0\\), then swap the first row with any other row whose first term is not zero and the result will be the starting matrix \\(A^{(1)}\\).\n\n\n\n\n\n\n\n\\(A^{(1)}\\)\n\n\n\n\\[A^{(1)}=A=\\begin{pmatrix} 2 & -1 & 1 \\\\ -1 & 1 & 2 \\\\ 1 & 2 & -1 \\end{pmatrix} \\quad \\text{and} \\quad \\boldsymbol{b}^{(1)}=\\boldsymbol{b}=\\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix}.\\]\n\n\n\nForm the multiplier vector: The desired outcome is to have the matrix \\(A\\) be upper triangular, i.e. all the terms below the diagonal should be 0. To achieve this, introduce a vector \\(\\boldsymbol{m}_1\\) of multipliers, whose \\({i}^{\\mathrm{th}}\\) entry is given by \\[ m_{i1}=\\frac{a_{i1}^{(1)}}{a_{11}^{(1)}} \\quad \\text{for all} \\quad i=1,2,\\dots,N,\\] hence the reason why the assumption \\(a_{11}^{(1)} \\neq 0\\) must be imposed. Essentially, the vector \\(\\boldsymbol{m}_1\\) is the first column of \\(A\\) divided the the first element of \\(A\\).\n\n\n\n\n\n\n\n\\(\\boldsymbol{m}_1\\)\n\n\n\n\\[\\boldsymbol{m}_1=\\frac{1}{a_{11}^{(1)}}\\begin{pmatrix} a_{11}^{(1)} \\\\ a_{21}^{(1)} \\\\ a_{31}^{(1)} \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}\\]\n\n\n\nElimination terms in the first column: For \\(j=2,3,\\dots,N\\), multiply row 1 by \\(-m_{j1}\\) and add it to row \\(j\\) to give the new row \\(j\\): \\[\\begin{pmatrix}\na_{11}^{(1)} & a_{12}^{(1)} & \\dots & a_{1j}^{(1)} & \\dots & a_{1N}^{(1)} \\\\\na_{21}^{(1)}-m_{21}a_{11}^{(1)} & a_{22}^{(1)}-m_{21}a_{12}^{(1)} & \\dots & a_{2j}^{(1)}-m_{21}a_{1j}^{(1)} & \\dots & a_{2N}^{(1)}-m_{21}a_{1N}^{(1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{j1}^{(1)}-m_{j1}a_{11}^{(1)} & a_{j2}^{(1)}-m_{j1}a_{12}^{(1)} & \\dots & a_{jj}^{(1)}-m_{j1}a_{1j}^{(1)} & \\dots & a_{jN}^{(1)}-m_{j1}a_{1N}^{(1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\na_{N1}^{(1)}-m_{N1}a_{11}^{(1)} & a_{N2}^{(1)}-m_{N1}a_{12}^{(1)} & \\dots & a_{Nj}^{(1)}-m_{N1}a_{1j}^{(1)} & \\dots & a_{NN}^{(1)}-m_{N1}a_{1N}^{(1)} \\end{pmatrix}.\\]\n\n\n\n\n\n\n\nRow \\(\\boldsymbol{j}\\) Operations\n\n\n\n\\[\\begin{pmatrix} 2 & -1 & 1 \\\\ -1 & 1 & 2 \\\\ 1 & 2 & -1 \\end{pmatrix}\\] \\[\\xrightarrow[r_2 \\to -\\left( -\\frac{1}{2} \\right)r_1+r_2]{} \\begin{pmatrix} 2 & -1 & 1 \\\\ -\\left( -\\frac{1}{2} \\right)(2)-1 & -\\left( -\\frac{1}{2} \\right)(-1)+1 & -\\left( -\\frac{1}{2} \\right)(1)+2 \\\\ 1 & 2 & -1 \\end{pmatrix}\\] \\[=\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 1 & 2 & -1 \\end{pmatrix}\\] \\[\\xrightarrow[r_3 \\to -\\left( \\frac{1}{2} \\right)r_1+r_3]{} \\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ -\\left( \\frac{1}{2} \\right)(2)+1 & -\\left( \\frac{1}{2} \\right)(-1)+2 & -\\left( \\frac{1}{2} \\right)(1)-1 \\end{pmatrix}\\] \\[=\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 0 & \\frac{5}{2} & -\\frac{3}{2} \\end{pmatrix}\\]\n\n\nNotice that by the definition of \\(m_{j1}\\), the first element in every row must be equal to 0, therefore, this set of operation makes all the terms in the first column equal to 0 except the first. Define this new matrix as the second term in the iteration: \\[\\begin{multline*}\n\\begin{pmatrix}\na_{11}^{(1)} & a_{12}^{(1)} & \\dots & a_{1j}^{(1)} & \\dots & a_{1n}^{(1)} \\\\\n0 & a_{22}^{(1)}-m_{21}a_{12}^{(1)} & \\dots & a_{2j}^{(1)}-m_{21}a_{1j}^{(1)} & \\dots & a_{2n}^{(1)}-m_{21}a_{1n}^{(1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n0 & a_{j2}^{(1)}-m_{j1}a_{12}^{(1)} & \\dots & a_{jj}^{(1)}-m_{j1}a_{1j}^{(1)} & \\dots & a_{jn}^{(1)}-m_{j1}a_{1n}^{(1)} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n0 & a_{n2}^{(1)}-m_{n1}a_{12}^{(1)} & \\dots & a_{nj}^{(1)}-m_{n1}a_{1j}^{(1)} & \\dots & a_{nn}^{(1)}-m_{n1}a_{1n}^{(1)} \\end{pmatrix} \\\\ \\implies \\quad \\begin{pmatrix} a_{11}^{(2)} & a_{12}^{(2)} & \\dots & a_{1j}^{(2)} & \\dots & a_{1n}^{(2)} \\\\ a_{21}^{(2)} & a_{22}^{(2)} & \\dots & a_{2j}^{(2)} & \\dots & a_{2n}^{(2)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ a_{j1}^{(2)} & a_{j2}^{(2)} & \\dots & a_{jj}^{(2)} & \\dots & a_{jn}^{(2)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\ a_{n1}^{(2)} & a_{n2}^{(2)} & \\dots & a_{nj}^{(2)} & \\dots & a_{nn}^{(2)} \\end{pmatrix}=A^{(2)}\n\\end{multline*}\\] where for all \\(i,j=2,3,\\dots,N\\) \\[a_{11}^{(2)}=a_{11}^{(1)} \\quad \\text{;} \\quad a_{1i}^{(2)}=a_{1i}^{(1)} \\quad \\text{;} \\quad a_{i1}^{(2)}=0 \\quad \\text{;} \\quad a_{ij}^{(2)}=a_{ij}^{(1)}-m_{i1}a_{1j}^{(1)}\\]\n\n\n\n\n\n\n\\(A^{(2)}\\)\n\n\n\n\\[A^{(2)}=\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 0 & \\frac{5}{2} & -\\frac{3}{2} \\end{pmatrix}\\]\n\n\n\nModification of the right hand side: The vector \\(\\boldsymbol{b}\\) has to also undergo the same operations as \\(A\\), i.e. for \\(j=2,\\dots,N\\), let row \\(j\\) of \\(\\boldsymbol{b}^{(1)}\\) be row 1 multiplied by \\(-m_{j1}\\) plus row \\(j\\) and the final vector is the vector \\(\\boldsymbol{b}^{(2)}\\).\n\n\n\n\n\n\n\n\\(\\boldsymbol{b}^{(1)}\\)\n\n\n\n\\[\\boldsymbol{b}^{(1)}=\\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\xrightarrow[\\begin{matrix} r_2 \\to -\\left( -\\frac{1}{2} \\right)r_1+r_2 \\\\ r_3 \\to -\\left( \\frac{1}{2} \\right)r_1+r_3 \\end{matrix}]{} \\begin{pmatrix} 1 \\\\ -\\left( -\\frac{1}{2} \\right)(1)+1 \\\\ -\\left( \\frac{1}{2} \\right)(1)+2 \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix}=\\boldsymbol{b}^{(2)}.\\]\n\n\n\nMatrix representation of elimination: This whole procedure can be written as \\(A^{(2)}=M^{(1)} A^{(1)}\\) and \\(\\boldsymbol{b}^{(2)}=M^{(1)}\\boldsymbol{b}^{(1)}\\) where \\[M^{(1)}=\\begin{pmatrix} 1 & 0 & 0 & \\dots & 0 \\\\ -m_{21} & 1 & 0 & \\dots & 0 \\\\ -m_{31} & 0 & 1 & \\dots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ -m_{n1} & 0 & 0 & \\dots & 1 \\end{pmatrix}.\\]\n\n\n\n\n\n\n\n\\(M^{(1)}\\)\n\n\n\n\\[M^{(1)}=\\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 1 & 0 \\\\ -\\frac{1}{2} & 0 & 1 \\end{pmatrix}\\] To check: \\[M^{(1)}A^{(1)}=\\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 1 & 0 \\\\ -\\frac{1}{2} & 0 & 1 \\end{pmatrix}\\begin{pmatrix} 2 & -1 & 1 \\\\ -1 & 1 & 2 \\\\ 1 & 2 & -1 \\end{pmatrix}=\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 0 & \\frac{5}{2} & -\\frac{3}{2} \\end{pmatrix}=A^{(2)}\\] \\[M^{(1)}\\boldsymbol{b}^{(1)}=\\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 1 & 0 \\\\ -\\frac{1}{2} & 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix}=\\boldsymbol{b}^{(2)}\\]\n\n\n\nRepeat for other columns: The process must now be repeated for the rest of the rows, specifically, those that have non-zero pivot points, i.e. the first point in a row that is non-zero. This process can be done more simply by generating the \\(M\\) matrices in the same way as before without going through the starting steps. This process should be reapeated until the last row is reached.\n\n\n\n\n\n\n\nMultiplier Matrices\n\n\n\nThe matrix \\(M^{(2)}\\) can be generated in the same way as \\(M^{(1)}\\), so \\[M^{(2)}=\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -5 & 1 \\end{pmatrix}.\\] To check: \\[M^{(2)}A^{(2)}=\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -5 & 1 \\end{pmatrix}\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 0 & \\frac{5}{2} & -\\frac{3}{2} \\end{pmatrix}=\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 0 & 0 & -14 \\end{pmatrix}=A^{(3)}\\] \\[M^{(2)}\\boldsymbol{b}^{(2)}=\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -5 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ -6 \\end{pmatrix}=\\boldsymbol{b}^{(3)}\\]\n\n\n\nSolve using backwards substitution: After repeating for all other columns (a total of \\(N-1\\) times), the final matrix \\(A^{(N)}\\) will be an upper triangular matrix with non-zero terms on the diagonal and the system can then be solved by backwards substitution.\n\n\n\n\n\n\n\nBackwards Substitution\n\n\n\n\\[A^{(1)}\\boldsymbol{x}=\\boldsymbol{b}^{(1)} \\quad \\implies \\quad A^{(2)}\\boldsymbol{x}=\\boldsymbol{b}^{(2)} \\quad \\implies \\quad A^{(3)}\\boldsymbol{x}=\\boldsymbol{b}^{(3)}\\] \\[\\implies \\quad\\begin{pmatrix} 2 & -1 & 1 \\\\ 0 & \\frac{1}{2} & \\frac{5}{2} \\\\ 0 & 0 & -14 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ -6 \\end{pmatrix} \\quad \\implies \\quad\\begin{matrix} 2x_1-x_2+x_3=1 \\\\ \\frac{1}{2}x_2+\\frac{5}{2}x_3=\\frac{3}{2} \\\\ -14x_3=-6 \\end{matrix}\\] \\[\\implies \\quad\\boldsymbol{x}=\\frac{1}{7}\\begin{pmatrix} 5 \\\\ 6 \\\\ 3 \\end{pmatrix}.\\]\n\n\nThe total number of operations in every step is given in the table below (the “steps” here refer to the matrix manipulation step and not exactly to the step numbers of the algorithm):\n\n\n\nStep\nMultiplications\nAdditions\nDivisions\n\n\n\n\n1\n\\((N-1)^2\\)\n\\((N-1)^2\\)\n\\(N-1\\)\n\n\n2\n\\((N-2)^2\\)\n\\((N-2)^2\\)\n\\(N-2\\)\n\n\n3\n\\((N-3)^2\\)\n\\((N-3)^2\\)\n\\(N-3\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(N-2\\)\n\\(4\\)\n\\(4\\)\n\\(2\\)\n\n\n\\(N-1\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\nThis means that the total number of multiplications is \\[1+4+\\dots+(N-3)^2+(N-2)^2+(N-1)^2=\\sum_{n=1}^{N-1}{n^2}=\\frac{N(N-1)(2N-1)}{6},\\] similarly for the additions. Whereas the total number of divisions is \\[1+2+\\dots+(N-3)+(N-2)+(N-1)=\\sum_{n=1}^{N-1}{n}=\\frac{N(N-1)}{2}.\\] Therefore the total number of operations is \\[\\frac{N(N-1)(2N-1)}{6}+\\frac{N(N-1)(2N-1)}{6}+\\frac{N(N-1)}{2}=\\frac{2}{3}N^3-\\frac{1}{2}N^2-\\frac{1}{6}N.\\] This means that for large \\(N\\), the Gaussian elimination algorithm requires \\(\\mathcal{O}\\left(\\frac{2}{3}N^3\\right)\\) operations when \\(A\\) is a non-sparse matrix. This procedure is computationally expensive even for moderate sized matrices, this also assumes that the pivot points are non-zero, or more specifically, that the matrix has non-zero determinant. As an illustration of this computational complexity, if \\(N=10^6\\) (which not atypical), then for a computer with the computing power of 1 Gigaflops per second, an \\(N \\times N\\) system will need 21 years to find a solution. A lot of more modern computational techniques are based on attempting to reduce this computational complexity, either by eliminating terms in some suitable way or chnaging the matrix in a more pallatable form.\nOverall, every step of this process can be represented by a matrix transformation \\(M^{(n)}\\). This means that in order to convert the matrix \\(A\\) into an upper triangular matrix \\(U\\), the matrix transformations \\(M^{(1)}, M^{(2)}, \\dots, M^{(N-1)}\\) have to be applied reverse order as \\[U=M^{(N-1)} M^{(N-2)} \\dots M^{(1)} A.\\] This can be written as \\[U=MA \\quad \\text{where} \\quad M=M^{(N-1)} M^{(N-2)} \\dots M^{(1)}. \\tag{2.2}\\]\nNotice that every matrix \\(M^{(n)}\\) is lower triangular and this fact will be used later on in ?sec-LU.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "02_LinAlg.html#iterative-methods",
    "href": "02_LinAlg.html#iterative-methods",
    "title": "2  Linear Algebra",
    "section": "2.3 Iterative Methods",
    "text": "2.3 Iterative Methods\nFor a large matrix \\(A\\), solving the system \\(A\\boldsymbol{x} = \\boldsymbol{b}\\) directly can be computationally restrictive as seen in the different methods shown in Section 2.2. An alternative would be to use iterative methods which generate a sequence of approximations \\(\\boldsymbol{x}_k\\) to the exact solution \\(\\boldsymbol{x}\\). The hope is that the iterative method converges to the exact solution, i.e. \\[\\lim_{k \\to \\infty}\\boldsymbol{x}^{(k)}=\\boldsymbol{x}.\\]\nA possible strategy to realise this process is to consider the following recursive definition \\[\\boldsymbol{x}^{(k+1)} = B\\boldsymbol{x}^{(k)}+\\boldsymbol{g} \\quad \\text{for} \\quad k\\geq 0,\\] where \\(B\\) is a suitable matrix called the Iteration Matrix (which would generally depend on \\(A\\)) and \\(\\boldsymbol{g}\\) is a suitable vector (depending on \\(A\\) and \\(\\boldsymbol{b}\\)). Since the iterations \\(\\boldsymbol{x}^{(k)}\\) must tend to \\(\\boldsymbol{x}\\) as \\(k\\) tends to infinity, then \\[\\boldsymbol{x}^{(k+1)} = B\\boldsymbol{x}^{(k)}+\\boldsymbol{g} \\tag{2.3}\\] \\[\\quad \\underset{k \\to \\infty}{\\Rightarrow} \\quad \\boldsymbol{x}=B\\boldsymbol{x}+\\boldsymbol{g}.\\]{#eqn-Bxg} Next, a sufficient condition needs to be derived; define \\(\\boldsymbol{e}^{(k)}\\) as the error incurred from iteration \\(k\\), i.e. \\(\\boldsymbol{e}^{(k)} := \\boldsymbol{x} - \\boldsymbol{x}^{(k)}\\) and consider the linear systems \\[\\boldsymbol{x} = B\\boldsymbol{x}+\\boldsymbol{g} \\quad \\text{and} \\quad \\boldsymbol{x}^{(k+1)} = B\\boldsymbol{x}^{(k)}+\\boldsymbol{g}.\\] Subtracting these gives \\[\\begin{align*}\n& \\quad \\boldsymbol{x}-\\boldsymbol{x}^{(k+1)} = \\left( B\\boldsymbol{x}+\\boldsymbol{g} \\right)-\\left( B\\boldsymbol{x}^{(k)}+\\boldsymbol{g} \\right) \\\\\n\\implies & \\quad \\boldsymbol{x}-\\boldsymbol{x}^{(k+1)} = B\\left( \\boldsymbol{x}-\\boldsymbol{x}^{(k)} \\right) \\\\\n\\implies & \\quad \\boldsymbol{e}^{(k+1)}=B\\boldsymbol{e}^{(k)}.\n\\end{align*}\\]\nFor the sake of argument, consider the case when the matrix \\(B\\) is symmetric. In order to find a bound for the error, take the 2-norm of the error equation \\[\\boldsymbol{e}^{(k+1)}=B\\boldsymbol{e}^{(k)} \\quad \\underset{\\| \\cdot \\|_2}{\\Rightarrow} \\quad \\|\\boldsymbol{e}^{(k+1)}\\|_2 = \\|B\\boldsymbol{e}^{(k)}\\|_2.\\] By the submultiplicative property of matrix norms given in Note 2.1 and the fact that the 2-norm of a symmetric matrix is equal to the spectral radius (largest eigenvalue in absolute terms), the error \\(\\| \\boldsymbol{e}^{(k+1)} \\|\\) can be bounded above \\[\\|\\boldsymbol{e}^{(k+1)}\\|_2 = \\|B\\boldsymbol{e}^{(k)}\\|_2 \\leq \\|B\\| \\| \\boldsymbol{e}^{(k)} \\|_2 =\\rho(B)\\|\\boldsymbol{e}^{(k)}\\|_2.\\] This can be iterated backwards, so for \\(k \\geq 0\\), \\[\\|\\boldsymbol{e}^{(k+1)}\\|_2 \\leq \\rho(B) \\|\\boldsymbol{e}^{(k)}\\|_2 \\leq \\rho(B)^2 \\|\\boldsymbol{e}^{(k-1)}\\|_2 \\leq \\dots \\leq \\rho(B)^{k+1} \\|\\boldsymbol{e}^{(0)}\\|_2.\\] Generally, this means that the error at any iteration \\(k\\) can be bounded above by the error at the initial iteration \\(\\boldsymbol{e}^{(0)}\\). Therefore, since \\(\\boldsymbol{e}^{(0)}\\) is arbitrary, if \\(\\rho(B)&lt;1\\) then the set of vectors \\(\\left\\{ \\boldsymbol{x}^{(k)} \\right\\}_{k \\in \\mathbb{N}}\\) generated by the iterative scheme \\(\\boldsymbol{x}^{(k+1)}=B\\boldsymbol{x}^{(k)}+\\boldsymbol{g}\\) will converge to the exact solution \\(\\boldsymbol{x}\\) which solves \\(A\\boldsymbol{x}=\\boldsymbol{b}\\), hence giving a sufficient condition for convergence.\n\n2.3.1 Constructing an Iterative Method\nA general technique to devise an iterative method to solve \\(A \\boldsymbol{x}=\\boldsymbol{b}\\) is based on a “splitting” of the matrix \\(A\\). First, write the matrix \\(A\\) as \\(A = P-(P-A)\\) where \\(P\\) is a suitable non-singular matrix (somehow linked to \\(A\\) and “easy” to invert). Then \\[\\begin{align*}\nP\\boldsymbol{x} & =\\left[ A+(P-A) \\right]\\boldsymbol{x} && \\quad \\text{since $P=A+P-A$}\\\\\n& =(P-A)\\boldsymbol{x}+A\\boldsymbol{x} && \\quad \\text{expanding} \\\\\n& =(P-A)\\boldsymbol{x}+\\boldsymbol{b} && \\quad \\text{since $A\\boldsymbol{x}=\\boldsymbol{b}$}\n\\end{align*}\\]\nTherefore, the vector \\(\\boldsymbol{x}\\) can be written implicitly as \\[\\boldsymbol{x}=P^{-1}(P-A)\\boldsymbol{x}+P^{-1}\\boldsymbol{b}\\] which is of the form given in ?eq-Bxg where \\(B=P^{-1}(P-A)=I-P^{-1}A\\) and \\(\\boldsymbol{g}=P^{-1}\\boldsymbol{b}\\). It would then stand to reason that if the iterative procedure was of the form \\[\\boldsymbol{x}^{(k)}=P^{-1}(P-A)\\boldsymbol{x}^{(k-1)}+P^{-1}\\boldsymbol{b}\\] (as in Equation 2.3), then the method should converge to the exact solution (provided a suitable choice for \\(P\\)). Of course, for the iterative procedure, the iteration needs an initial vector to start which will be \\[\\boldsymbol{x}^{(0)}=\\begin{pmatrix} x_1^{(0)} \\\\ x_2^{(0)} \\\\ \\vdots \\\\ x_N^{(0)} \\end{pmatrix}.\\]\nThe choice of the matrix \\(P\\) should depend on \\(A\\) in some way. So suppose that the matrix \\(A\\) is broken down into three parts, \\(A=D+L+U\\) where \\(D\\) is the matrix of the diagonal entries of \\(A\\), \\(L\\) is the strictly lower triangular part or \\(A\\) (i.e. not including the diagonal) and \\(U\\) is the strictly upper triangular part of \\(A=D+L+U\\).\n\n\n\n\n\n\nNote\n\n\n\nFor example \\[\\underbrace{\\begin{pmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{pmatrix}}_{A}=\\underbrace{\\begin{pmatrix} a &  & \\\\  & e &  \\\\  &  & i \\end{pmatrix}}_{D}+\\underbrace{\\begin{pmatrix}  &  &  \\\\ d &  &  \\\\ g & h &  \\end{pmatrix}}_{L}+\\underbrace{\\begin{pmatrix}  & b & c \\\\  &  & f \\\\  &  &  \\end{pmatrix}}_{U}.\\]\n\n\nThe matrix \\(P\\) can be chosen as a combination of \\(D,L\\) and \\(U\\) in some way to generate the matrix \\(B\\). There are many ways in which this can be done, each with their own advantages:\n\nJacobi Method: \\(\\boldsymbol{P=D}\\)\n\nThe matrix \\(P\\) is chosen to be equal to the diagonal part of \\(A\\), then the splitting procedure gives the iteration matrix \\(B=I-D^{-1}A\\) and the iteration itself is \\(\\boldsymbol{x}^{(k+1)} = B\\boldsymbol{x}^{(k)}+D^{-1}\\boldsymbol{b}\\) for \\(k \\geq 0\\), which can be written component-wise as \\[x_i^{(k+1)}=\\frac{1}{a_{ii}}\\left(b_i-\\sum_{\\substack{j=1 \\\\ j\\neq i}}^{N}a_{ij}x_j^{(k)}\\right) \\quad \\text{for all} \\quad i=1,\\dots,N. \\tag{2.4}\\]\nIf \\(A\\) is strictly diagonally dominant by rows2, then the Jacobi method converges (i.e. \\(\\rho(B)&lt;1\\), where \\(B=\\mathcal{I}-D^{-1}A\\)). Note that each component \\(x_i^{(k+1)}\\) of the new vector \\(\\boldsymbol{x}^{(k+1)}\\) is computed independently of the others, meaning that the update is simultaneous which makes this method suitable for parallel programming.\n\nGauss-Seidel Method: \\(\\boldsymbol{P=D+L}\\)\n\nThe matrix \\(P\\) is chosen to be equal to the lower triangular part of \\(A\\), therefore the iteration matrix is given by \\(B = (D + L)^{-1}(D + L - A)\\) and the iteration itself is \\(\\boldsymbol{x}^{(k+1)}=B\\boldsymbol{x}^{(k)}+(D+L)^{-1}\\boldsymbol{b}\\) which can be written component-wise as \\[x_i^{(k+1)}=\\frac{1}{a_{ii}}\\left(b_i-\\sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\\sum_{j=i+1}^{N}a_{ij}x_j^{(k)}\\right) \\quad \\text{for all} \\quad i=1,\\dots,N. \\tag{2.5}\\]\nContrary to Jacobi method, Gauss-Seidel method updates the components in sequential mode.\nThere are many other methods that use splitting like:\n\nRichardson method: \\(P=\\frac{1}{\\omega}\\mathcal{I}\\) where \\(\\mathcal{I}\\) is the identity matrix and \\(\\omega \\neq 0\\)\nDamped Jacobi method: \\(P=\\frac{1}{\\omega}D\\) for some \\(\\omega \\neq 0\\)\nSuccessive over-relaxation method: \\(P=\\frac{1}{\\omega}D+L\\) for some \\(\\omega \\neq 0\\)\nSymmetric successive over-relaxation method: \\(P=\\frac{1}{\\omega(2-\\omega)}(D+\\omega L) D^{-1} (D+\\omega U)\\) for some \\(\\omega \\neq 0,2.\\)\n\n\n\n2.3.2 Computational Cost & Stopping Criteria\nThere are essentially two factors contributing to the effectiveness of an iterative method for \\(A\\boldsymbol{x}=\\boldsymbol{b}\\): the computational cost per iteration and the number of performed iterations. The computational cost per iteration depends on the structure and sparsity of the original matrix \\(A\\) and on the choice of the splitting. For both Jacobi and Gauss-Seidel methods, without further assumptions on \\(A\\), the computational cost per iteration is \\(\\mathcal{O}\\left(N^2\\right)\\). Iterations should be stopped when one or more stopping criteria are satisfied, as will be discussed below. For both Jacobi and Gauss-Seidel methods, the cost of performing \\(k\\) iterations is \\(\\mathcal{O}\\left(kN^2\\right)\\); so as long as \\(k \\ll N\\), these methods are much cheaper than Gaussian elimination.\nIn theory, iterative methods require an infinite number of iterations to converge to the exact solution of a linear system but in practice, aiming at the exact solution is neither reasonable nor necessary. Indeed, what is actually needed is an approximation \\(\\boldsymbol{x}^{(k)}\\) for which the error is guaranteed to be lower than a desired tolerance \\(\\tau&gt;0\\). On the other hand, since the error is itself unknown (as it depends on the exact solution), a suitable a posteriori error estimator is needed which predicts the error starting from quantities that have already been computed. There are two natural estimators one may consider:\n\nThe residual at the \\({k}^{\\mathrm{th}}\\) iteration, i.e. \\(\\boldsymbol{r}^{(k)}=\\boldsymbol{b}-A\\boldsymbol{x}^{(k)}\\). More precisely, an iterative method can be stopped at the first iteration step \\(k=k_{\\min}\\) for which \\[\\|\\boldsymbol{r}^{(k)}\\|\\leq\\tau \\|\\boldsymbol{b}\\|.\\] When the above estimate is satisfied, it is guaranteed that \\[\\frac{\\|\\boldsymbol{e}^{(k)}\\|}{\\|\\boldsymbol{x}\\|}\\leq\\tau \\kappa(A),\\] i.e. the control on the residual is meaningful only for those matrices whose condition number is reasonably small; in this way the relative error will be of the same size as the relative residual.\nThe increment at the \\((k+1)^{\\mathrm{st}}\\) iteration, i.e. \\(\\boldsymbol{\\delta}^{(k)}=\\boldsymbol{x}^{(k+1)}-\\boldsymbol{x}^{(k)}\\). More precisely, iterative method would stop after the first iteration step \\(k=k_{\\min}\\) for which \\[\\|\\boldsymbol{\\delta}^{(k)}\\|\\leq\\tau.\\] If \\(B\\) is symmetric and positive definite, then \\[\\|\\boldsymbol{e}^{(k+1)}\\|=\\|\\boldsymbol{e}^{(k)}+ \\boldsymbol{\\delta}^{(k)}\\|\\leq \\rho(B)\\|\\boldsymbol{e}^{(k)}\\|+\\|\\boldsymbol{\\delta}^{(k)}\\|.\\]\n\nRecalling that \\(\\rho(B)\\) should be less than 1 in order for the iterative method to converge, we deduce \\[\\|\\boldsymbol{e}^{(k)}\\|\\leq\\frac{1}{1-\\rho(B)}\\|\\boldsymbol{\\delta}^{(k)}\\|,\\] i.e. the control on the increment is meaningful only if \\(\\rho(B) \\ll 1\\) since in that case the error will be of the same size as the increment.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "02_LinAlg.html#in-built-matlab-procedures",
    "href": "02_LinAlg.html#in-built-matlab-procedures",
    "title": "2  Linear Algebra",
    "section": "2.4 In-Built MATLAB Procedures",
    "text": "2.4 In-Built MATLAB Procedures\nGiven that MATLAB is well-suited to dealing with matrices, it has a very powerful method of solving linear systems and it is using the Backslash Operator. This is a powerful in-built method that can solve any square linear system regardless of its form. MATLAB does this by first determining the general form of the matrix (sparse, triangular, Hermitian, etc.) before applying the appropriate optimised method.\nFor the linear system \\[A \\boldsymbol{x} = \\boldsymbol{b} \\quad \\text{where} \\quad A \\in \\mathbb{R}^{N \\times N}, \\quad \\boldsymbol{x} \\in \\mathbb{R}^N, \\quad \\boldsymbol{b} \\in \\mathbb{R}^N\\] MATLAB can solve this using the syntax x=A\\b.\n\n\n\n\n\n\nStarting Example\n\n\n\nReturning to the example in the beginning of this section, the matrix system was \\[\\underbrace{\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & -2 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}}_{A} \\underbrace{\\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix}}_{\\boldsymbol{x}} = \\underbrace{\\begin{pmatrix} 20 \\\\ 0 \\\\ 10 \\end{pmatrix}}_{\\boldsymbol{b}}.\\] This can be solved as follows:\n&gt;&gt; A=[1,1,1;1,-2,0;0,1,-1];\n&gt;&gt; b=[20;0;10];\n&gt;&gt; A\\b\nans =\n     15.0000\n      7.5000\n     -2.5000\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe MATLAB website shows the following flowcharts for how A\\b classifies the problem before solving it.\n\n\n\nIf the matrix \\(A\\) is full.\n\n\n\n\n\nIf the matrix \\(A\\) is sparse.",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "02_LinAlg.html#footnotes",
    "href": "02_LinAlg.html#footnotes",
    "title": "2  Linear Algebra",
    "section": "",
    "text": "Note that \\(A^{-1}\\) exists only if \\(A\\) is non-singular, meaning that the condition number number only exists if \\(A\\) is non-singular.↩︎\nA matrix \\(A \\in \\mathbb{R}^{N \\times N}\\) is Diagonally Dominant if every diagonal entry is larger in absolute value than the sum of the absolute value of all the other terms in that row. More formally \\[|a_{ii}|\\geq \\sum_{\\substack{j=1 \\\\ j\\neq i}}^{N}|a_{ij}| \\quad \\text{for all} \\quad i=1,\\dots,N.\\] The matrix is Strictly Diagonally Dominant if the inequality is strict.↩︎",
    "crumbs": [
      "Linear Algebra",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Algebra</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html",
    "href": "07_IVP_Euler.html",
    "title": "3  The Euler Method",
    "section": "",
    "text": "3.1 Steps of the Euler Method\nConsider the IVP \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=f(t,y), \\quad \\text{with} \\quad y(t_0)=y_0 \\quad t \\in [t_0,t_f].\\]\nSince \\(h\\) is assumed to be sufficiently small, then all terms higher order terms, in this case \\(h^2\\) or higher, can be neglected (i.e. \\(h^n \\approx 0\\) for \\(n \\geq 2\\)). Therefore \\[y(t_1) \\approx y(t_0)+h y'(t_0).\\]\nLet \\(Y_1\\) denote the approximated value of the solution at the point \\(t_1\\), i.e. \\(Y_1 \\approx y(t_1)\\), so in this case, \\[Y_1=y_0+h y'(t_0). \\tag{3.1}\\] This determines the value of \\(Y_1\\) which is an approximation to \\(y(t_1)\\).\nThe Euler method needs \\(N\\) steps to complete and every step \\(n \\in \\left\\{ 1,2,\\dots,N \\right\\}\\) requires finding \\(y'(t_{n-1})=f(t_{n-1},y_{n-1})\\) and \\(Y_n=Y_{n-1}+h y'(t_{n-1})\\). Of course, the larger \\(N\\) is, the smaller \\(h\\) becomes, meaning that more steps will be required but the solution will be closer to the exact solution\nNotice that the terms on the right hand side of Equation 3.1 are all known and for this reason, the Euler method is known as an Explicit Method.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#steps-of-the-euler-method",
    "href": "07_IVP_Euler.html#steps-of-the-euler-method",
    "title": "3  The Euler Method",
    "section": "",
    "text": "Parallel Example\n\n\n\nThe steps of the Euler method will be explained theoretically and applied to this IVP in parallel to demonstrate the steps: \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=6-2y \\quad \\text{with} \\quad y(0)=0, \\quad t \\in [0,2].\\] In this case, the function on the RHS is \\(f(t,y)=6-2y\\). Note that this IVP has the exact solution \\[y(t)=3-3\\mathrm{e}^{-2t}.\\]\n\n\n\nDiscretise the interval \\([t_0,t_f]\\) with stepsize \\(h\\) to form the set of points \\[\\left\\{ t_0, t_0+h, t_0+2h, \\dots, t_0+Nh \\right\\}.\\]\n\n\n\n\n\n\n\nInverval Discretisation\n\n\n\nSuppose that the interval \\([0,2]\\) is to be split into \\(5\\) subintervals, then \\(N=5\\) and \\[h=\\frac{t_f-t_0}{N}=\\frac{2-0}{5}=0.4.\\] Therefore the discretised points are \\[\\left\\{ 0.0, 0.4, 0.8, 1.2, 1.6, 2.0 \\right\\}.\\] Note that \\(N\\) denotes the number of subintervals and not the number of points, that would be \\(N+1\\) points since the starting point is \\(0\\).\n\n\n\nAt the starting point \\((t_0,y_0)\\), the gradient is known since \\[y'(t_0)=f(t_0,y_0).\\]\n\n\n\n\n\n\n\nGradient at \\(\\boldsymbol{(t_0,y_0)}\\)\n\n\n\nAt the initial point, \\[y'(t_0)=f(t_0,y_0) \\quad \\implies \\quad y'(0)=f(0,0)=6-2(0)=6.\\] So the starting gradient is \\(6\\).\n\n\n\nThe next step is to find the the value of \\(y\\) at the subsequent time \\(t_1=t_0+h\\). For this purpose, consider the Taylor series expansion of \\(y\\) at \\(t=t_1\\), \\[y(t_1)=y(t_0+h)=y(t_0)+h y'(t_0)+ \\frac{h^2}{2!} y''(t_0)+\\mathcal{O}\\left(h^3\\right).\\]\n\n\n\n\n\n\n\nNote\n\n\n\nThe term \\(\\mathcal{O}\\left(h^3\\right)\\) simply means that the terms after this point have a common factor of \\(h^3\\) and these terms are regarded as higher order terms and can be neglected since they are far smaller than the first terms provided \\(h\\) is small.\n\n\n\n\n\n\n\n\n\n\nCalcuating \\(\\boldsymbol{Y_1}\\)\n\n\n\nThe point \\(Y_1\\) can be calculated as follows: \\[Y_1=y_0+hy'(t_0)=0+(0.4)(6)=2.4.\\] This means that the next point is \\((t_1,Y_1)=(0.4,2.4)\\).\n\n\n\nThis iteration can be continued to find \\(Y_{n+1}\\) (which is the approximate value of \\(y(t_{n+1})\\)) for all \\(n=1, 2, \\dots, N-1\\) \\[Y_{n+1}=Y_n+h y'(t_n) \\quad \\text{where} \\quad y'(t_n)=f(t_n,Y_n).\\]\n\n\n\n\n\n\n\nCalculating \\(\\boldsymbol{Y_n}\\)\n\n\n\nThe values of \\(Y_2, Y_3, Y_4\\) and \\(Y_5\\) can be calculated as follows: \\[Y_2: \\quad y'(t_1)=f(t_1,Y_1) \\quad \\implies \\quad y'(0.4)=f(0.4,2.4)=6-2(2.4)=1.2\\] \\[\\implies \\quad Y_2=Y_1+hy'(t_1)=2.4+(0.4)(1.2)=2.88\\]\n\\[Y_3: \\quad y'(t_2)=f(t_2,Y_2) \\quad \\implies \\quad y'(0.8)=f(0.8,2.88)=6-2(2.88)=0.24\\] \\[\\implies \\quad Y_3=Y_2+hy'(t_2)=2.88+(0.4)(0.24)=2.976\\]\n\\[Y_4: \\quad y'(t_3)=f(t_3,Y_3) \\quad \\implies \\quad y'(1.2)=f(1.2,2.976)=6-2(2.976)=0.048\\] \\[\\implies \\quad Y_4=Y_3+hy'(t_3)=2.976+(0.4)(0.048)=2.9952\\]\n\\[Y_5: \\quad y'(t_4)=f(t_4,Y_4) \\quad \\implies \\quad y'(1.6)=f(1.6,2.9952)=6-2(2.9952)=0.0096\\] \\[\\implies \\quad Y_5=Y_4+hy'(t_4)=2.9952+(0.4)(0.0096)=2.99904\\]\n\n\n\nThe solution to the IVP can now be approximated by the function that passes through the points \\[(t_0,Y_0), \\quad (t_1, Y_1), \\quad \\dots \\quad (t_N,Y_N).\\]\n\n\n\n\n\n\n\nSolution to the IVP\n\n\n\nThe approximate solution to the IVP \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=6-2y \\quad \\text{with} \\quad y(0)=0, \\quad t \\in [0,2]\\] is the function that passes through the points: \\[(0,0), \\quad (0.4,2.4), \\quad (0.8,2.88), \\quad (1.2,2.976), \\quad (1.6,2.9952), \\quad (2,2.99904).\\] This is a good approximation since the exact locations, as per the exact solution are, (to 4 decimal places): \\[(0,0), \\quad (0.4,1.6520), \\quad (0.8,2.3943), \\quad (1.2,2.7278), \\quad (1.6,2.8777), \\quad (2,2.9451)\\] which is not bad for such a coarse interval breakdown.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#accuracy",
    "href": "07_IVP_Euler.html#accuracy",
    "title": "3  The Euler Method",
    "section": "3.2 Accuracy",
    "text": "3.2 Accuracy\nConsider the Taylor series expansion for the function \\(y\\) at the point \\(t_1=t_0+h\\), \\[y(t_1)=y(t_0+h)=y(t_0)+h y'(t_0)+ \\frac{h^2}{2!} y''(t_0)+\\mathcal{O}\\left(h^3\\right).\\] Using Taylor’s Theorem2, this can be written as \\[y(t_1)=y(t_0+h)=y(t_0)+h y'(t_0)+ \\frac{h^2}{2!} y''(\\tau_1)\\] for some point \\(\\tau_1\\) between \\(t_0\\) and \\(t_1\\). The Euler method determines the approximation \\(Y_1\\) to the function \\(y\\) at the point \\(t_1\\), particularly, \\[Y_1=y(t_0)+hy'(t_0) \\approx y(t_1).\\]\nThe Local Truncation Error at the first step, denoted \\(e_1\\), is defined as the absolute difference between the exact and approximated values at the first step, and this is given by \\[e_1=\\left| y(t_1)-Y_1 \\right|=\\frac{h^2}{2!}\\left| y''(\\tau_1) \\right|.\\]\nThis can be done for all the locations to give a list of local truncation errors \\(e_1, e_2, e_3,\\dots,e_N\\). Note that technically, these errors are hypothetical since the exact solution \\(y\\), and thus \\(y(t_n)\\), are not known but these are put as placeholders to establish the full accuracy of the method. In this case, the local truncation error \\(e\\) is said to be of second order since \\(e=\\mathcal{O}\\left(h^2\\right)\\).\nAs the iteration progresses, the errors will accumulate to result in a Global Integration Error denoted \\(E\\). In this case, the global integration error is \\[E=|y(t_f)-Y_N|.\\] The global integration error has to be at most the accumulation of all the local truncation errors, namely \\[E=|y(t_f)-Y_N| \\leq \\underbrace{\\sum_{n=1}^{N}{e_n}}_{\\substack{\\text{sum of all} \\\\ \\text{local truncation} \\\\ \\text{errors}}}= \\sum_{n=1}^{N}{\\frac{h^2}{2!}\\left| y''(\\tau_n) \\right|}=h^2\\sum_{n=1}^{N}{\\frac{1}{2}\\left| y''(\\tau_n) \\right|}.\\]\n\\[\\implies \\quad E \\leq h^2\\sum_{n=1}^{N}{\\frac{1}{2}\\left| y''(\\tau_n) \\right|} \\tag{3.2}\\]\nA bound for the sum needs to be found in order bound the global integration error. To this end, consider the set of the second derivatives in the sum above, i.e. \\[\\left\\{ \\frac{1}{2}\\left| y''(\\tau_1) \\right|, \\frac{1}{2}\\left| y''(\\tau_2) \\right|, \\dots, \\frac{1}{2}\\left| y''(\\tau_n) \\right| \\right\\}.\\]\nSince all these terms take a finite value, then at least one of these terms must be larger than all the rest, this is denoted \\(M\\) and can be written as \\[M=\\max\\left\\{ \\frac{1}{2}\\left| y''(\\tau_1) \\right|, \\frac{1}{2}\\left| y''(\\tau_2) \\right|, \\dots, \\frac{1}{2}\\left| y''(\\tau_n) \\right| \\right\\}.\\]\nThis can also be expressed differently as \\[M=\\max_{\\tau \\in [t_0, t_f]}\\left\\{ \\frac{1}{2}\\left| y''(\\tau) \\right| \\right\\}.\\] Therefore, since \\[\\frac{1}{2}\\left| y''(\\tau_n) \\right| \\leq M \\quad \\text{for all} \\quad n=1,2,\\dots,N\\] then \\[\\sum_{n=1}^{N}{\\frac{1}{2}\\left| y''(\\tau_n) \\right|} \\leq \\sum_{n=1}^{N}{M}=NM.\\] Thus, returning back to the expression for \\(E\\) in Equation 3.2 \\[E \\leq h^2\\sum_{n=1}^{N}{\\frac{1}{2}\\left| y''(\\tau_n) \\right|} \\leq NMh^2=Mh \\cdot (Nh)=Mh(t_f-t_0)=\\mathcal{O}\\left(h\\right).\\] Hence, the global integration error \\(E=\\mathcal{O}\\left(h\\right)\\), this means that the Euler method is a First Order Method. This means that both \\(h\\) and the global integration error behave linearly to one another, so if \\(h\\) is halved, then the global integration error is halved as well.\nIn conclusion, the local truncation error of the Euler method is \\(e=\\mathcal{O}\\left(h^2\\right)\\) while the global integration error \\(E=\\mathcal{O}\\left(h\\right)\\) when \\(h\\) is small.\n\n\n\n\n\n\nDifferent Stepsizes\n\n\n\nReturning to the IVP \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=6-2y \\quad \\text{with} \\quad y(0)=0, \\quad t \\in [0,2].\\]\nThe Euler method can be repeated for different values of \\(h\\) and these can be seen in the figure below.\n\nThe table below shows the global integration error for the different values of \\(h\\):\n\n\n\n\\(h\\)\n\\(E\\)\n\n\n\n\n0.4\n0.05399\n\n\n0.2\n0.03681\n\n\n0.1\n0.02036\n\n\n0.05\n0.01060\n\n\n\nWhen the value of \\(h\\) is halved, the global integration error is approximately halved as well.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#sec-EulerSys",
    "href": "07_IVP_Euler.html#sec-EulerSys",
    "title": "3  The Euler Method",
    "section": "3.3 Set of IVPs",
    "text": "3.3 Set of IVPs\nSO far, the Euler Method has been used to solve a single IVP, however this can be extended to solving a set of linear IVPs.\nConsider the set of \\(K\\) linear IVPs defined on the interval \\([t_0,t_f]\\): \\[\\begin{align*}\n& \\frac{\\mathrm{d} y_1}{\\mathrm{d} t} = a_{11} y_1 + a_{12} y_2 + \\dots + a_{1K} y_K + b_1, & y_1(t_0)=\\tilde{y}_1 \\\\\n& \\frac{\\mathrm{d} y_2}{\\mathrm{d} t} = a_{21} y_1 + a_{22} y_2 + \\dots + a_{2K} y_K + b_2, & y_2(t_0)=\\tilde{y}_2 \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\vdots & \\\\\n& \\frac{\\mathrm{d} y_K}{\\mathrm{d} t} = a_{K1} y_1 + a_{K2} y_2 + \\dots + a_{KK} y_K + b_K, & y_K(t_0)=\\tilde{y}_K \\\\\n\\end{align*}\\] where, for \\(i,j=1, 2, \\dots, K\\), the functions \\(y_i=y_i(t)\\) are unknown, \\(a_{ij}\\) are known constant coefficients and \\(b_i\\) are all known (these can generally depend on \\(t\\)).\nThis set of initial value problems need to be written in matrix form as \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b} \\quad \\text{with} \\quad \\boldsymbol{y}(t_0)=\\boldsymbol{y}_0, \\quad t \\in [t_0,t_f]\\] \\[\\text{where} \\quad \\boldsymbol{y}(t)=\\begin{pmatrix} y_1(t) \\\\ y_2(t) \\\\ \\vdots \\\\ y_K(t) \\end{pmatrix}, \\quad A=\\begin{pmatrix} a_{11} & a_{12} & \\dots & a_{1K} \\\\ a_{21} & a_{22} & \\dots & a_{2K} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{K1} & a_{K2} & \\dots & a_{KK} \\end{pmatrix},\\] \\[\\boldsymbol{b}=\\begin{pmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_K \\end{pmatrix}, \\quad \\boldsymbol{y}_0=\\begin{pmatrix} \\tilde{y}_1 \\\\ \\tilde{y}_2 \\\\ \\vdots \\\\ \\tilde{y}_K \\end{pmatrix}.\\] In this case, \\(\\boldsymbol{y}(t)\\) is the unknown solution vector, \\(A\\) is a matrix of constants, \\(\\boldsymbol{y}_0\\) is the vector of initial values and \\(\\boldsymbol{b}\\) is a vector of known terms (possibly depending on \\(t\\)) and is referred to as the Inhomogeneity or Forcing Term.\nThe Euler iteration would be performed in a similar way as before. First, the interval \\([t_0,t_f]\\) needs to be discretised into \\(N\\) equally spaced subintervals, each of width \\(h\\) to give the set of discrete times \\((t_0, t_1, \\dots, t_N)\\) where \\(t_n=t_0+nh\\) for \\(n=0,1,\\dots,N\\). Let \\(\\boldsymbol{Y}_n\\) be the approximation to the function vector \\(\\boldsymbol{y}\\) at the time \\(t=t_n\\), then \\[\\boldsymbol{Y}_{n+1}=\\boldsymbol{Y}_n+h\\boldsymbol{y}'(t_n) \\quad \\text{where} \\quad \\boldsymbol{y}'(t_n)=A\\boldsymbol{Y_n}+\\boldsymbol{b}_n \\quad \\text{for} \\quad n=0,1,2,\\dots,N-1\\] subject to the initial values \\(\\boldsymbol{Y}_0=\\boldsymbol{y}_0\\). (Note that if the vector \\(\\boldsymbol{b}\\) depends on \\(t\\), then \\(\\boldsymbol{b}_n=\\boldsymbol{b}(t_n)\\).)\n\n\n\n\n\n\nSets of IVPs\n\n\n\nConsider the two coupled IVPs on the interval \\([0,1]\\): \\[\\begin{align*}\n\\frac{\\mathrm{d} y}{\\mathrm{d} t} = y + 2z, & \\quad y(0)=1 \\\\\n\\frac{\\mathrm{d} z}{\\mathrm{d} t} = \\frac{3}{2}y-z, &   \\quad z(0)=0\n\\end{align*}\\]\nBefore attempting to solve this set of IVPs, it needs to be written in matrix form as \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b} \\quad \\text{with} \\quad \\boldsymbol{y}(0)=\\boldsymbol{y}_0.\\] In this case, \\[\\boldsymbol{y}(t)=\\begin{pmatrix}y(t) \\\\ z(t)\\end{pmatrix}, \\quad A=\\begin{pmatrix}1 & 2 \\\\ \\frac{3}{2} & -1\\end{pmatrix}, \\quad \\boldsymbol{b}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\boldsymbol{y}_0=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}.\\]\nLet \\(N=5\\), so \\[h=\\frac{t_f-t_0}{N}=\\frac{1-0}{5}=0.2.\\] The Euler iteration will be \\[\\boldsymbol{Y}_{n+1}=\\boldsymbol{Y}_n+h\\boldsymbol{y}'(t_n) \\quad \\text{where} \\quad \\boldsymbol{y}'(t_n)=A \\boldsymbol{Y}_n+\\boldsymbol{b}_n \\quad \\text{for} \\quad n=0,1,2,3,4.\\] This can be written as \\[\\boldsymbol{Y}_{n+1}=\\boldsymbol{Y}_n+h\\left[ A \\boldsymbol{Y}_n+\\boldsymbol{b}_n \\right] \\quad \\text{for} \\quad n=0,1,2,3,4\\] keeping in mind that \\(t_n=hn=0.2n\\) the vector \\(\\boldsymbol{b}_n=\\boldsymbol{b}(t_n)=\\boldsymbol{0}\\) and \\(\\boldsymbol{Y}_0=\\boldsymbol{y}_0\\): \\[\\begin{align*}\n    &\\boldsymbol{Y}_1=\\boldsymbol{Y}_0+0.2\\left[ A \\boldsymbol{Y}_0+\\boldsymbol{b}_0 \\right]=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}+0.2\\left[ \\begin{pmatrix}1 & 2 \\\\ \\frac{3}{2} & -1\\end{pmatrix}\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}+\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} \\right]=\\begin{pmatrix}1.2 \\\\ 0.3\\end{pmatrix} \\\\\n    &\\boldsymbol{Y}_2=\\boldsymbol{Y}_1+0.2\\left[ A \\boldsymbol{Y}_1+\\boldsymbol{b}_1 \\right]=\\begin{pmatrix}1.2 \\\\ 0.3\\end{pmatrix}+0.2\\left[ \\begin{pmatrix}1 & 2 \\\\ \\frac{3}{2} & -1\\end{pmatrix}\\begin{pmatrix}1.2 \\\\ 0.3\\end{pmatrix}+\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} \\right]=\\begin{pmatrix}1.56 \\\\ 0.6\\end{pmatrix} \\\\\n    &\\boldsymbol{Y}_3=\\boldsymbol{Y}_2+0.2\\left[ A \\boldsymbol{Y}_2+\\boldsymbol{b}_2 \\right]=\\begin{pmatrix}1.56 \\\\ 0.6\\end{pmatrix}+0.2\\left[ \\begin{pmatrix}1 & 2 \\\\ \\frac{3}{2} & -1\\end{pmatrix}\\begin{pmatrix}1.56 \\\\ 0.6\\end{pmatrix}+\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} \\right]=\\begin{pmatrix}2.112 \\\\ 0.948\\end{pmatrix} \\\\\n    &\\boldsymbol{Y}_4=\\boldsymbol{Y}_3+0.2\\left[ A \\boldsymbol{Y}_3+\\boldsymbol{b}_3 \\right]=\\begin{pmatrix}2.112 \\\\ 0.948\\end{pmatrix}+0.2\\left[ \\begin{pmatrix}1 & 2 \\\\ \\frac{3}{2} & -1\\end{pmatrix}\\begin{pmatrix}2.112 \\\\ 0.948\\end{pmatrix}+\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} \\right]=\\begin{pmatrix}2.9136 \\\\ 1.3920\\end{pmatrix} \\\\\n    &\\boldsymbol{Y}_5=\\boldsymbol{Y}_4+0.2\\left[ A \\boldsymbol{Y}_4+\\boldsymbol{b}_4 \\right]=\\begin{pmatrix}2.9136 \\\\ 1.3920\\end{pmatrix}+0.2\\left[ \\begin{pmatrix}1 & 2 \\\\ \\frac{3}{2} & -1\\end{pmatrix}\\begin{pmatrix}2.9136 \\\\ 1.3920\\end{pmatrix}+\\begin{pmatrix}0 \\\\ 0\\end{pmatrix} \\right]=\\begin{pmatrix}4.0531 \\\\ 1.9877\\end{pmatrix} \\\\\n\\end{align*}\\] \\[\\text{therefore} \\quad y(1)=4.0531, \\quad z(1)=1.9877.\\]",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#higher-order-ivps",
    "href": "07_IVP_Euler.html#higher-order-ivps",
    "title": "3  The Euler Method",
    "section": "3.4 Higher Order IVPs",
    "text": "3.4 Higher Order IVPs\nThe previous sections solved one first order IVP and a set of first order IVPs. What happens if a higher order IVP is to be solved? Or a set of higher order IVPs? The difference will be minimal, subject to a few manipulations first.\nConsider the \\({K}^{\\mathrm{th}}\\) order linear IVP on the interval \\([t_0,t_f]\\) \\[\\frac{\\mathrm{d}^{K} y}{\\mathrm{d} t^{K}}+a_{K-1} \\frac{\\mathrm{d}^{K-1} y}{\\mathrm{d} t^{K-1}} + \\dots + a_2 \\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}+a_1 \\frac{\\mathrm{d} y}{\\mathrm{d} t}+a_0 y=f(t) \\tag{3.3}\\] where \\(a_k \\in \\mathbb{R}\\) and \\(f\\) is a known function. This IVP is to be solved subject to the initial conditions \\[y(t_0)=\\eta_0, \\quad \\frac{\\mathrm{d} y}{\\mathrm{d} t}(t_0)=\\eta_1 \\quad \\dots \\quad \\frac{\\mathrm{d}^{K-1} y}{\\mathrm{d} t^{K-1}}(t_0)=\\eta_{K-1}.\\]\nThis \\({K}^{\\mathrm{th}}\\) order IVP can be written as a set of \\(K\\) first order IVPs. Indeed, let the functions \\(y_k\\) be given by \\[y_1(t)=\\frac{\\mathrm{d} y}{\\mathrm{d} t}\\] \\[y_2(t)=y_1'(t)=\\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}\\] \\[y_3(t)=y_2'(t)=\\frac{\\mathrm{d}^{3} y}{\\mathrm{d} t^{3}}\\] \\[\\vdots\\] \\[y_{K-3}(t)=y_{K-4}'(t)=\\frac{\\mathrm{d}^{K-3} y}{\\mathrm{d} t^{K-3}}\\] \\[y_{K-2}(t)=y_{K-3}'(t)=\\frac{\\mathrm{d}^{K-2} y}{\\mathrm{d} t^{K-2}}\\] \\[y_{K-1}(t)=y_{K-2}'(t)=\\frac{\\mathrm{d}^{K-1} y}{\\mathrm{d} t^{K-1}}\\]\nNotice that \\[\\begin{align*}\n\\frac{\\mathrm{d} y_{K-1}}{\\mathrm{d} t}=\\frac{\\mathrm{d}^{K} y}{\\mathrm{d} t^{K}} & =-a_{K-1} \\frac{\\mathrm{d}^{K-1} y}{\\mathrm{d} t^{K-1}} - \\dots - a_2 \\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}- a_1 \\frac{\\mathrm{d} y}{\\mathrm{d} t} - a_0 y+f(t) \\\\\n& =-a_{K-1} y_{K-1}- \\dots - a_2 y_2 - a_1 y_1 -a_0 y+ f(t)\n\\end{align*}\\] Let \\(\\boldsymbol{y}\\) be the vector of the unknown functions \\(y, y_1, y_2, \\dots, y_{K-1}\\). This means that the IVP in Equation 3.3 can be written in matrix form \\(\\boldsymbol{y}'=A\\boldsymbol{y}+\\boldsymbol{b}\\) as follows: \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=\\frac{\\mathrm{d} }{\\mathrm{d} t}\\begin{pmatrix} y \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{K-3} \\\\ y_{K-2} \\\\ y_{K-1} \\end{pmatrix}=\\begin{pmatrix} y' \\\\ y_1' \\\\ y_2' \\\\ \\vdots \\\\ y_{K-3}' \\\\ y_{K-2}' \\\\ y_{K-1}' \\end{pmatrix}=\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{K-2} \\\\ y_{K-1} \\\\ \\frac{\\mathrm{d}^{K} y}{\\mathrm{d} t^{K}} \\end{pmatrix}\\] \\[=\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{K-2} \\\\ y_{K-1} \\\\ -a_{K-1} y_{K-1}- \\dots - a_2 y_2 - a_1 y_1 - a_0 y+ f(t) \\end{pmatrix}\\] \\[=\\underbrace{\\begin{pmatrix} 0 & 1 & 0 & \\dots & 0 & 0 & 0 \\\\ 0 & 0 & 1 & \\dots & 0 & 0 & 0 \\\\ 0 & 0 & 0 & \\dots & 0 & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\dots & 0 & 1 & 0 \\\\ 0 & 0 & 0 & \\dots & 0 & 0 & 1 \\\\ -a_0 & -a_1 & -a_2 & \\dots & -a_{K-3} & -a_{K-2} & -a_{K-1}  \\end{pmatrix}}_{A} \\underbrace{\\begin{pmatrix} y \\\\ y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{K-3} \\\\ y_{K-2} \\\\ y_{K-1} \\end{pmatrix}}_{\\boldsymbol{y}}+\\underbrace{\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\\\ f(t) \\end{pmatrix}}_{\\boldsymbol{b}}=A \\boldsymbol{y}+\\boldsymbol{b}.\\]\nThe initial condition vector will be \\[\\boldsymbol{y}_0=\\begin{pmatrix} y(0) \\\\ y_1(0) \\\\ y_2(0) \\\\ \\vdots \\\\ y_{K-3}(0) \\\\ y_{K-2}(0) \\\\ y_{K-1}(0) \\end{pmatrix}=\\begin{pmatrix} y(0) \\\\ \\frac{\\mathrm{d} y}{\\mathrm{d} t}(0) \\\\ \\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}(0) \\\\ \\vdots \\\\ \\frac{\\mathrm{d}^{K-3} y}{\\mathrm{d} t^{K-3}}(0) \\\\ \\frac{\\mathrm{d}^{K-2} y}{\\mathrm{d} t^{K-2}}(0) \\\\ \\frac{\\mathrm{d}^{K-1} y}{\\mathrm{d} t^{K-1}}(0) \\end{pmatrix}=\\begin{pmatrix} \\eta_0 \\\\ \\eta_1 \\\\ \\eta_2 \\\\ \\vdots \\\\ \\eta_{K-3} \\\\ \\eta_{K-2} \\\\ \\eta_{K-1} \\end{pmatrix}.\\]\nThe matrix \\(A\\) is called the Companion Matrix and is a matrix with 1 on the super diagonal and the last row is the minus of the coefficients in the higher order IVP, and zeros otherwise. Now that the \\({K}^{\\mathrm{th}}\\) order IVP has been converted into a set of \\(K\\) linear IVPs, it can be solved just as in Section 3.3. Note that any linear \\({K}^{\\mathrm{th}}\\) order IVP can always be converted into a set of \\(K\\) first order IVPs but the converse is not always possible.\n\n\n\n\n\n\nHigher Order IVPs\n\n\n\nConsider the following higher order IVP \\[\\frac{\\mathrm{d}^{4} y}{\\mathrm{d} t^{4}}-8\\frac{\\mathrm{d}^{3} y}{\\mathrm{d} t^{3}}+7\\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}-\\frac{\\mathrm{d} y}{\\mathrm{d} t}+2y=\\cos(t) \\quad \\text{for} \\quad t \\in \\mathbb{R}_{\\geq 0}\\] \\[\\text{with} \\quad y(0)=4, \\quad \\frac{\\mathrm{d} y}{\\mathrm{d} t}(0)=1, \\quad \\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}(0)=3, \\quad \\frac{\\mathrm{d}^{3} y}{\\mathrm{d} t^{3}}(0)=0.\\]\nLet \\(u=\\frac{\\mathrm{d} y}{\\mathrm{d} t}, v=u'=\\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}\\) and \\(w=v'=\\frac{\\mathrm{d}^{3} y}{\\mathrm{d} t^{3}}\\). The derivatives of \\(u,v\\) and \\(w\\) are: \\[\\begin{align*}\n& u'=v \\\\\n& v'=w \\\\\n& w'= \\frac{\\mathrm{d}^{4} y}{\\mathrm{d} t^{4}}=8\\frac{\\mathrm{d}^{3} y}{\\mathrm{d} t^{3}}-7\\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}+\\frac{\\mathrm{d} y}{\\mathrm{d} t}-2y+\\cos(t)=8w-7v+u+2y+\\cos(t)\n\\end{align*}\\] Define the vector \\(\\boldsymbol{y}={(y, u, v, w)}^{\\mathrm{T}}\\) \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=\\frac{\\mathrm{d} }{\\mathrm{d} t}\\begin{pmatrix} y \\\\ u \\\\ v \\\\ w \\end{pmatrix}=\\begin{pmatrix} u \\\\ v \\\\ w \\\\ \\cos(t)+8w-7v+u-2y \\end{pmatrix}\\] \\[=\\underbrace{\\begin{pmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ -2 & 1 & -7 & 8 \\end{pmatrix}}_{A}\\underbrace{\\begin{pmatrix} y \\\\ u \\\\ v \\\\ w \\end{pmatrix}}_{\\boldsymbol{y}}+\\underbrace{\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\cos(t) \\end{pmatrix}}_{\\boldsymbol{b}(t)}=A\\boldsymbol{y}+\\boldsymbol{b}(t).\\]\nThe initial condition vector will be \\[\\boldsymbol{y}_0=\\begin{pmatrix} y(0) \\\\ u(0) \\\\ v(0) \\\\ w(0) \\end{pmatrix}=\\begin{pmatrix} y(0) \\\\ \\frac{\\mathrm{d} y}{\\mathrm{d} t}(0)\\\\ \\frac{\\mathrm{d}^{2} y}{\\mathrm{d} t^{2}}(0) \\\\ \\frac{\\mathrm{d}^{3} y}{\\mathrm{d} t^{3}}(0) \\end{pmatrix}=\\begin{pmatrix} 4 \\\\ 1 \\\\ 3 \\\\ 0 \\end{pmatrix}.\\]\nNow the IVP can be solved using the Euler method as before but only the first function is the most relevant, all others have been used as placeholders.\n\n\n\n3.4.1 Sets of Higher Order IVPs\nThe method above can be extended into a set of higher order IVPs.\n\n\n\n\n\n\nSet of Higher Order IVPs\n\n\n\nConsider the following coupled system of higher order IVPs \\[y''+6y'+y=\\sin(t), \\quad z'''-8z''=5y-2y'+\\mathrm{e}^{2t}\\] \\[\\text{with} \\quad y(0)=1, \\quad \\frac{\\mathrm{d} y}{\\mathrm{d} t}(0)=2, \\quad z(0)=4, \\quad \\frac{\\mathrm{d} z}{\\mathrm{d} t}(0)=1, \\quad \\frac{\\mathrm{d}^{2} z}{\\mathrm{d} t^{2}}(0)=2\\]\nIn the case of a coupled system, the vector function \\(\\boldsymbol{y}\\) should consist of all the unknown functions and their derivatives up to but not including their highest order derivative. In other words, \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=\\frac{\\mathrm{d} }{\\mathrm{d} t}\\begin{pmatrix} y \\\\ y' \\\\ z \\\\ z' \\\\ z'' \\end{pmatrix}=\\begin{pmatrix} y' \\\\ y'' \\\\ z' \\\\ z'' \\\\ z''' \\end{pmatrix}=\\begin{pmatrix} y' \\\\ -y-6y'+\\sin(t) \\\\ z' \\\\ z'' \\\\ 5y-2y'+8z''+\\mathrm{e}^{2t} \\end{pmatrix}\\] \\[=\\underbrace{\\begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\\\ -1 & -6 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 1 \\\\ 5 & -2 & 0 & 0 & 8 \\end{pmatrix}}_{A}\\underbrace{\\begin{pmatrix} y \\\\ y' \\\\ z \\\\ z' \\\\ z'' \\end{pmatrix}}_{\\boldsymbol{y}}+\\underbrace{\\begin{pmatrix} 0 \\\\ \\sin(t) \\\\ 0 \\\\ 0 \\\\ \\mathrm{e}^{2t} \\end{pmatrix}}_{\\boldsymbol{b}}.\\] The vector of initial values would be \\[\\boldsymbol{y}(0)=\\begin{pmatrix} y(0) \\\\ y'(0) \\\\ z(0) \\\\ z'(0) \\\\ z''(0) \\end{pmatrix}=\\begin{pmatrix} 1 \\\\ 2 \\\\ 4 \\\\ 1 \\\\ 2 \\end{pmatrix}.\\]\nNow this can be solved just as before with the most relevant terms being the first and third (since those are \\(y\\) and \\(z\\)).\n\n\n\n\n3.4.2 Stability of a Set of ODEs\nConsider the set of \\(K\\) homogeneous ODEs \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}.\\] Let \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_K\\) be the eigenvalues of the matrix \\(A\\) and \\(\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K\\) be their distinct corresponding eigenvectors (distinct for the sake argument). Analytically, the set of differential equations \\(\\boldsymbol{y}'=A\\boldsymbol{y}\\) has the general solution \\[\\boldsymbol{y}(t)=C_1 \\boldsymbol{v}_1 \\mathrm{e}^{\\lambda_1 t}+C_2 \\boldsymbol{v}_2 \\mathrm{e}^{\\lambda_2 t}+\\dots+C_K \\boldsymbol{v}_K \\mathrm{e}^{\\lambda_K t}\\] where \\(C_1, C_2, \\dots, C_n\\) are constants that can be determined from the initial values.\n\nDefinition 3.1 The initial value problem \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b} \\quad \\text{with} \\quad \\boldsymbol{y}(0)=\\boldsymbol{y}_0\\] is said to be Asymptotically Stable if \\(\\boldsymbol{y} \\to \\boldsymbol{0}\\) as \\(t \\to \\infty\\), in other words, all functions in \\(\\boldsymbol{y}\\) tend to 0 as \\(t\\) tends to infinity.\n\nThis definition will be important when looking at the long term behaviour of solutions from the eigenvalues to then determine stepsize bounds.\n\nTheorem 3.1 The initial value problem \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b}\\] is asymptotically stable if all the eigenvalues of the matrix \\(A\\) have negative real parts. If \\(A\\) has at least one eigenvalue with a non-negative real part, then the system is not asymptomatically stable.\n\nNotice that the stability of a set of ODEs does not depend on the forcing term \\(\\boldsymbol{b}\\) nor does it depend on the initial condition \\(\\boldsymbol{y}(0)\\).",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#limitations-of-the-euler-method",
    "href": "07_IVP_Euler.html#limitations-of-the-euler-method",
    "title": "3  The Euler Method",
    "section": "3.5 Limitations of the Euler Method",
    "text": "3.5 Limitations of the Euler Method\nIn some cases, if the stepsize \\(h\\) is taken to be too large, then the Euler method can give misleading results.\nFor example, consider the initial value problem: \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=-3y \\quad \\text{with} \\quad y(0)=1, \\quad t\\in [0,5].\\] Choosing a large stepsize \\(h\\) can render the method ineffective. Case in point, when \\(h=1\\), the approximate solution oscillates and grows quite rapidly, however choosing a smaller value of \\(h\\), say \\(h=0.1\\), gives a very good approximation to the exact solution. These are illustrated in the figures below.\n \nAnother situation when the Euler method fails is when the IVP does not have a unique solution. For example, consider the IVP: \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=y^{\\frac{1}{3}} \\quad \\text{with} \\quad y(0)=0, \\quad t \\in [0,2].\\] This has the exact solution \\(y(t)=\\left( \\frac{2}{3}t \\right)^{\\frac{3}{2}}\\) however this is not unique since \\(y(t)=0\\) is also a perfectly valid solution. The Euler method in this case will not be able to capture the first non-trivial solution but will only capture the second trivial solution giving a straight line at 03.\n\n3.5.1 Bounds on the Stepsize\nConsider the initial value problem \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b} \\quad \\text{with} \\quad \\boldsymbol{y}(0)=\\boldsymbol{y}_0.\\] If \\(A\\) is asymptotically stable, then a maximum bound \\(h_0\\) for the stepsize can be found to ensure that the iterations converge. (This means that asymptotic stability of \\(A\\) is a necessary and sufficient condition for the existence of an upper bound \\(h_0\\) such that if \\(h&lt;h_0\\), then the Euler iteration converges.)\nIf the stepsize is too large, then the method may not converge but on the other hand if it is too low, then the iteration will take a considerable amount of time to perform. Therefore an “optimal” stepsize is needed to obtain sufficiently accurate solutions.\n\n\n\n\n\n\nDifferent Stepsizes\n\n\n\nConsider the following initial value problem \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=100(\\sin(t)-y) \\quad \\text{with} \\quad y(0)=0.\\] The figure below shows the Euler method being used to solve the initial value problem in the interval \\([0,1]\\) for the stepsizes \\(h=0.03, 0.02, 0.01, 0.001\\).\n\nWhen \\(h=0.03\\), the Euler method does not converge. At \\(h=0.02\\), the Euler method converges but there clearly is a distinct artefact in the solution that shows a slight oscillation. For \\(h\\) less than \\(0.02\\), this oscillation is no longer observed and the Euler method is convergent.\n\n\n\n\n3.5.2 Exact Bound\nConsider the IVP \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b} \\quad \\text{with} \\quad \\boldsymbol{y}(0)=\\boldsymbol{y}_0.\\] Let \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_K\\) be the eigenvalues of \\(A\\). Suppose that the matrix \\(A\\) is asymptotically stable (i.e. \\(\\Re(\\lambda_k)&lt;0\\) for all \\(k=1,2,\\dots,K\\)). In order for the Euler iterations to converge, the stepsize \\(h\\) needs be less than the threshold stepsize \\(h_0\\) where \\[h_0=2\\min_{k=1,2,\\dots,K}\\left\\{ \\frac{|\\Re(\\lambda_k)|}{|\\lambda_k|^2} \\right\\} \\tag{3.4}\\] \\[\\text{or} \\quad h_0=2 \\min_{k=1,2,\\dots,K} \\left\\{ \\frac{1}{|\\lambda_k|} \\right\\} \\quad \\text{if all the eigenvalues are real.}\\] In other words, if the initial value problem is asymptotically stable, then the Euler method is stable if an only if \\(h&lt;h_0\\). This means that the convergence of the Euler is characterised by the eigenvalue that is furthest away from the origin, also called the Dominant Eigenvalue.\n!!!!!DO!!!!!\n\n\n\n\n\n\nEuler Upper Bound\n\n\n\nConsider the system of differential equations \\(\\boldsymbol{y}'=A\\boldsymbol{y}\\) with \\(\\boldsymbol{y}(0)=\\boldsymbol{y}_0\\) where \\[A=\\begin{pmatrix} -1 & 0 & 3 \\\\ 0 & -10 & 0 \\\\ 18 & -1 & -100 \\end{pmatrix}.\\] The eigenvalues of the matrix \\(A\\) are \\(-0.4575, -100.5425, -10\\). Since all the eigenvalues are negative, this system is asymptotically stable. Since all the eigenvalues are real, then the threshold stepsize for a convergent Euler method is \\[h_0=2\\min\\left\\{ \\frac{1}{|\\lambda_k|} \\right\\} =2\\min \\left\\{ \\frac{1}{|-0.4575|}, \\frac{1}{|-100.5425|}, \\frac{1}{|-10|} \\right\\}\\] \\[=2\\min \\left\\{ 2.0858, 0.0099, 0.1 \\right\\}=2 \\times 0.0099=0.0199.\\]\nSolutions for different stepsizes are as shown below with the initial values \\(y_1(0)=1\\) (blue), \\(y_2(0)=2\\) (red) and \\(y_3(0)=1\\) (magenta). It can be seen that if \\(h \\geq h_0\\), then at least one solution will diverge but if \\(h&lt;h_0\\), then all solutions converge to 0.\n\n\n\n\n\n3.5.3 Estimated Bound\nOne drawback in attempting to determine the value of \\(h_0\\) using Equation 3.4 is that all the eigenvalues of the matrix \\(A\\) have to be determined before \\(h_0\\) can be found. This can be computationally expensive for especially for very large matrices.\nAn estimate for the threshold stepsize \\(h_0\\) can be found with far fewer computations using the sup-norm \\(|| \\cdot ||_{\\infty}\\) (also known as the infinity norm or the Chebyshev norm). Recall that for a vector \\(\\boldsymbol{x}=(x_1, x_2, \\dots, x_n)\\), the sup-norm of \\(\\boldsymbol{x}\\) is the maximum absolute value in the vector, i.e. \\[|| \\boldsymbol{x} ||_{\\infty}=\\max |x_n|.\\]\nWhereas for a matrix \\(A\\), the sup-norm of \\(A\\) is the maximal absolute row sum. In other words, for a given matrix \\(A\\), take the absolute value of all the terms, take the sum of each row and the sup-norm will be the largest out of these.\n\n\n\n\n\n\nSup-Norm of Vectors & Matrices\n\n\n\nConsider the vector \\(\\boldsymbol{x}\\) and matrix \\(M\\) given by \\[\\boldsymbol{x}=\\begin{pmatrix} 1 \\\\ -4 \\\\ -9 \\\\ 7 \\end{pmatrix}, \\quad M=\\begin{pmatrix}\n     5 & 2 &  4 &  1 \\\\\n    -9 & 5 &  3 & -7 \\\\\n     6 & 0 & -1 &  4 \\\\\n     9 & 5 & -2 &  4\n\\end{pmatrix}.\\]\nThe sup-norm of \\(\\boldsymbol{x}\\) is simply the largest absolute element which is \\(9\\), therefore \\(|| \\boldsymbol{x} ||_{\\infty}=9\\).\nAs for \\(M\\), to find the sup-norm, first take the absolute value of all the terms, then add the rows. The sup-norm is the maximum element that results: \\[\n\\begin{pmatrix}\n     5 & 2 &  4 &  1 \\\\\n    -9 & 5 &  3 & -7 \\\\\n     6 & 0 & -1 &  4 \\\\\n     9 & 5 & -2 &  4\n\\end{pmatrix} \\quad \\xrightarrow[| \\bullet |]{} \\quad\n\\begin{pmatrix}\n     5 & 2 &  4 &  1 \\\\\n     9 & 5 &  3 &  7 \\\\\n     6 & 0 &  1 &  4 \\\\\n     9 & 5 &  2 &  4\n\\end{pmatrix}\n\\left. \\begin{matrix}\n     \\to 12 \\\\ \\to 24 \\\\ \\to 11 \\\\ \\to 20\n\\end{matrix} \\right\\} \\text{maximum is 24.}\n\\] Therefore \\(|| M ||_{\\infty}=24\\).\nBoth of these can be found in MATLAB using norm(x,Inf) and norm(M,Inf).\n\n\n\nTheorem 3.2 Consider the set of linear IVPs \\[\\frac{\\mathrm{d} \\boldsymbol{y}}{\\mathrm{d} t}=A\\boldsymbol{y}+\\boldsymbol{b} \\quad \\text{with} \\quad \\boldsymbol{y}(0)=\\boldsymbol{y}_0\\] where \\(A\\) is asymptotically stable. Then the Euler method is numerically convergent for any choice of \\(h\\) which satisfies \\[|| \\mathcal{I}+hA ||_{\\infty} \\leq 1.\\]\n\nComputing all the eigenvalues of the matrix \\(A\\) can be computationally expensive but obtaining the sup-norm is takes far fewer computations, however as a drawback, the resulting value of \\(h_0\\) would be an estimate.\n\n\n\n\n\n\nStepsize Bound Estimate 1 (Tridiagonal)\n\n\n\nConsider the differential equation \\(\\boldsymbol{y}'=A\\boldsymbol{y}\\) where \\[\nA=\\begin{pmatrix}\n    -2 &  1 &  0 &  0 &  0 \\\\\n     1 & -2 &  1 &  0 &  0 \\\\\n     0 &  1 & -2 &  1 &  0 \\\\\n     0 &  0 &  1 & -2 &  1 \\\\\n     0 &  0 &  0 &  1 & -2\n\\end{pmatrix}.\n\\]\nTo find the upper bound for the stepsize for which the Euler method converges, first evaluate \\(\\mathcal{I}+hA\\): \\[\n\\mathcal{I}+hA=\\begin{pmatrix}\n    1-2h &  h &  0 &  0 &  0 \\\\\n     h & 1-2h &  h &  0 &  0 \\\\\n     0 &  h & 1-2h &  h &  0 \\\\\n     0 &  0 &  h & 1-2h &  h \\\\\n     0 &  0 &  0 &  h & 1-2h\n\\end{pmatrix}\n\\]\nTo find the sup-norm, take the absolute value of all the terms and find the maximal row sum: \\[\n\\xrightarrow[| \\bullet |]{} \\quad\n\\begin{pmatrix}\n    |1-2h| &  h &  0 &  0 &  0 \\\\\n     h & |1-2h| &  h &  0 &  0 \\\\\n     0 &  h & |1-2h| &  h &  0 \\\\\n     0 &  0 &  h & |1-2h| &  h \\\\\n     0 &  0 &  0 &  h & |1-2h|\n\\end{pmatrix}\n\\begin{matrix} \\to \\\\ \\to \\\\ \\to \\\\\\to \\\\ \\to \\end{matrix}\\begin{matrix} |1-2h|+h \\\\ |1-2h|+2h \\\\ |1-2h|+2h \\\\ |1-2h|+2h \\\\ |1-2h|+h. \\end{matrix}\n\\] Let \\(a=|1-2h|+2h\\) and \\(b=|1-2h|+h\\). Since \\(h&gt;0\\), then \\(a&gt;b\\), therefore \\[|| \\mathcal{I}+hA ||_{\\infty}=|1-2h|+2h.\\]\nIn order to satisfy the inequality \\(|| \\mathcal{I}+hA ||_{\\infty}\\leq 1\\), consider the cases when \\(1-2h \\geq 0\\) and \\(1-2h&lt;0\\) separately:\n\nIf \\(1-2h \\geq 0\\), then \\(h \\leq \\frac{1}{2}\\): \\[|| \\mathcal{I}+hA ||_{\\infty}=|1-2h|+2h=1-2h+2h=1.\\] Therefore \\(|| \\mathcal{I}+hA ||_{\\infty}=1 \\leq 1\\) is indeed true.\nIf \\(1-2h &lt; 0\\), then \\(h &gt; \\frac{1}{2}\\): \\[|| \\mathcal{I}+hA ||_{\\infty}=|1-2h|+2h=2h-1+2h=4h-1.\\] If \\(|| \\mathcal{I}+hA ||_{\\infty} \\leq 1\\), then \\(4h-1 \\leq 1\\). Simplifying this would result in \\(h \\leq \\frac{1}{2}\\) which contradicts with the assumption that \\(h&gt;\\frac{1}{2}\\).\n\nFrom these two cases, it is clear that \\(h \\ngtr \\frac{1}{2}\\) (since that case leads to a contradiction), therefore \\(h \\leq \\frac{1}{2}\\). Thus for a convergent Euler method, the stepsize \\(h\\) must be less than the threshold stepsize \\(h_0=\\frac{1}{2}\\).\nThis can be compared to the exact bound; the eigenvalues of the matrix \\(A\\) are \\[-3.7321, \\quad -3, \\quad -2, \\quad -1, \\quad -0.2679.\\] Therefore \\[h_0=2 \\min \\left\\{ \\frac{1}{|\\lambda_k|} \\right\\}=0.5359\\] which is a larger bound compared to the one obtained using the sup-norm method. Observe that if the size of the matrix was larger but followed the same theme (i.e. \\(2\\) on the main diagonal and \\(-1\\) and the sub and super diagonals), then no further calculations are required for the sup-norm method, the outcome will still be \\(h_0=\\frac{1}{2}\\). As for the eigenvalue method, all the eigenvalues have to be recalculated again.\n\n\n\n\n\n\n\n\nStepsize Bound Estimate 2 (Bidiagonal)\n\n\n\nConsider the differential equation \\(\\boldsymbol{y}'=A\\boldsymbol{y}\\) where \\[\nA=\\begin{pmatrix}\n    -1 &  0 &  0 &  0 &  0 \\\\\n     1 & -1 &  0 &  0 &  0 \\\\\n     0 &  1 & -1 &  0 &  0 \\\\\n     0 &  0 &  1 & -1 &  0 \\\\\n     0 &  0 &  0 &  1 & -1\n\\end{pmatrix}.\n\\]\nTo find the upper bound for the stepsize for which the Euler method converges, first evaluate \\(\\mathcal{I}+hA\\): \\[\n\\mathcal{I}+hA=\\begin{pmatrix}\n    1-h &  0 &  0 &  0 &  0 \\\\\n     h & 1-h &  0 &  0 &  0 \\\\\n     0 &  h & 1-h &  0 &  0 \\\\\n     0 &  0 &  h & 1-h &  0 \\\\\n     0 &  0 &  0 &  h & 1-h\n\\end{pmatrix}\n\\] To find the sup-norm, take the absolute value of all the terms and find the maximal row sum: \\[\n\\xrightarrow[| \\bullet |]{} \\quad\n\\begin{pmatrix}\n    |1-h| &  0 &  0 &  0 &  0 \\\\\n     h & |1-h| &  0 &  0 &  0 \\\\\n     0 &  h & |1-h| &  0 &  0 \\\\\n     0 &  0 &  h & |1-h| &  0 \\\\\n     0 &  0 &  0 &  h & |1-h|\n\\end{pmatrix}\n\\begin{matrix} \\to \\\\ \\to \\\\ \\to \\\\\\to \\\\ \\to \\end{matrix}\\begin{matrix} |1-h| \\\\ |1-h|+h \\\\ |1-h|+h \\\\ |1-h|+h \\\\ |1-h|+h. \\end{matrix}\n\\] Let \\(a=|1-h|+h\\) and \\(b=|1-h|\\). Clearly \\(a&gt;b\\) since \\(h&gt;0\\), therefore \\[|| \\mathcal{I}+hA ||_{\\infty}=|1-h|+h.\\]\nIn order to satisfy the inequality, \\(|| \\mathcal{I}+hA ||_{\\infty}\\leq 1\\), consider the cases when \\(1-h \\geq 0\\) and \\(1-h&lt;0\\):\n\nIf \\(1-h \\geq 0\\), then \\(h \\leq 1\\): \\[|| \\mathcal{I}+hA ||_{\\infty}=|1-h|+h=1-h+h=1,\\] therefore \\(|| \\mathcal{I}+hA ||_{\\infty} \\leq 1\\) is indeed true.\nIf \\(1-h &lt; 0\\), then \\(h &gt; 1\\): \\[|| \\mathcal{I}+hA ||_{\\infty}=|1-h|+h=h-1+h=2h-1.\\] If \\(|| \\mathcal{I}+hA ||_{\\infty} \\leq 1\\), then \\(2h-1 \\leq 1\\), meaning that \\(h \\leq 1\\) which contradicts with the assumption that \\(h&gt;1\\).\n\nThis means that for a convergent Euler method, the stepsize \\(h\\) must be less than \\(h_0=1\\).\nThis can be compared to the exact upper bound. The eigenvalues of the matrix \\(A\\) are just \\(-1\\) five times, therefore \\[h_0=2 \\min \\left\\{ \\frac{1}{|\\lambda_k|} \\right\\}=2,\\] this shows that the sup-norm method gives a tighter than using eigenvalues.\n\n\nThe sup-norm method works well when the matrix in question has a diagonal, bidiagonal or tridiagonal structure where the diagonal terms are the same. In general, the sup-norm method might not be suitable for any matrix.\n\n\n\n\n\n\nStepsize Bound Estimate 3 (General)\n\n\n\nConsider the differential equation \\(\\boldsymbol{y}'=A\\boldsymbol{y}\\) where \\[A=\\begin{pmatrix} -1 & -2 \\\\ 4 & -3 \\end{pmatrix}.\\]\nFind the sup-norm: \\[\\mathcal{I}+hA=\\begin{pmatrix}\n        1-h & -2h \\\\\n        4h & 1-3h\n    \\end{pmatrix} \\xrightarrow[| \\bullet |]{} \\begin{pmatrix}\n    |1-h| & 2h \\\\\n    4h & |1-3h|\n    \\end{pmatrix} \\begin{matrix}\n    \\to \\\\ \\to\n    \\end{matrix} \\begin{matrix}\n    |1-h|+2h \\\\ |1-3h|+4h\n    \\end{matrix}\n\\] Let \\(a=|1-h|+2h\\) and \\(b=|1-3h|+4h\\). Here, it is not obvious which is larger, \\(a\\) or \\(b\\). Therefore, consider the three cases \\(0&lt;h&lt;\\frac{1}{3}\\), \\(\\frac{1}{3}&lt;h&lt;1\\) and \\(h&gt;1\\).\n\n\\(0&lt;h&lt;\\frac{1}{3}\\): In this case, \\(1-h&gt;0\\) and \\(1-3h&gt;0\\), therefore \\(a=|1-h|+2h=1+h\\) and \\(b=|1-3h|+4h=1+h\\), hence \\(|| \\mathcal{I}+hA ||_{\\infty}=1+h\\). In order to satisfy \\(|| \\mathcal{I}+hA ||_{\\infty} \\leq 1\\), this would mean that \\(h&lt;0\\) which contradicts with the fact that \\(h&gt;0\\). Therefore \\(h \\notin \\left( 0,\\frac 13 \\right)\\).\n\\(\\frac{1}{3}&lt;h&lt;1\\): In this case, \\(1-h&gt;0\\) and \\(1-3h&lt;0\\), therefore \\(a=|1-h|+2h=1+h\\) and \\(b=|1-3h|+4h=7h-1\\). This should now be split into two subcases to check which one will lead to a contradiction:\n\nSuppose that \\(a&gt;b\\), then \\[1+h &gt; 7h-1 \\quad \\implies \\quad h&lt;\\frac{1}{3}\\] which contradicts with \\(h&gt;\\frac13\\)\nSuppose that \\(a&lt;b\\), then \\[1+h &lt; 7h-1 \\quad \\implies \\quad h&gt;\\frac{1}{3}\\] not leading to any contradiction. therefore since \\(b&gt;a\\), then \\(|| \\mathcal{I}+hA ||_{\\infty}=b=7h-1\\).\n\n\nIn order to satisfy \\(|| \\mathcal{I}+hA ||_{\\infty} \\leq 1\\) then \\(h&lt;\\frac{2}{7}\\) which contradicts with the fact that \\(\\frac{1}{3}&lt;h\\). Therefore \\(h \\notin \\left( \\frac 13,1 \\right)\\).\n\n\\(h&gt;1\\): In this case, \\(1-h&lt;0\\) and \\(1-3h&lt;0\\), therefore \\(a=|1-h|+2h=3h-1\\) and \\(b=|1-3h|+4h=7h-1\\). Clearly \\(b&gt;a\\) since \\(h&gt;0\\), so \\(|| \\mathcal{I}+hA ||_{\\infty}=7h-1\\). In order to satisfy \\(|| \\mathcal{I}+hA ||_{\\infty} \\leq 1\\) then \\(h&lt;\\frac{2}{7}\\) which contradicts with the fact that \\(h&gt;1\\). This means that \\(h \\ngtr 1\\).\n\nSo in every possible case, there will be a contradiction when using the sup-norm method. This does not mean that the system is asymptotically unstable, in fact, the eigenvalues of the matrix \\(A\\) are \\(-2 \\pm 2.65 \\mathrm{i}\\) meaning that the system is asymptotically stable and the threshold stepsize is in fact \\(h_0=0.0992\\).\nThis example shows that the sup-norm method cannot be used for any matrix system, but if a matrix has a banded structure, then it would be appropriate and would require fewer computations compared to finding all the eigenvalues.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#matlab-code",
    "href": "07_IVP_Euler.html#matlab-code",
    "title": "3  The Euler Method",
    "section": "3.6 MATLAB Code",
    "text": "3.6 MATLAB Code\nThe following MATLAB code performs the Euler iteration for the following set of IVPs on the interval \\([0,1]\\): \\[\\begin{align*}\n& \\frac{\\mathrm{d} u}{\\mathrm{d} t}=2u+v+w+\\cos(t), & \\quad u(0)=0 \\\\\n& \\frac{\\mathrm{d} v}{\\mathrm{d} t}=\\sin(u)+\\mathrm{e}^{-v+w}, & \\quad v(0)=1 \\\\\n& \\frac{\\mathrm{d} w}{\\mathrm{d} t}=uv-w, & \\quad w(0)=0.\n\\end{align*}\\]\n\n\n\n\n\n\nLinearity\n\n\n\nNote that this code is built for a general case that does not have to be linear even though the entire derivation process was built on the fact that the system is linear.\n\n\nfunction IVP_Euler\n\n%% Solve a set of first order IVPs using Euler\n\n% This code solves a set of IVP when written explicitly\n% on the interval [t0,tf] subject to the initial conditions\n% y(0)=y0.  The output will be the graph of the solution(s)\n% and the vector value at the final point tf.  Note that the\n% IVPs do not need to be linear or homogeneous.\n\n%% Lines to change:\n\n% Line 28   : t0 - Start time\n% Line 31   : tf - End time\n% Line 34   : N  - Number of subdivisions\n% Line 37   : y0 - Vector of initial values\n% Line 105+ : Which functions to plot, remembering to assign\n%             a colour, texture and legend label\n% Line 125+ : Set of differential equations written\n%             explicitly.  These can also be non-linear and\n%             include forcing terms.  These equations can\n%             also be written in matrix form if the\n%             equations are linear.\n\n%% Set up input values\n\n% Start time\nt0=0;\n\n% End time\ntf=1;\n\n% Number of subdivisions\nN=50;\n\n% Column vector initial values y0=y(t0)\ny0=[0;1;0];\n\n%% Set up IVP solver parameters\n\n% T = Vector of times t0,t1,...,tN.\n% This is generated using linspace which splits the\n% interval [t0,tf] into N+1 points (or N subintervals)\nT=linspace(t0,tf,N+1);\n\n% Stepsize\nh=(tf-t0)/N;\n\n% Number of differential equations\nK=length(y0);\n\n%% Perform the Euler iteration\n\n% Y = Solution matrix\n% The matrix Y will contain K rows and N+1 columns.  Every\n% row corresponds to a different IVP and every column\n% corresponds to a different time.  So the matrix Y will\n% take the following form:\n% y_1(t_0)  y_1(t_1)  y_1(t_2)  ...  y_1(t_N)\n% y_2(t_0)  y_2(t_1)  y_2(t_2)  ...  y_2(t_N)\n% ...\n% y_K(t_0)  y_K(t_1)  y_K(t_2)  ...  y_K(t_N)\nY=zeros(K,N+1);\n\n% The first column of the vector Y is the initial vector y0\nY(:,1)=y0;\n\n% Set the current time t to be the starting time t0 and the\n% current value of the vector y to be the strtaing values y0\nt=t0;\ny=y0;\n\nfor n=2:1:N+1\n\n    dydt=DYDT(t,y,K);  % Find gradient at the current step\n\n    y=y+h*dydt;        % Find y at the current step\n\n    t=T(n);            % Update the new time\n\n    Y(:,n)=y;          % Replace row n in Y with y\n\nend\n\n%% Setting plot parameters\n\n% Clear figure\nclf\n\n% Hold so more than one line can be drawn\nhold on\n\n% Turn on grid\ngrid on\n\n% Setting font size and style\nset(gca,'FontSize',20,'FontName','Times')\n\n% Label the axes\nxlabel('$t$','Interpreter','Latex')\nylabel('$\\mathbf{y}(t)$','Interpreter','Latex')\n\n% Plot the desried solutions.  If all the solutions are\n% needed, then consider using a for loop in that case\nplot(T,Y(1,:),'-b','LineWidth',2)\nplot(T,Y(2,:),'-r','LineWidth',2)\nplot(T,Y(3,:),'-k','LineWidth',2)\n\n% Legend labels\nlegend('$y_1(t)$','$y_2(t)$','$y_3(t)$')\nset(legend,'Interpreter','Latex')\n\n% Display the values of the vector y at tf\ndisp(strcat('The vector y at t=',num2str(tf),' is:'))\ndisp(Y(:,end))\n\nend\n\nfunction [dydt]=DYDT(t,y,K)\n\n% When the equation are written in explicit form\n\ndydt=zeros(K,1);\n\ndydt(1)=2*y(1)+y(2)+y(3)+cos(t);\n\ndydt(2)=sin(y(1))+exp(-y(2)+y(3));\n\ndydt(3)=y(1)*y(2)-y(3);\n\n% If the set of equations is linear, then these can be\n% written in matrix form as dydt=A*y+b(t).  For example, if\n% the set of equations is:\n% dudt = 7u - 2v +  w + exp(t)\n% dvdt = 2u + 3v - 9w + cos(t)\n% dwdt =      2v + 5w + 2\n% Then:\n% A=[7,-2,1;2,3,-9;0,2,5];\n% b=@(t) [exp(t);cos(t);2];\n% dydt=A*y+b(t)\n\nend",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "07_IVP_Euler.html#footnotes",
    "href": "07_IVP_Euler.html#footnotes",
    "title": "3  The Euler Method",
    "section": "",
    "text": "In most cases, the interval width \\(h\\) is constant but more advanced numerical techniques have different subinterval widths.↩︎\nTaylor’s Theorem states that for a function \\(f\\) that is at least \\(N+1\\) times differentiable in the open interval \\((x,x_0)\\) (or \\((x_0,x)\\)), then \\[f(x)=f(x_0)+f'(x_0)(x-x_0)+\\frac{1}{2!}f''(x_0)(x-x_0)^2+\\frac{1}{3!}f'''(x_0)(x-x_0)\\] \\[+\\dots+\\frac{1}{N!}f^{(N)}(x_0)(x-x_0)^N+\\frac{1}{(N+1)!}f^{(N+1)}(\\xi)(x-x_0)^{N+1}\\] for some point \\(\\xi\\) between \\(x\\) and \\(x_0\\).↩︎\nIn general, according to the Picard-Lindelöf Theorem, an IVP of the form \\(y'=f(t,y)\\) with \\(y(0)=y_0\\) has a unique solution if the function \\(f\\) is continuous in \\(t\\) and uniformly Lipschitz continuous in \\(y\\). In this example shown above, the function \\(f(t,y)=y^{\\frac{1}{3}}\\) does not satisfy the aforementioned conditions and therefore the initial value problem does not have a unique solution. These concepts of continuity are far beyond the realms of this course and no further mention of them will be made.↩︎",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Euler Method</span>"
    ]
  },
  {
    "objectID": "08_IVP_ModEuler.html",
    "href": "08_IVP_ModEuler.html",
    "title": "4  The Modified Euler Method",
    "section": "",
    "text": "4.1 Steps of the Modified Euler Method\nThe Modified Euler Method utilises the Fundamental Theorem of Calculus which states that for a differentiable function \\(y\\) defined on the interval \\([t_0,t_1]\\) (where \\(t_1=t_0+h\\) for some stepsize \\(h\\)), \\[y(t_1)-y(t_0)=\\int\\limits_{t_0}^{t_1} \\! y'(t) \\; \\mathrm{d} t.\\] In the interval \\([t_0,t_1]\\), the derivative \\(y'(t)\\) may be approximated by the derivative at the leftmost point \\(y'(t_0)\\), this approximation forms the basis of the standard Euler method; \\[\\begin{align*}\ny(t_1)-y(t_0) & =\\int\\limits_{t_0}^{t_1} \\! y'(t) \\; \\mathrm{d} t \\\\\n& =\\int\\limits_{t_0}^{t_1} \\! y'(t_0) \\; \\mathrm{d} t \\\\\n& =h y'(t_0)\n\\end{align*}\\] \\[\\implies \\quad y(t_1)=y(t_0)+hy'(t_0).\\]\nHowever, if \\(y'(t)\\) varies substantially then this approximation can lead to some poor predictions. This can be modified so rather than approximating \\(y'(t)\\) by \\(y'(t_0)\\) only, it can be approximated by taking an average between \\(y'(t_0)\\) and \\(y'(y_1)\\), namely \\[y'(t) \\approx \\frac{1}{2}\\left( y'(t_0)+y'(t_1) \\right).\\] Thus \\[\\begin{align*}\ny(t_1)-y(t_0) & =\\int\\limits_{t_0}^{t_1} \\! y'(t) \\; \\mathrm{d} t \\\\\n& =\\int\\limits_{t_0}^{t_1} \\! \\frac{1}{2}\\left( y'(t_0)+y'(t_1) \\right) \\; \\mathrm{d} t \\\\\n& = \\frac{h}{2}\\left( y'(t_0)+y'(t_1) \\right)\n\\end{align*}\\] \\[\\implies \\quad y(t_1)=y(t_0)+\\frac{h}{2}\\left( y'(t_0)+y'(t_1) \\right).\\]\nInitially, one might suspect that the derivative \\(y'(t_1)\\) can be found from the differential equation itself, namely, \\(y'(t_1)=f(t_1,y(t_1))\\) but to do that, a Prediction-Correction procedure needs to be employed where the Euler method can be used to predict a value of \\(y(t_1)\\) and this is then corrected afterwards. This is done as follows: \\[\\begin{align*}\n    \\bullet \\quad \\text{Predictor:} \\quad & \\tilde{Y}_{n+1}=Y_n+h f(t_n,Y_n) \\\\\n    \\bullet \\quad \\text{Corrector:} \\quad & Y_{n+1}=Y_n+\\frac{h}{2} \\left[  f(t_n,Y_n)+f(t_{n+1},\\tilde{Y}_{n+1})  \\right].\n\\end{align*}\\]",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Modified Euler Method</span>"
    ]
  },
  {
    "objectID": "08_IVP_ModEuler.html#steps-of-the-modified-euler-method",
    "href": "08_IVP_ModEuler.html#steps-of-the-modified-euler-method",
    "title": "4  The Modified Euler Method",
    "section": "",
    "text": "Modified Euler Method\n\n\n\nConsider the differential equation \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=(1-2t)y^2 \\quad \\text{with} \\quad y(0)=1, \\quad t\\in [0,2].\\] This differential equation is non-linear but has a known particular solution which is \\[y(t)=\\frac{1}{t^2-t+1}\\] and this will be compared to the approximate solutions obtained from the standard and Modified Euler methods.\nThe figure below shows how the standard and modified Euler methods compare to the exact solution for the same stepsize \\(h=0.1\\). This suggests that the Modified Euler method has improved accuracy compared to the Euler method for the same stepsize, however as a consequence, the function \\(f\\) on the right hand side of the differential equation has to be calculated twice for every step; once in the prediction stage and once for the correction. However even with this in mind, doubling the number of calculations to improve accuracy can also warrant for a coarser choice of the stepsize to allow for a more efficient use of computational time.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Modified Euler Method</span>"
    ]
  },
  {
    "objectID": "08_IVP_ModEuler.html#accuracy-of-the-modified-euler-method",
    "href": "08_IVP_ModEuler.html#accuracy-of-the-modified-euler-method",
    "title": "4  The Modified Euler Method",
    "section": "4.2 Accuracy of the Modified Euler Method",
    "text": "4.2 Accuracy of the Modified Euler Method\nIn order to asses the accuracy of the Modified Euler method, consider the Taylor series expansion of \\(y\\) at the points \\(t_0\\) and \\(t_1\\) about \\(t_{0.5}=t_0+\\frac12 h\\):\n\\[y(t_1)=y\\left( t_{0.5}+\\frac{h}{2} \\right)=y(t_{0.5})+\\frac{h}{2}y'(t_{0.5})+\\left(  \\frac{h}{2}  \\right)^2 \\frac{1}{2!} y''(t_{0.5})+\\mathcal{O}\\left(h^3\\right),\\] \\[y(t_0)=y\\left( t_{0.5}-\\frac{h}{2} \\right)=y(t_{0.5})-\\frac{h}{2}y'(t_{0.5})+\\left(  \\frac{h}{2}  \\right)^2 \\frac{1}{2!} y''(t_{0.5})+\\mathcal{O}\\left(h^3\\right).\\]\nSubtracting \\(y(t_0)\\) from \\(y(t_1)\\) gives \\[y(t_1)-y(t_0)=hy'(t_{0.5})+\\mathcal{O}\\left(h^3\\right). \\tag{4.1}\\]\nThe Taylor series expansion can also be done for the derivative of \\(y\\) at the points \\(t_0\\) and \\(t_1\\) about \\(t_{0.5}=t_0+\\frac12 h\\) in a similar way as above, i.e. \\[y'(t_1)=y'\\left( t_{0.5}+\\frac{h}{2} \\right)=y'(t_{0.5})+\\frac{h}{2}y''(t_{0.5})+\\mathcal{O}\\left(h^2\\right),\\] \\[y'(t_0)=y'\\left( t_{0.5}-\\frac{h}{2} \\right)=y'(t_{0.5})-\\frac{h}{2}y''(t_{0.5})+\\mathcal{O}\\left(h^2\\right).\\] Adding \\(y'(t_0)\\) to \\(y'(t_1)\\) gives \\[y'(t_1)+y'(t_0)=2y'(t_{0.5})+\\mathcal{O}\\left(h^2\\right),\\] thus multiplying by \\(\\frac{h}{2}\\) and using equation Equation 4.1 yields \\[\\frac{h}{2}\\left[ y'(t_1)+y'(t_0) \\right]=y(t_1)-y(t_0)+\\mathcal{O}\\left(h^3\\right). \\tag{4.2}\\]\nThe first step of the Modified Euler method is to predict the value of \\(y'(t_1)\\) using the Euler iteration; \\[\\tilde{Y}_1=\\underbrace{y(t_0)+h y'(t_0)}_{\\approx y(t_1)}+\\mathcal{O}\\left(h^2\\right).\\] Hence \\[y'(t_1)=f(t_1,y(t_1))\\approx f(t_1,\\tilde{Y}_1)+\\mathcal{O}\\left(h^2\\right).\\] All this information can now be used to obtain the improved update \\(Y_1\\) which is the corrected form of \\(\\tilde{Y}_1\\). Thus from equation Equation 4.2, \\[\\underbrace{y(t_1)}_{\\approx Y_1} =\\underbrace{y(t_0)}_{=Y_0}+ \\frac{h}{2}[ \\underbrace{y'(t_1)}_{=f(t_1,\\tilde{Y}_1)}+\\underbrace{y'(t_0)}_{=f(t_0,Y_0)}]+\\mathcal{O}\\left(h^3\\right)\\] \\[\\implies \\quad Y_1=Y_0+\\frac{h}{2}\\left[  f(t_1,\\tilde{Y}_1)+f(t_0,Y_0)  \\right]. \\tag{4.3}\\]\nEquations Equation 4.3 and Equation 4.2 can be used to find the local truncation error for the Modified Euler method at the first time step which is \\[e=\\left| y(t_1)-Y_1 \\right|=\\left| y(t_1)-\\left[  y(t_0)+\\frac{h}{2}\\left(  y'(t_1)+y'(t_0)  \\right)  \\right]\\right|+\\mathcal{O}\\left(h^3\\right)=\\mathcal{O}\\left(h^3\\right).\\] Therefore the local truncation error \\(e=\\mathcal{O}\\left(h^3\\right)\\) meaning that the Modified Euler method is third order accurate which is an improvement over the Euler method.\nThe global integration error can be obtained just as before to show that the global integration error of the Modified Euler method is \\(E=\\mathcal{O}\\left(h^2\\right)\\) meaning that this is a second order method. In particular, if the stepsize \\(h\\) is halved, the global integration error will be reduced by a factor of four while the local truncation error will reduce by a factor of eight.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Modified Euler Method</span>"
    ]
  },
  {
    "objectID": "08_IVP_ModEuler.html#matlab-code",
    "href": "08_IVP_ModEuler.html#matlab-code",
    "title": "4  The Modified Euler Method",
    "section": "4.3 MATLAB Code",
    "text": "4.3 MATLAB Code\nThe following MATLAB code performs the Modified Euler iteration for the following set of IVPs on the interval \\([0,1]\\): \\[\\begin{align*}\n& \\frac{\\mathrm{d} u}{\\mathrm{d} t}=2u+v+w+\\cos(t), & \\quad u(0)=0 \\\\\n& \\frac{\\mathrm{d} v}{\\mathrm{d} t}=\\sin(u)+\\mathrm{e}^{-v+w}, & \\quad v(0)=1 \\\\\n& \\frac{\\mathrm{d} w}{\\mathrm{d} t}=uv-w, & \\quad w(0)=0.\n\\end{align*}\\]\n\n\n\n\n\n\nLinearity\n\n\n\nNote that this code is built for a general case that does not have to be linear even though the entire derivation process was built on the fact that the system is linear.\n\n\nfunction IVP_Mod_Euler\n\n%% Solve a set of first order IVPs using Modified Euler\n\n% This code solves a set of IVP when written explicitly\n% on the interval [t0,tf] subject to the initial conditions\n% y(0)=y0.  The output will be the graph of the solution(s)\n% and the vector value at the final point tf.  Note that the\n% IVPs do not need to be linear or homogeneous.\n\n%% Lines to change:\n\n% Line 28   : t0 - Start time\n% Line 31   : tf - End time\n% Line 34   : N  - Number of subdivisions\n% Line 37   : y0 - Vector of initial values\n% Line 115+ : Which functions to plot, remembering to assign\n%             a colour, texture and legend label\n% Line 135+ : Set of differential equations written\n%             explicitly.  These can also be non-linear and\n%             include forcing terms.  These equations can\n%             also be written in matrix form if the\n%             equations are linear.\n\n%% Set up input values\n\n% Start time\nt0=0;\n\n% End time\ntf=1;\n\n% Number of subdivisions\nN=50;\n\n% Column vector initial values y0=y(t0)\ny0=[0;1;0];\n\n%% Set up IVP solver parameters\n\n% T = Vector of times t0,t1,...,tN.\n% This is generated using linspace which splits the\n% interval [t0,tf] into N+1 points (or N subintervals)\nT=linspace(t0,tf,N+1);\n\n% Stepsize\nh=(tf-t0)/N;\n\n% Number of differential equations\nK=length(y0);\n\n%% Perform the Modified Euler iteration\n\n% Y = Solution matrix\n% The matrix Y will contain K rows and N+1 columns.  Every\n% row corresponds to a different IVP and every column\n% corresponds to a different time.  So the matrix Y will\n% take the following form:\n% y_1(t_0)  y_1(t_1)  y_1(t_2)  ...  y_1(t_N)\n% y_2(t_0)  y_2(t_1)  y_2(t_2)  ...  y_2(t_N)\n% ...\n% y_K(t_0)  y_K(t_1)  y_K(t_2)  ...  y_K(t_N)\nY=zeros(K,N+1);\n\n% The first column of the vector Y is the initial vector y0\nY(:,1)=y0;\n\n% Set the current time t to be the starting time t0 and the\n% current value of the vector y to be the strtaing values y0\nt=t0;\ny=y0;\n\nfor n=2:1:N+1\n\n    % Prediction Step:\n    % Use the Euler iteration to obtain an appromxation for\n    % the derivatives at the current time step\n\n    dydt=DYDT(t,y,K);     % Find gradient at the current step\n    y_pred=y+h*dydt;   % Predict y at current time step\n\n    % Corrector Step:\n    % Use the Modified Euler to correct y_pred\n\n    dydt_pred=DYDT(t,y_pred,K);    % Predict the gradient\n                                % from the predicted y\n    y=y+0.5*h*(dydt+dydt_pred); % Find y at the current step\n\n    t=T(n);            % Update the new time\n\n    Y(:,n)=y;          % Replace row n in Y with y\n\nend\n\n%% Setting plot parameters\n\n% Clear figure\nclf\n\n% Hold so more than one line can be drawn\nhold on\n\n% Turn on grid\ngrid on\n\n% Setting font size and style\nset(gca,'FontSize',20,'FontName','Times')\n\n% Label the axes\nxlabel('$t$','Interpreter','Latex')\nylabel('$\\mathbf{y}(t)$','Interpreter','Latex')\n\n% Plot the desried solutions.  If all the solutions are\n% needed, then consider using a for loop in that case\nplot(T,Y(1,:),'-b','LineWidth',2)\nplot(T,Y(2,:),'-r','LineWidth',2)\nplot(T,Y(3,:),'-k','LineWidth',2)\n\n% Legend labels\nlegend('$y_1(t)$','$y_2(t)$','$y_3(t)$')\nset(legend,'Interpreter','Latex')\n\n% Display the values of the vector y at tf\ndisp(strcat('The vector y at t=',num2str(tf),' is:'))\ndisp(Y(:,end))\n\nend\n\nfunction [dydt]=DYDT(t,y,K)\n\n% When the equation are written in explicit form\n\ndydt=zeros(K,1);\n\ndydt(1)=2*y(1)+y(2)+y(3)+cos(t);\n\ndydt(2)=sin(y(1))+exp(-y(2)+y(3));\n\ndydt(3)=y(1)*y(2)-y(3);\n\n% If the set of equations is linear, then these can be\n% written in matrix form as dydt=A*y+b(t).  For example, if\n% the set of equations is:\n% dudt = 7u - 2v +  w + exp(t)\n% dvdt = 2u + 3v - 9w + cos(t)\n% dwdt =      2v + 5w + 2\n% Then:\n% A=[7,-2,1;2,3,-9;0,2,5];\n% b=@(t) [exp(t);cos(t);2];\n% dydt=A*y+b(t)\n\nend",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Modified Euler Method</span>"
    ]
  },
  {
    "objectID": "09_IVP_RK4.html",
    "href": "09_IVP_RK4.html",
    "title": "5  The Runge-Kutta Method",
    "section": "",
    "text": "5.1 MATLAB Code\nThe following MATLAB code performs the fourth order Runge-Kutta iteration for the following set of IVPs on the interval \\([0,1]\\): \\[\\begin{align*}\n& \\frac{\\mathrm{d} u}{\\mathrm{d} t}=2u+v+w+\\cos(t), & \\quad u(0)=0 \\\\\n& \\frac{\\mathrm{d} v}{\\mathrm{d} t}=\\sin(u)+\\mathrm{e}^{-v+w}, & \\quad v(0)=1 \\\\\n& \\frac{\\mathrm{d} w}{\\mathrm{d} t}=uv-w, & \\quad w(0)=0.\n\\end{align*}\\]",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Runge-Kutta Method</span>"
    ]
  },
  {
    "objectID": "09_IVP_RK4.html#matlab-code",
    "href": "09_IVP_RK4.html#matlab-code",
    "title": "5  The Runge-Kutta Method",
    "section": "",
    "text": "Linearity\n\n\n\nNote that this code is built for a general case that does not have to be linear even though the entire derivation process was built on the fact that the system is linear.\n\n\nfunction IVP_RK4\n\n%% Solve a set of first order IVPs using RK4\n\n% This code solves a set of IVP when written explicitly\n% on the interval [t0,tf] subject to the initial conditions\n% y(0)=y0.  The output will be the graph of the solution(s)\n% and the vector value at the final point tf.  Note that the\n% IVPs do not need to be linear or homogeneous.\n\n%% Lines to change:\n\n% Line 28   : t0 - Start time\n% Line 31   : tf - End time\n% Line 34   : N  - Number of subdivisions\n% Line 37   : y0 - Vector of initial values\n% Line 109+ : Which functions to plot, remembering to assign\n%             a colour, texture and legend label\n% Line 129+ : Set of differential equations written\n%             explicitly.  These can also be non-linear and\n%             include forcing terms.  These equations can\n%             also be written in matrix form if the\n%             equations are linear.\n\n%% Set up input values\n\n% Start time\nt0=0;\n\n% End time\ntf=1;\n\n% Number of subdivisions\nN=50;\n\n% Column vector initial values y0=y(t0)\ny0=[0;1;0];\n\n%% Set up IVP solver parameters\n\n% T = Vector of times t0,t1,...,tN.\n% This is generated using linspace which splits the\n% interval [t0,tf] into N+1 points (or N subintervals)\nT=linspace(t0,tf,N+1);\n\n% Stepsize\nh=(tf-t0)/N;\n\n% Number of differential equations\nK=length(y0);\n\n%% Perform the RK4 iteration\n\n% Y = Solution matrix\n% The matrix Y will contain K rows and N+1 columns.  Every\n% row corresponds to a different IVP and every column\n% corresponds to a different time.  So the matrix Y will\n% take the following form:\n% y_1(t_0)  y_1(t_1)  y_1(t_2)  ...  y_1(t_N)\n% y_2(t_0)  y_2(t_1)  y_2(t_2)  ...  y_2(t_N)\n% ...\n% y_K(t_0)  y_K(t_1)  y_K(t_2)  ...  y_K(t_N)\nY=zeros(K,N+1);\n\n% The first column of the vector Y is the initial vector y0\nY(:,1)=y0;\n\n% Set the current time t to be the starting time t0 and the\n% current value of the vector y to be the strtaing values y0\nt=t0;\ny=y0;\n\nfor n=2:1:N+1\n\n    % Determine the coefficients of RK4\n\n    K1=DYDT(t,y,K);\n    K2=DYDT(t+h/2,y+h*K1/2,K);\n    K3=DYDT(t+h/2,y+h*K2/2,K);\n    K4=DYDT(t+h,y+h*K3,K);\n    y=y+(h/6)*(K1+2*K2+2*K3+K4);\n\n    t=T(n);            % Update the new time\n\n    Y(:,n)=y;          % Replace row n in Y with y\n\nend\n\n%% Setting plot parameters\n\n% Clear figure\nclf\n\n% Hold so more than one line can be drawn\nhold on\n\n% Turn on grid\ngrid on\n\n% Setting font size and style\nset(gca,'FontSize',20,'FontName','Times')\n\n% Label the axes\nxlabel('$t$','Interpreter','Latex')\nylabel('$\\mathbf{y}(t)$','Interpreter','Latex')\n\n% Plot the desried solutions.  If all the solutions are\n% needed, then consider using a for loop in that case\nplot(T,Y(1,:),'-b','LineWidth',2)\nplot(T,Y(2,:),'-r','LineWidth',2)\nplot(T,Y(3,:),'-k','LineWidth',2)\n\n% Legend labels\nlegend('$y_1(t)$','$y_2(t)$','$y_3(t)$')\nset(legend,'Interpreter','Latex')\n\n% Display the values of the vector y at tf\ndisp(strcat('The vector y at t=',num2str(tf),' is:'))\ndisp(Y(:,end))\n\nend\n\nfunction [dydt]=DYDT(t,y,K)\n\n% When the equation are written in explicit form\n\ndydt=zeros(K,1);\n\ndydt(1)=2*y(1)+y(2)+y(3)+cos(t);\n\ndydt(2)=sin(y(1))+exp(-y(2)+y(3));\n\ndydt(3)=y(1)*y(2)-y(3);\n\n% If the set of equations is linear, then these can be\n% written in matrix form as dydt=A*y+b(t).  For example, if\n% the set of equations is:\n% dudt = 7u - 2v +  w + exp(t)\n% dvdt = 2u + 3v - 9w + cos(t)\n% dwdt =      2v + 5w + 2\n% Then:\n% A=[7,-2,1;2,3,-9;0,2,5];\n% b=@(t) [exp(t);cos(t);2];\n% dydt=A*y+b(t)\n\nend",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Runge-Kutta Method</span>"
    ]
  },
  {
    "objectID": "10_IVP_InBuilt.html",
    "href": "10_IVP_InBuilt.html",
    "title": "6  MATLAB’s In-Built Procedures",
    "section": "",
    "text": "So far, the three main iterative methods have been developed that solve IVPs numerically. MATLAB, however, has its own built-in procedures that can solve IVPs with a combination of several methods. The two main ones are ode23 (which uses a combination of a second and third order RK methods) and ode45 (which uses a combination of a fourth and fifth order RK methods).\nBoth ode45 and ode23 are hybrid methods and use adaptive meshing, this means that the time span grid is not necessarily uniform, but it changes depending on the gradients; if the gradient is large at some point, then the stepsize will be small to capture these drastic changes.\nThe following MATLAB code solves the following set of IVPs on the interval \\([0,1]\\) using ode45: \\[\\begin{align*}\n& \\frac{\\mathrm{d} u}{\\mathrm{d} t}=2u+v+w+\\cos(t), & \\quad u(0)=0 \\\\\n& \\frac{\\mathrm{d} v}{\\mathrm{d} t}=\\sin(u)+\\mathrm{e}^{-v+w}, & \\quad v(0)=1 \\\\\n& \\frac{\\mathrm{d} w}{\\mathrm{d} t}=uv-w, & \\quad w(0)=0.\n\\end{align*}\\]\nfunction IVP_InBuilt\n\n%% Solve a set of first order IVPs using In-Built codes\n\n% This code solves a set of IVP when written explicitly\n% on the interval [t0,tf] subject to the initial conditions\n% y(0)=y0.  The output will be the graph of the solution(s)\n% and the vector value at the final point tf.  Note that the\n% IVPs do not need to be linear or homogeneous.\n\n%% Lines to change:\n\n% Line 28   : t0 - Start time\n% Line 31   : tf - End time\n% Line 43   : T_Span - Time span for evaluation\n% Line 46   : y0 - Vector of initial values\n% Line 85+  : Which functions to plot, remembering to assign\n%             a colour, texture and legend label\n% Line 105+ : Set of differential equations written\n%             explicitly.  These can also be non-linear and\n%             include forcing terms.  These equations can\n%             also be written in matrix form if the\n%             equations are linear.\n\n%% Set up input values\n\n% Start time\nt0=0;\n\n% End time\ntf=1;\n\n% Time span\n% In-built methods tend to use adaptive meshing; decreasing\n% the stepsize near locations with drastic derivative\n% changes and increasing near small derivative changes.\n% Sometimes this is not desired but a uniform meshing is\n% requiredfrom the start time t0 to the end time tf being\n% split into N equal sub intervals.  This can be changed\n% here:\n% Adaptive meshing:  T_Span=[t0 tf]\n% Specific meshing:  T_Span=linspace(t0,tf,N)\nT_Span=[t0 tf];\n\n% Column vector initial values y0=y(t0)\ny0=[0;1;0];\n\n%% Set up IVP solver parameters\n\n% Number of differential equations\nK=length(y0);\n\n%% Use solver\n\n% Set the solver tolerance\ntol=odeset('RelTol',1e-6);\n\n% Solve the IVP using ode45 or ode23\n[T,Y]=ode45(@(t,y) DYDT(t,y,K),T_Span,y0,tol);\n\n% Convert T and Y to columns for consistency\nT=T';\nY=Y';\n\n%% Setting plot parameters\n\n% Clear figure\nclf\n\n% Hold so more than one line can be drawn\nhold on\n\n% Turn on grid\ngrid on\n\n% Setting font size and style\nset(gca,'FontSize',20,'FontName','Times')\n\n% Label the axes\nxlabel('$t$','Interpreter','Latex')\nylabel('$\\mathbf{y}(t)$','Interpreter','Latex')\n\n% Plot the desried solutions.  If all the solutions are\n% needed, then consider using a for loop in that case\nplot(T,Y(1,:),'-b','LineWidth',2)\nplot(T,Y(2,:),'-r','LineWidth',2)\nplot(T,Y(3,:),'-k','LineWidth',2)\n\n% Legend labels\nlegend('$y_1(t)$')\nset(legend,'Interpreter','Latex')\n\n% Display the values of the vector y at tf\ndisp(strcat('The vector y at t=',num2str(tf),' is:'))\ndisp(Y(:,end))\n\nend\n\nfunction [dydt]=DYDT(t,y,K)\n\n% When the equation are written in explicit form\n\ndydt=zeros(K,1);\n\ndydt(1)=2*y(1)+y(2)+y(3)+cos(t);\n\ndydt(2)=sin(y(1))+exp(-y(2)+y(3));\n\ndydt(3)=y(1)*y(2)-y(3);\n\n% If the set of equations is linear, then these can be\n% written in matrix form as dydt=A*y+b(t).  For example, if\n% the set of equations is:\n% dudt = 7u - 2v +  w + exp(t)\n% dvdt = 2u + 3v - 9w + cos(t)\n% dwdt =      2v + 5w + 2\n% Then:\n% A=[7,-2,1;2,3,-9;0,2,5];\n% b=@(t) [exp(t);cos(t);2];\n% dydt=A*y+b(t)\n\nend",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>MATLAB's In-Built Procedures</span>"
    ]
  },
  {
    "objectID": "11_IVP_Implicit.html",
    "href": "11_IVP_Implicit.html",
    "title": "7  Implicit IVP Solvers",
    "section": "",
    "text": "7.1 Backwards Euler Method\nConsider the Euler method at the starting time \\(t=t_0\\). The value of the function \\(y\\) at \\(t_1=t_0+h\\) is approximated by \\[\n    y(t_1) \\approx Y_1=Y_0+hy'(t_0)\n\\] and this gives an upper bound for a stable stepsize of \\[\n    h_0=2\\min\\left(\\frac{|\\Re(\\lambda_k)|}{|\\lambda_k|^2}\\right)\n\\] in order to ensure that the Euler method is computationally stable. However, suppose that this modified slightly by using the gradient at \\(y(t_1)\\) rather than at \\(y(t_0)\\), in other words, suppose that the value of \\(y\\) at \\(t_1\\) is approximated by \\[\n    y(t_1) \\approx Y_1=Y_0+h\\underline{\\underline{y'(t_1)}}.\n\\] This approach is known as the Backwards Euler Method and is an implicit procedure since the value of \\(y'(t_1)\\) is not known to begin with.\nThe general formulation is as follows: Consider the system of differential equations \\[\\boldsymbol{y}'=A\\boldsymbol{y}+\\boldsymbol{b}(t) \\quad \\text{with} \\quad \\boldsymbol{y}(0)=\\boldsymbol{y}_0, \\quad x \\in [t_0,t_f].\\] Discretise the interval \\([t_0,t_f]\\) into \\(N\\) equal subintervals, each with width \\(h=\\frac{t_f-t_0}{N}\\). At the time step \\(t=t_n=t_0+nh\\), the backwards Euler method is \\[\\boldsymbol{Y}_{n+1}=\\boldsymbol{Y}_n+h\\boldsymbol{y}'(t_{n+1})=\\boldsymbol{Y}_n+h\\left[ A\\boldsymbol{Y}_{n+1}+\\boldsymbol{b}(t_{n+1}) \\right].\\] This can be rearranged to give \\[(I-hA)\\boldsymbol{Y}_{n+1}=\\boldsymbol{Y}_n+h\\boldsymbol{b}(t_{n+1}).\\]\nRearranging further fives the basis for the Backwards Euler iteration which is \\[\\boldsymbol{Y}_{n+1}=(I-hA)^{-1}\\left[\\boldsymbol{Y}_n+h\\boldsymbol{b}(t_{n+1})\\right]\\] whereas the standard Euler method in matrix form is \\[\\boldsymbol{Y}_{n+1}=(I+hA)\\boldsymbol{Y}_n+h\\boldsymbol{g}(t_{n}).\\] The Euler method requires explicit calculations using matrix multiplications but the backwards Euler method requires matrix inversion instead.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implicit IVP Solvers</span>"
    ]
  },
  {
    "objectID": "11_IVP_Implicit.html#stability-of-the-backwards-euler-method",
    "href": "11_IVP_Implicit.html#stability-of-the-backwards-euler-method",
    "title": "7  Implicit IVP Solvers",
    "section": "7.2 Stability of the Backwards Euler Method",
    "text": "7.2 Stability of the Backwards Euler Method\nConsider the initial value problem in its scalar form \\[\\frac{\\mathrm{d} y}{\\mathrm{d} t}=\\lambda y+b(t) \\quad \\text{with} \\quad y(0)=y_0.\\] The backwards Euler method at the time \\(t=t_{n+1}=t_0+(n+1)h\\) gives \\[Y_{n+1}=(1-h \\lambda)^{-1}\\left[Y_n+hg(t_{n+1})\\right].\\] This initial condition can be perturbed by adding a small parameter \\(\\varepsilon\\neq 0\\) to give the perturbed differential equation \\[\\frac{\\mathrm{d} z}{\\mathrm{d} t}=\\lambda z+g(t) \\quad \\text{with} \\quad z(0)=y_0+\\varepsilon.\\] The backwards Euler then yields \\[Z_{n+1}=(1-h \\lambda)^{-1}\\left[Z_n+hg(t_{n+1})\\right]\\] The differential equations in \\(Y\\) and \\(Z\\) can be subtracted to give a perturbation term \\(E\\) where \\[E_{n+1}=Z_{n+1}-Y_{n+1}=(1-h \\lambda)^{-1}\\left[Z_n-Y_n\\right]=(1-h \\lambda)^{-1}E_n.\\] Notice that once again, the forcing function \\(g(t)\\) has been eliminated and therefore does not affect the stability of the backwards Euler method. The differential equation for \\(E\\) will have the initial condition \\(E_0=Z_0-Y_0=\\varepsilon\\). This expression can be used to represent \\(E_n\\) in terms of \\(\\varepsilon\\) recursively as: \\[\\begin{multline*}\nE_n=(1-h\\lambda)^{-1}E_{n-1}=(1-h\\lambda)^{-2}E_{n-2}\\\\\n=\\dots=(1-h\\lambda)^{-(n-1)}E_1=(1-h\\lambda)^{-n}E_0=(1-h\\lambda)^{-n}\\varepsilon.\n\\end{multline*}\\] \\[\\implies \\quad E_n=(1-h\\lambda)^{-n}\\varepsilon.\\] This means that the method is stable for stepsizes \\(h\\) that satisfy \\(|1-h\\lambda|&gt;1\\) and since \\(\\lambda&lt;0\\) for an asymptotically stable system, then this inequality is always satisfied. This means that the backwards Euler method is stable for all stepsizes \\(h&gt;0\\), no matter how large.\n\n\n\n\n\n\nBackwards Euler Method\n\n\n\nConsider the differential equation \\[\n    y'=-100y+100\\sin(t) \\quad \\text{with} \\quad y(0)=1.\n\\] In this case, \\(\\lambda&lt;0\\) meaning that this differential equation is asymptotic stable. The maximum allowable stepsize for the Euler method is \\(h_0=\\frac{2}{|-100|}=0.02\\). However, the backwards Euler method is stable for any stepsize \\(h\\) as seen below (very large stepsizes will still converge but they will not give any useful information).\n\n\n\nThe formulation presented above also holds for sets of differential equations in the same way with one difference. Instead of having \\((1-h\\lambda)^{-1}=\\frac{1}{1-h\\lambda}\\), the procedure for systems will require the matrix inverse \\((1-\\lambda A)^{-1}\\) or the MATLAB backslash operator can be used instead.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implicit IVP Solvers</span>"
    ]
  },
  {
    "objectID": "11_IVP_Implicit.html#order-of-accuracy",
    "href": "11_IVP_Implicit.html#order-of-accuracy",
    "title": "7  Implicit IVP Solvers",
    "section": "7.3 Order of Accuracy",
    "text": "7.3 Order of Accuracy\nThe backwards Euler method is numerically stable for all values the stepsize \\(h\\) and has the same order of accuracy as the Euler method, i.e. the local truncation error is of \\(\\mathcal{O}\\left(h^2\\right)\\) while the global integration error is of \\(\\mathcal{O}\\left(h\\right)\\). However, this increased stability comes at a cost, the backwards Euler methods requires double the computational cost compared to the Euler method.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implicit IVP Solvers</span>"
    ]
  },
  {
    "objectID": "11_IVP_Implicit.html#stiff-differential-equations",
    "href": "11_IVP_Implicit.html#stiff-differential-equations",
    "title": "7  Implicit IVP Solvers",
    "section": "7.4 Stiff Differential Equations",
    "text": "7.4 Stiff Differential Equations\nStiff sets of differential equations with a large value of the total computational cost \\(N_0\\) can be very difficult to solve numerically using explicit methods but implicit methods can work very well. MATLAB has its very own built-in stiff differential equation solver under the command ode15s and this can be implemented exactly as ode45. This solves sets of differential equations implicitly using numerical differentiation of orders 1 to 5.\n\n\n\n\n\n\nStiff IVPs\n\n\n\nConsider the set of differential equations on the interval \\([0,3500]\\) \\[\\begin{align*}\n& \\frac{\\mathrm{d} y_1}{\\mathrm{d} t}=y_2 & y_1(0)=2 \\\\\n& \\frac{\\mathrm{d} y_2}{\\mathrm{d} t}=1000(1-y_1^2)y_2-y_1 & y_2(0)=0.\n\\end{align*}\\]\nThis is a very stiff set of differential equations, solving this using ode45 takes upwards of 92 seconds while solving using the stiff solver ode15s requires a mere 0.233 seconds (depending on you machine). The result of solving this differential equation is shown below for \\(y_1(t)\\) only since \\(y_2(t)\\) takes very large values and this distorts the graphical interpretation.\n\nUsing the stiff solver optimises the stepsizes for stiff regions. Particularly, if a region is deemed to be considerably “stiff”, the ode15s will use smaller stepsizes to solve the problem but if there is a region where the differential is not “stiff”, then larger stepsizes will be used. Therefore, ode15s usually requires fewer grid points overall, for instance to solve the above set of differential equations, ode15s only requires 1,836 grid points while ode45 requires 7,820,485 grid points, that is over 4,200 times more grid points than ode15s. This just goes to show that stiff differential need implicit methods, even though the cost for every step is greater than that of an explicit method, fewer steps are required in total.\nAn alternative stiff differential equation solver is ode23s which achieves that same outcome as ode15s but with a lower accuracy and more grid points using only second and third order methods.",
    "crumbs": [
      "Solving Initial Value Problems",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implicit IVP Solvers</span>"
    ]
  },
  {
    "objectID": "12_BVP.html",
    "href": "12_BVP.html",
    "title": "8  Boundary Value Problems",
    "section": "",
    "text": "8.1 Example of Boundary Value Problems\nConsider a mass \\(m\\) hanging from a spring with spring constant \\(K\\). Suppose that the spring is extended (by pulling the mass) by a distance \\(x\\) as seen below.\nThen by Hooke’s Law, the force pulling the mass back to its equilibrium position is given by \\[F=-Kx.\\] As the mass is released, it will accelerate upwards with an acceleration \\(a\\) and the force responsible for this acceleration is given by Newton’s Second Law of Motion \\[F=ma.\\] The acceleration \\(a\\) is the second derivative of the displacement \\(x\\) with respect to time and since it acts in a direction opposite to the extension, then \\[a=-\\frac{\\mathrm{d}^{2} x}{\\mathrm{d} t^{2}} \\quad \\implies \\quad F=-m\\frac{\\mathrm{d}^{2} x}{\\mathrm{d} t^{2}}\\] Equating the two expressions for the force from Newton’s Second Law and Hooke’s Law will give \\[-Kx=-m\\frac{\\mathrm{d}^{2} x}{\\mathrm{d} t^{2}} \\quad \\implies \\quad\\frac{\\mathrm{d}^{2} x}{\\mathrm{d} t^{2}}+\\omega^2x=0 \\quad \\text{where} \\quad \\omega=\\sqrt{\\frac{K}{m}}.\\] This differential equation represents the simple harmonic motion of a mass hanging on a frictionless massless spring which oscillates with a frequency \\(\\omega\\). Since this is a second order differential equation, two conditions need to be imposed:",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "12_BVP.html#example-of-boundary-value-problems",
    "href": "12_BVP.html#example-of-boundary-value-problems",
    "title": "8  Boundary Value Problems",
    "section": "",
    "text": "Initial conditions can be imposed at the starting time, specifically \\(x(0)\\) and \\(x'(0)\\) which prescribe the initial position and initial speed,\nBoundary conditions can be imposed at different times, say \\(x(0)\\) and \\(x(10)\\) which prescribe the location at time \\(t=0\\) and time \\(t=10\\).",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "12_BVP.html#sec-FDM",
    "href": "12_BVP.html#sec-FDM",
    "title": "8  Boundary Value Problems",
    "section": "8.2 Finite Difference Method for Boundary Value Problems",
    "text": "8.2 Finite Difference Method for Boundary Value Problems\nConsider the general second order boundary value problem \\[a(x) \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}+b(x) \\frac{\\mathrm{d} u}{\\mathrm{d} x}+c(x) u=f(x) \\quad \\text{with} \\quad 0&lt; x &lt; L\\] \\[\\text{and} \\quad u(0)=u_l, \\quad u(L)=u_r\\] where the functions \\(a,b,c\\) and \\(f\\) are known functions of \\(x\\). Boundary value problems like this are solved using an incredibly versatile method known as the Finite Difference Method. This procedure essentially changes a differential equation into a set of difference equations by using approximations to the derivatives.",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "12_BVP.html#existence-uniqueness-of-solutions-to-bvps",
    "href": "12_BVP.html#existence-uniqueness-of-solutions-to-bvps",
    "title": "8  Boundary Value Problems",
    "section": "8.3 Existence & Uniqueness of Solutions to BVPs",
    "text": "8.3 Existence & Uniqueness of Solutions to BVPs\nConsider the differential equation for the undamped simple harmonic oscillator with frequency 1, namely \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} t^{2}}=-u.\\] This differential equation has the general analytic solution \\[u(t)=C_1 \\cos(t)+C_2 \\sin(t)\\] where \\(C_1\\) and \\(C_2\\) are constants of integration which will be determined form the boundary conditions.\nThree qualitatively different sets of boundary conditions will be investigated:\n\n\\(u(0)=1\\) and \\(u(\\frac{5\\pi}{2})=-1\\): The constants \\(C_1\\) and \\(C_2\\) can be found as: \\[1=u(0)=C_1 \\cos(0)+C_2 \\sin(0)=C_1 \\quad \\implies \\quad C_1=1\\] \\[-1=u\\left(\\frac{5\\pi}{2}\\right)=C_1 \\cos\\left(\\frac{5\\pi}{2}\\right)+C_2 \\sin\\left(\\frac{5\\pi}{2}\\right)=C_2 \\quad \\implies \\quad C_2=-1.\\] Therefore the analytic solution to the boundary value problem subject to these conditions is \\[u(t)=\\cos(t)-\\sin(t)\\] and this is captured by the finite difference approximation. In this case, the solution to the boundary value problem exists and is unique.\n\\(u(0)=0\\) and \\(u(2\\pi)=0\\): The constants \\(C_1\\) and \\(C_2\\) can be found as: \\[0=u(0)=C_1 \\cos(0)+C_2\\sin(0)=C_1 \\quad \\implies \\quad C_1=0\\] \\[0=u(2\\pi)=C_1 \\cos(2\\pi)+C_2\\sin(2\\pi)=C_1 \\quad \\implies \\quad C_1=0.\\] These two conditions provide an expressions for the constant \\(C_1\\) only and not \\(C_2\\), therefore the particular solution will be \\[u(t)=C_2\\sin(t)\\] which is valid for any value of \\(C_2\\). Therefore in this case, the solution exists but is not unique.\n\\(u(0)=1\\) and \\(u(2\\pi)=-1\\): The constants \\(C_1\\) and \\(C_2\\) can be found as: \\[1=u(0)=C_1 \\cos(0)+C_2\\sin(0)=C_1 \\quad \\implies \\quad C_1=1\\] \\[-1=u(\\pi)=C_1 \\cos(2\\pi)+C_2\\sin(2\\pi)=C_1 \\quad \\implies \\quad C_1=-1.\\] In this case, the boundary values have resulted in a contradiction and therefore the solution does not exist when subject to these boundary conditions.\n\nThis final case is when the solution to a boundary value problem does not exist.\n\n8.3.1 Finite Difference Approximations to the Derivatives\nThe term finite difference approximation refers to how derivatives can be approximated using linear expressions. For instance, the derivative of some function \\(f\\) at a given point \\(X\\) can be approximated as the gradient of \\(f\\) between two points around \\(X\\), for example \\[\\frac{\\mathrm{d} f}{\\mathrm{d} x}(X) \\approx \\frac{f(X+h)-f(X-h)}{2h}.\\] There are many other ways in which these approximations can be made depending on the way in which the grid has been set up or on the context of the problem.\nConsider a general unknown function \\(u(x)\\) defined on \\([0,L]\\) where \\(u(0)\\) and \\(u(L)\\) are given (as boundary conditions). First, split the interval into \\(N\\) equally sized sections, each of width \\(h\\), and label the points as \\(x_0, x_1, \\dots, x_N\\) where \\(x_n=nh\\).\n\nFor first and second derivatives, there are three main approximations that are most widely used:\n\nForward Difference: \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{u(x_{n+1})-u(x_n)}{h}\\] \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n) \\approx \\frac{u(x_{n+2})-2u(x_{n+1})+u(x_n)}{h^2}\\]\nBackward Difference: \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{u(x_{n})-u(x_{n-1})}{h}\\] \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n) \\approx \\frac{u(x_{n})-2u(x_{n-1})+u(x_{n-2})}{h^2}\\]\nCentred Difference: \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{u(x_{n+1})-u(x_{n-1})}{2h}\\] \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n) \\approx \\frac{u(x_{n+1})-2u(x_{n})+u(x_{n-1})}{h^2}\\]\n\nThe graphical interpretation of the approximations to the first derivatives are shown below.\n\n\n\n\n\n\n\nSecond Derivative Expression\n\n\n\nTo show how the second derivative expressions are obtained, consider the centred difference approximation \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{u(x_{n+1})-u(x_{n-1})}{2h}.\\]\nTo derive the expression for the second derivative, introduce two fictitious points \\(x_{n-0.5}\\) (which is half-way between \\(x_{n-1}\\) and \\(x_{n}\\)) and \\(x_{n+0.5}\\) (which is half-way between \\(x_{n}\\) and \\(x_{n+1}\\)). Then \\[\\begin{align*}\n    \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n)=\\frac{\\mathrm{d} }{\\mathrm{d} x}\\left( \\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\right) &\\approx \\frac{\\mathrm{d} }{\\mathrm{d} x} \\left( \\frac{u(x_{n+0.5})-u(x_{n-0.5})}{h} \\right) \\\\ & \\approx \\frac{u'(x_{n+0.5})-u'(x_{n-0.5})}{h} \\\\ & \\approx \\frac{\\frac{u(x_{n+1})-u(x_{n})}{h}-\\frac{u(x_{n})-u(x_{n-1})}{h}}{h} \\\\ & = \\frac{u(x_{n+1})-2u(x_n)+u(x_{n-1})}{h^2}.\n\\end{align*}\\]\nThe derivation of the second derivative approximations for the forward and backward differences can be done in a very similar way but without the need for half steps.\n\n\nAny of these three approximations can be used to approximate the derivatives of the function \\(u\\) at the point \\(x_n\\). Denote the approximation of \\(u\\) at the point \\(x_n\\) by \\(U_n\\), i.e. \\(U_n \\approx u(x_n)\\), then\n\nForward Difference: \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{U_{n+1}-U_n}{h} \\quad ; \\quad \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n) \\approx \\frac{U_{n+2}-2U_{n+1}+U_n}{h^2}\\]\nBackward Difference: \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{U_{n}-U_{n-1}}{h} \\quad ; \\quad \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n) \\approx \\frac{U_{n}-2U_{n-1}+U_{n-2}}{h^2}\\]\nCentred Difference: \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n) \\approx \\frac{U_{n+1}-U_{n-1}}{2h} \\quad ; \\quad \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n) \\approx \\frac{U_{n+1}-2U_{n}+U_{n-1}}{h^2}.\\]\n\nThese approximations will form the basis for solving the BVP.\n\n\n8.3.2 Discretisation of the Differential Equation\nReturning to the differential equation \\[a(x) \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}+b(x) \\frac{\\mathrm{d} u}{\\mathrm{d} x}+c(x) u=f(x).\\] Evaluate this equation at \\(x=x_n\\) for some \\(n\\), then \\[a(x_n) \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}(x_n)+b(x_n) \\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_n)+c(x_n) u(x_n)=f(x_n).\\]\nFor now, suppose the centred differencing approximation is used to approximate the derivatives. Replacing the approximations of the derivatives of \\(u\\) at \\(x_n\\) gives \\[a(x_n) \\frac{U_{n+1}-2U_n+U_{n-1}}{h^2}+b(x_n)  \\frac{U_{n+1}-U_{n-1}}{2h}+c(x_n) U_n=f(x_n).\\] This can be simplified by collecting the \\(U\\) terms resulting in: \\[\\alpha_n U_{n-1}+\\beta_n U_n+\\gamma_n U_{n+1}=f(x_n)\\] \\[\\text{where} \\quad \\alpha_n=\\frac{a(x_n)}{h^2}-\\frac{b(x_n)}{2h}, \\quad \\beta_n=-\\frac{2a(x_n)}{h^2}+c(x_n), \\quad \\gamma_n=\\frac{a(x_n)}{h^2}+\\frac{b(x_n)}{2h}.\\] This expression will hold for all the values of \\(n=1, 2, \\dots, N-1\\) (otherwise there will be points \\(x_{-1}\\) and \\(x_{N+1}\\) which are outside the domain \\([0,L]\\)). Therefore, this mean that there will be \\(N-1\\) equations in \\(N+1\\) unknowns which are \\(U_0, U_1, U_2, \\dots, U_N\\).\nThis system may seem to be undetermined however, there are two boundary conditions that have not been taken into consideration yet, namely \\(u(x_0)=u_l\\) and \\(u(x_N)=u(L)=u_r\\). Since these are known, the approximations \\(U_0\\) and \\(U_N\\) have defined values, i.e. \\(U_0 \\approx u(x_0)=u_l\\) and \\(U_N=u(x_N)=u_r\\). This eliminates two of the unknowns giving \\(N-1\\) equations in \\(N-1\\) unknowns.\nAt \\(n=1\\), the approximation to the differential equation is \\[\\alpha_1 U_{0}+\\beta_1 U_1+\\gamma_1 U_{2}=f(x_1)\\] and since \\(U_0\\) is already known, then it can be taken to the right hand side to give \\[\\beta_1 U_1+\\gamma_1 U_{2}=f(x_1)-\\alpha_1 u_{0}.\\] Similarly, at \\(n=N-1\\), the approximation is \\[\\alpha_{N-1} U_{N-2}+\\beta_{N-1} U_{N-1}+\\gamma_{N-1} U_{N}=f(x_{N-1})\\] and since \\(U_N\\) is known, this can be rewritten as \\[\\alpha_{N-1} U_{N-2}+\\beta_{N-1} U_{N-1}=f(x_{N-1})-\\gamma_{N-1} u_{L}.\\] For \\(n=2,3,\\dots,N-2\\), the approximation is \\[\\alpha_n U_{n-1}+\\beta_n U_n+\\gamma_n U_{n+1}=f(x_n)\\] where \\(U_{n-1}, U_n\\) and \\(U_{n+1}\\) are al unknown. In summary, all of these \\(N-1\\) equations are: \\[\\begin{align*}\n    n=1: & \\quad \\; \\, {\\color{white}\\alpha_1 U_0+}\\beta_1 U_1+\\gamma_1 U_{2}=f(x_1)-\\alpha_1 u_{0} \\\\\n    n=2: & \\quad \\alpha_2 U_{1}+\\beta_2 U_2+\\gamma_2 U_{3}=f(x_2) \\\\\n    n=3: & \\quad \\alpha_3 U_{2}+\\beta_3 U_3+\\gamma_3 U_{4}=f(x_3) \\\\\n     & \\qquad \\qquad \\qquad \\vdots \\\\\n     n=N-3: & \\quad \\alpha_{N-3} U_{N-4}+\\beta_{N-3} U_{N-3}+\\gamma_{N-3} U_{N-2}=f(x_{N-3})\\\\\n     n=N-2: & \\quad \\alpha_{N-2} U_{N-3}+\\beta_{N-2} U_{N-2}+\\gamma_{N-2} U_{N-1}=f(x_{N-2})\\\\\n    n=N-1: & \\quad \\alpha_{N-1} U_{N-2}+\\beta_{N-1} U_{N-1}{\\color{white}}=f(x_{N-1})-\\gamma_{N-1} u_{L}\n\\end{align*}\\] These can be written in matrix form as \\(A \\boldsymbol{U}=\\boldsymbol{g}\\), namely \\[\\begin{multline*}\n\\underbrace{\\begin{pmatrix}\n    \\beta_1  & \\gamma_1 & 0        & 0 & \\dots  & 0 & 0 & 0 & 0 \\\\\n    \\alpha_2 & \\beta_2  & \\gamma_2 & 0 & \\dots  & 0 & 0 & 0 & 0 \\\\\n    0        & \\alpha_3 & \\beta_3  & \\gamma_3 & \\dots  & 0 & 0 & 0 & 0 \\\\\n    0 & 0       & \\alpha_4 & \\beta_4  & \\dots  & 0 & 0 & 0 & 0 \\\\\n    \\vdots   & \\vdots   & \\vdots   & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n    0        & 0        & 0        & 0 & \\dots  & \\beta_{N-4} & \\gamma_{N-4} & 0 & 0 \\\\\n    0        & 0        & 0        & 0 & \\dots  & \\alpha_{N-3} & \\beta_{N-3} & \\gamma_{N-3} & 0 \\\\\n    0        & 0        & 0        & 0 & \\dots  & 0 & \\alpha_{N-2} & \\beta_{N-2} & \\gamma_{N-2} \\\\\n    0        & 0        & 0        & 0 & \\dots  & 0 & 0 & \\alpha_{N-1} & \\beta_{N-1}\n\\end{pmatrix}}_{A}\n\\underbrace{\\begin{pmatrix}\n    U_1 \\\\ U_2 \\\\ U_3 \\\\ U_4 \\\\ \\vdots \\\\ U_{N-4} \\\\ U_{N-3} \\\\ U_{N-2} \\\\ U_{N-1}\n\\end{pmatrix}}_{\\boldsymbol{U}} \\\\=\n\\underbrace{\\begin{pmatrix}\n    f(x_1)-\\alpha_1 u_l \\\\ f(x_2) \\\\ f(x_3) \\\\ f(x_4) \\\\ \\vdots \\\\ f(x_{N-4}) \\\\ f(x_{N-3}) \\\\ f(x_{N-2}) \\\\ f(x_{N-1})-\\gamma_{N-1}u_r\n\\end{pmatrix}}_{\\boldsymbol{g}}.\n\\end{multline*}\\] The matrix \\(A\\) is of size \\((N-1) \\times (N-1)\\) all of whose terms are known, the vector \\(\\boldsymbol{g}\\) of size \\((N-1) \\times 1\\) also has terms that are all known. The unknown vector here is \\(\\boldsymbol{U}\\) and it can be found by inverting \\(A\\) to give \\(\\boldsymbol{U}=A^{-1}\\boldsymbol{g}\\).\nCarrying out matrix inversions by hand can become increasingly cumbersome if \\(A\\) is larger than \\(2 \\times 2\\) and therefore this process should be done computationally. This can be solved in MATLAB by using either U=inv(A)*g or U=A\\g. The backslash method is faster than explicit matrix inversion if the matrix is of a large size.\nThe same process can be done for the forward and backward differencing approximations as well.\n\n\n8.3.3 Steps of The Finite Difference Method\nIn summary, these are the steps of the finite difference method:\n\nDivide the interval \\([0,L]\\) into \\(N\\) equally sized sections, each of width \\(h=\\frac{L}{N}\\) and label the points as \\(x_0, x_1, x_2, \\dots, x_N\\) where \\(x_n=nh\\). \nThe values of the function \\(u\\) are to be found at all the locations \\(x_n\\). Denote the approximation to the function \\(u\\) at the point \\(x_n\\) by \\(U_n\\), i.e. \\(U_n \\approx u(x_n)\\) for all \\(n=0,1,2,\\dots,N\\).\nEvaluate the differential equation at all the points \\(x_n\\) where the derivatives are replaced by their finite difference approximations.\nThis will result in a set of \\(N-1\\) linear equations in \\(N+1\\) unknowns, namely, \\(U_0, U_1, U_2, \\dots, U_N\\).\nThe values for \\(U_0\\) and \\(U_N\\) are known from the boundary conditions, since \\(U_0=u(0)=u_l\\) and \\(U_N=u(L)=u_r\\) and no approximation is needed since the exact values are known.\nWrite the whole system of equations in the matrix form \\(A\\boldsymbol{U}=\\boldsymbol{g}\\) and solve using MATLAB’s backlash operator.\n\n\n\n\n\n\n\nBVP Example\n\n\n\nConsider the boundary value problem \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}=x^3, \\quad x \\in[0,2] \\quad \\text{with} \\quad u(0)=0 \\quad \\text{and} \\quad u(2)=1.\\] The differential equation itself can be solved analytically to give \\[u(x)=\\frac{1}{20}x^5-\\frac{3}{10}x.\\] This example will be used for the purposes of demonstration and comparison between the numerically obtained solution and the exact solution.\nSuppose the interval \\([0,2]\\) is to be divided into \\(5\\) equally sized sections, therefore \\(N=5\\) and \\(h=\\frac{L}{N}=\\frac{2}{5}=0.4\\). \nThe functions \\(a(x), b(x), c(x)\\) and \\(f(x)\\) in this interval are: \\[a(x)=1, \\quad b(x)=0, \\quad c(x)=0, \\quad f(x)=x^3.\\]\nThe matrix values are \\[\\alpha_n=\\frac{a(x_n)}{h^2}-\\frac{b(x_n)}{2h}=6.25\\] \\[\\beta_n=-\\frac{2a(x_n)}{h^2}+c(x_n)=-12.5\\] \\[\\gamma_n=\\frac{a(x_n)}{h^2}+\\frac{b(x_n)}{2h}=6.25.\\]\nThese can be used to obtain expressions for the matrix \\(A\\) and the vector \\(\\boldsymbol{g}\\) as \\[\n    A=\\begin{pmatrix}\n    -12.5  &  6.25  &  0     & 0     \\\\\n     6.25  & -12.5  &  6.25  & 0     \\\\\n     0     &  6.25  & -12.5  & 6.25  \\\\\n     0     &  0     &  6.25  & -12.5\n    \\end{pmatrix}, \\quad \\boldsymbol{g}=\\begin{pmatrix}\n    0.064 \\\\ 0.512 \\\\ 1.728 \\\\ 4.096\n    \\end{pmatrix}.\n\\] This system can be solved using U=inv(A)*g or U=A\\g. The numerical solution is compared to exact solution below. \nThe advantage of using this boundary value solver is that the computations are in no way taxing on MATLAB. The system that results is composed entirely of linear equations and this system is solvable (provided the boundary value problem does indeed have a solution which may not always be possible). MATLAB’s backslash operator is very effective in dealing with matrices, especially owing to the fact that the matrix \\(A\\) is a tridiagonal matrix.",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "12_BVP.html#matlab-code",
    "href": "12_BVP.html#matlab-code",
    "title": "8  Boundary Value Problems",
    "section": "8.4 MATLAB Code",
    "text": "8.4 MATLAB Code\nBelow is the MATLAB code that solves the BVP \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}+2\\frac{\\mathrm{d} u}{\\mathrm{d} t}+\\mathrm{e}^{-x}u=\\sin(x), \\quad x \\in[0,10] \\quad \\text{with} \\quad u(0)=1 \\quad \\text{and} \\quad u(10)=-1\\] using the centred differencing method with \\(N=50\\).\nfunction BVP_CD\n\n%% Solve BVPs using centered differences\n\n% The bvp is written in the form\n% a(x) u'' + b(x) u' + c(x) u = f(x) on x in [x0,L]\n% with the boundary conditions u(x0)=ul and u(L)=ur.\n% After the centered difference approximation is\n% used, the system will be written in the form AU=g.\n\n%% Lines to change:\n\n% Line 26  : x0 - Start point\n% Line 29  : L  - End point\n% Line 32  : N  - Number of subdivisions\n% Line 35  : xl - Left boundary value\n% Line 38  : xr - Right boundary value\n% Line 119 : Expression for the function a(x)\n% Line 127 : Expression for the function b(x)\n% Line 135 : Expression for the function c(x)\n% Line 143 : Expression for the function f(x)\n\n%% Set up input values\n\n% Start point\nx0=0;\n\n% End point\nL=10;\n\n% Number of subdivisions\nN=50;\n\n% Boundary value at x=x0\nul=1;\n\n% Boundary value at x=L\nur=-1;\n\n%% Set up BVP solver parameters\n\n% Interval width\nh=(L-x0)/N;\n\n% X = Vector of locations\n% (x1, x2, x3, ..., xN) (notice the start is x1 NOT x0)\nX=x0+h:h:L;\n\n% Evaluate the functions a(x), b(x), c(x) and f(x) at X\naX=a(X);\nbX=b(X);\ncX=c(X);\nfX=f(X);\n\n% Find the expressions for alpha, beta and gamma at X\nalpha=aX/(h^2)-bX/(2*h);\nbeta=-2*aX/(h^2)+cX;\ngamma=aX/(h^2)+bX/(2*h);\n\n% Set up the vector g on the right hand side\ng=zeros(N-1,1);\ng(1)=fX(1)-alpha(1)*ul;\ng(N-1)=fX(N-1)-gamma(N-1)*ur;\nfor j=2:1:N-2\n    g(j)=fX(j);\nend\n\n% Set up the matrix A on the left hand side (LHS_A is\n% to avoid confusion with the function a(x))\nA=zeros(N-1,N-1);\nA(1,1)=beta(1);\nA(1,2)=gamma(1);\nA(N-1,N-1)=beta(N-1);\nA(N-1,N-2)=alpha(N-1);\nfor j=2:1:N-2\n    A(j,j-1)=alpha(j);\n    A(j,j)=beta(j);\n    A(j,j+1)=gamma(j);\nend\n\n% Solve for the unknown vector U (it is then readjusted\n% from a column vector to a row vector for plotting)\nU=A\\g;\nU=U';\n\n% Add the missing term x0 to the start of the vector x\nX=[x0,X];\n\n% Add the left and right boundary values to the vector U\nU=[ul,U,ur];\n\n%% Setting plot parameters\n\n% Clear figure\nclf\n\n% Hold so more than one line can be drawn\nhold on\n\n% Turn on grid\ngrid on\n\n% Setting font size and style\nset(gca,'FontSize',20,'FontName','Times')\n\n% Label the axes\nxlabel('$t$','Interpreter','Latex')\nylabel('$u(t)$','Interpreter','Latex')\n\n% Plot solution\nplot(X,U,'-k','LineWidth',2)\n\nend\n\nfunction [A]=a(X)\nA=zeros(size(X));\nfor i=1:1:length(X)\n    x=X(i);\n    A(i)=1;\nend\nend\n\nfunction [B]=b(X)\nB=zeros(size(X));\nfor i=1:1:length(X)\n    x=X(i);\n    B(i)=2;\nend\nend\n\nfunction [C]=c(X)\nC=zeros(size(X));\nfor i=1:1:length(X)\n    x=X(i);\n    C(i)=exp(-x);\nend\nend\n\nfunction [F]=f(X)\nF=zeros(size(X));\nfor i=1:1:length(X)\n    x=X(i);\n    F(i)=sin(x);\nend\nend",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "12_BVP.html#comparison-between-forward-backward-centred-difference-approximations",
    "href": "12_BVP.html#comparison-between-forward-backward-centred-difference-approximations",
    "title": "8  Boundary Value Problems",
    "section": "8.5 Comparison Between Forward, Backward & Centred Difference Approximations",
    "text": "8.5 Comparison Between Forward, Backward & Centred Difference Approximations\nThe main difference between the different differencing schemes if the order of accuracy. Indeed, the error of the forward and backward differencing methods are \\(\\mathcal{O}\\left(h\\right)\\) whereas the error for the centred differencing is \\(\\mathcal{O}\\left(h^2\\right)\\). This means that if the stepsize \\(h\\) was reduced by a factor of 10, then the error for the forward and backward finite difference approximations would also reduce by a factor of 10 while the centred would reduce by a factor of 100.\n\n\n\n\n\n\nComparison\n\n\n\nConsider the BVP \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}=25\\pi^2 \\sin(5\\pi x), \\quad x \\in[0,1] \\quad \\text{with} \\quad u(0)=0 \\quad \\text{and} \\quad u(1)=0.\\]\nThis has the exact solution \\[u(x)=-\\sin(5 \\pi x).\\] Below are the plots for the numerical solution to this boundary value problem using the forward (red), backward (blue) and centred (green) difference approximations compared to the exact solution when \\(N=10\\). \nIt can be seen that even for this relatively crude interval subdivision of \\(N=10\\), the centred approximation has yielded a far more favourable result compared to the other two methods. The following table shows the 2-norm error between the exact solution and the approximation for different values of \\(N\\):\n\n\n\nMethod\n\\(N=10\\)\n\\(N=20\\)\n\\(N=50\\)\n\\(N=100\\)\n\n\n\n\nForward\n4.1444\n3.0875\n1.9823\n1.4048\n\n\nBackward\n4.7243\n3.8535\n2.0939\n1.4251\n\n\nCentred\n0.5226\n0.1677\n0.0413\n0.0146\n\n\n\nIt can be seen that even when \\(N=100\\), the 2-norm error has still not reduced below 1 for the forward and backward difference approximations but the centred has already achieved that even at \\(N=10\\). This is just a demonstration to show that how a simple change in the way in which derivatives are approximated can have such a drastic effect on the final solution.",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "12_BVP.html#matlabs-in-built-procedures",
    "href": "12_BVP.html#matlabs-in-built-procedures",
    "title": "8  Boundary Value Problems",
    "section": "8.6 MATLAB’s In-Built Procedures",
    "text": "8.6 MATLAB’s In-Built Procedures\nMATLAB has an in-built mechanism that can also solve second (or even higher order) BVPs, this is done using the bvp4c command.\nBelow is the MATLAB code that solves the BVP \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}+2\\frac{\\mathrm{d} u}{\\mathrm{d} t}+\\mathrm{e}^{-x}u=\\sin(x), \\quad x \\in[0,10] \\quad \\text{with} \\quad u(0)=1 \\quad \\text{and} \\quad u(10)=-1\\] using bvp4c.\nfunction BVP_InBuilt\n\n%% Solve BVPs using bvp4c\n\n% The bvp is written in the form\n% a(x) u'' + b(x) u' + c(x) u = f(x) on x in [x0,L]\n% with the boundary conditions u(x0)=ul and u(L)=ur.\n\n%% Lines to change:\n\n% Line 24 : x0 - Start point\n% Line 27 : L  - End point\n% Line 30 : N  - Number of spatial points\n% Line 33 : xl - Left boundary value\n% Line 36 : xr - Right boundary value\n% Line 44 : Expression for the function a(x)\n% Line 45 : Expression for the function b(x)\n% Line 46 : Expression for the function c(x)\n% Line 47 : Expression for the function f(x)\n\n%% Set up input values\n\n% Start point\nx0=0;\n\n% End point\nL=10;\n\n% Number of spatial points\nN=50;\n\n% Boundary value at x=x0\nul=1;\n\n% Boundary value at x=L\nur=-1;\n\n%% Set up BVP solver parameters\n\n% Set up solving space\nX=linspace(x0,L,N);\n\n% Define the functions in the BVP\na= @(x) 1;\nb= @(x) 2;\nc= @(x) exp(-x);\nf= @(x) sin(x);\n\n%% Set up BVP solving parameters\n\n% First, write the second order ODE as a set of first order\n% ODEs:\n% U'=V\n% V'=(-b(x)*V-c(x)*U+f(x))/a(x)\n\n% Second order BVPs can have more than one solution\n% and vector v is the initialising vector of solutions.\n% It can be kept as a vector of zeros\nv=[0 0];\n\n% Initialise vectors for space and v\ninit=bvpinit(X,v);\n\n% Solve the bvp subject to the boundary values and\n% inital guesses\nsol=bvp4c(@(x,u) DUDT(x,u,a,b,c,f),@(x0,L) BCs(x0,L,ul,ur),init);\n\n% Evaluate the solution at the grid points\nU=deval(sol,X);\n\n% Convert U to columns for consistency\nU=U';\n\n%% Setting plot parameters\n\n% Clear figure\nclf\n\n% Hold so more than one line can be drawn\nhold on\n\n% Turn on grid\ngrid on\n\n% Setting font size and style\nset(gca,'FontSize',20,'FontName','Times')\n\n% Label the axes\nxlabel('$t$','Interpreter','Latex')\nylabel('$u(t)$','Interpreter','Latex')\n\n% Plot solution\nplot(X,U(:,1),'-k','LineWidth',2)\n\nend\n\nfunction [dudx]=DUDT(x,u,a,b,c,f)\n\ndudx(1)=u(2);\n\ndudx(2)=(-b(x)*u(2)-c(x)*u(1)+f(x))/(a(x));\n\nend\n\nfunction res=BCs(x0,L,ul,ur)\n% The boundary conditions are written as\n% u(x0)=ul\n% x(L)=ur\n\nres=[x0(1)-ul;L(1)-ur];\n\nend",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Boundary Value Problems</span>"
    ]
  },
  {
    "objectID": "13_MVP.html",
    "href": "13_MVP.html",
    "title": "9  Mixed Value Problems",
    "section": "",
    "text": "9.1 Finite Difference Method for MVPs\nConsider the differential equation \\[a(x) \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}+b(x) \\frac{\\mathrm{d} u}{\\mathrm{d} x}+c(x) u=f(x) \\quad \\text{with} \\quad 0&lt; x &lt; L\\] as before. The interval \\([0,L]\\) will be split into \\(N\\) equally sized sections each of width \\(h=\\frac{L}{N}\\) and the grid points are labelled \\(x_n=nh\\) for \\(n=0,1,2,\\dots,N\\). This differential equation can be discretised using the centred difference approximation just as before to give \\[\\alpha_n U_{n-1}+\\beta_n U_n+\\gamma_n U_{n+1}=f(x_n) \\quad \\text{for} \\quad n=1, 2, \\dots, N-1\\] \\[\\text{where} \\quad \\alpha_n=\\frac{a(x_n)}{h^2}-\\frac{b(x_n)}{2h}, \\quad \\beta_n=-\\frac{2a(x_n)}{h^2}+c(x_n), \\quad \\gamma_n=\\frac{a(x_n)}{h^2}+\\frac{b(x_n)}{2h}.\\] This gives a set of \\(N-1\\) equations in \\(N+1\\) unknowns, namely \\(U_0, U_1, U_2, \\dots, U_N\\) (recall that \\(U_n \\approx u(x_n)\\) for \\(n=0, 1, 2, \\dots, N\\)).\nWhen the differential equation is subjected to two boundary conditions, say \\[u(0)=u_l \\quad \\text{and} \\quad u(L)=u_r,\\] then expressions for \\(U_0\\) and \\(U_L\\) are provided which gives \\(N-1\\) equations in \\(N-1\\) unknowns, hence resulting in a well-defined system which can be solved as before.\nHowever, suppose that a set of mixed conditions is given as \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(0)=\\tilde{u}_l \\quad \\text{and} \\quad u(L)=u_r.\\] In this case, only \\(U_N \\approx u(L)=u_r\\) is explicitly known, meaning that there will be \\(N-1\\) equations in \\(N\\) unknowns since \\(U_0 \\approx u(x_0)\\) is not known giving an under-determined system (a system with more unknowns than equations). So either one more equation is needed or one more unknown needs to be removed. All the unknowns are certainly needed, otherwise the solution will be incomplete, so the alternative is to find another equation to add to the set of equations.\nThe set of \\(N-1\\) equations is: \\[\\begin{align*}\n    n=1: & \\quad \\alpha_1 U_0+\\beta_1 U_1+\\gamma_1 U_{2}=f(x_1) \\\\\n    n=2: & \\quad \\alpha_2 U_{1}+\\beta_2 U_2+\\gamma_2 U_{3}=f(x_2) \\\\\n     & \\qquad \\qquad \\qquad \\vdots \\\\\n    n=N-1: & \\quad \\alpha_{N-1} U_{N-2}+\\beta_{N-1} U_{N-1}=f(x_{N-1})-\\gamma_{N-1} u_{L}.\n\\end{align*}\\] All these come from the discretisation \\[\\alpha_n U_{n-1}+\\beta_n U_n+\\gamma_n U_{n+1}=f(x_n).\\] Evaluating this at \\(n=0\\) gives \\[\\alpha_0 U_{-1}+\\beta_0 U_0+\\gamma_0 U_{1}=f(x_0). \\tag{9.1}\\] Initially, this may seem to be quite strange since there is a point \\(U_{-1}\\) which is the approximation to the solution \\(u\\) at the point \\(x=x_{-1}=-h\\) which is certainly out of the range of consideration. This point is considered to be an artificial grid point that will act as a placeholder in meantime.\nConsider the condition at the start point \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(0)=\\tilde{u}_l.\\] Using the centred finite difference approximation on the derivative gives \\[\\tilde{u}_l=\\frac{\\mathrm{d} u}{\\mathrm{d} x}(0)=\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_0) \\approx \\frac{u(x_{1})-u(x_{-1})}{2h} \\approx \\frac{U_{1}-U_{-1}}{2h} \\quad \\implies \\quad \\frac{U_{1}-U_{-1}}{2h} \\approx \\tilde{u}_l\\] This approximation can be manipulated to provide an expression for the artificial point \\(U_{-1}\\) as \\[U_{-1}=U_{1}-2h\\tilde{u}_l.\\] Replacing this into the equation Equation 9.1 will eliminate \\(U_{-1}\\) completely giving an equation in terms of \\(U_0\\) and \\(U_1\\) only, namely \\[\\beta_0 U_0+(\\gamma_0+\\alpha_0) U_{1}=f(x_0)+2h\\tilde{u}_l\\alpha_0.\\] Therefore, another equation has been found which now completes the system of \\(N\\) equations in \\(N\\) unknowns. Thus the system of equations is: \\[\\begin{align*}\n    n=0: & \\quad \\beta_0 U_0+(\\gamma_0+\\alpha_0) U_{1}=f(x_0)+2h\\tilde{u}_l\\alpha_0 \\\\\n    n=1: & \\quad \\alpha_1 U_0+\\beta_1 U_1+\\gamma_1 U_{2}=f(x_1) \\\\\n    n=2: & \\quad \\alpha_2 U_{1}+\\beta_2 U_2+\\gamma_2 U_{3}=f(x_2) \\\\\n     & \\qquad \\qquad \\qquad \\vdots \\\\\n    n=N-1: & \\quad \\alpha_{N-1} U_{N-2}+\\beta_{N-1} U_{N-1}=f(x_{N-1})-\\gamma_{N-1} u_{L}.\n\\end{align*}\\] This can be written in matrix form as \\(A\\boldsymbol{U}=\\boldsymbol{g}\\) where \\[\\begin{multline*}\n\\underbrace{\\begin{pmatrix}\n    \\beta_0  & \\gamma_0+\\alpha_0 & 0        & \\dots  & 0 & 0 & 0 \\\\\n    \\alpha_1 & \\beta_1  & \\gamma_1 & \\dots  & 0 & 0 & 0 \\\\\n    0        & \\alpha_2 & \\beta_2  & \\dots  & 0 & 0 & 0 \\\\\n    \\vdots   & \\vdots   & \\vdots   & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n    0        & 0        & 0        & \\dots  & \\beta_{N-3} & \\gamma_{N-3} & 0 \\\\\n    0        & 0        & 0        & \\dots  & \\alpha_{N-2} & \\beta_{N-2} & \\gamma_{N-2} \\\\\n    0        & 0        & 0        & \\dots  & 0 & \\alpha_{N-1} & \\beta_{N-1}\n\\end{pmatrix}}_{A}\n\\underbrace{\\begin{pmatrix}\n    U_0 \\\\ U_1 \\\\ U_2 \\\\ \\vdots \\\\ U_{N-3} \\\\ U_{N-2} \\\\ U_{N-1}\n\\end{pmatrix}}_{\\boldsymbol{U}}=\\\\\n\\underbrace{\\begin{pmatrix}\n    f(x_0)+2h\\alpha_0 \\tilde{u}_l \\\\ f(x_1) \\\\ f(x_2) \\\\ \\vdots \\\\ f(x_{N-3}) \\\\ f(x_{N-2}) \\\\ f(x_{N-1})-\\gamma_{N-1}u_r\n\\end{pmatrix}}_{\\boldsymbol{g}}.\n\\end{multline*}\\] This can once again be solved on MATLAB using U=inv(A)*g or U=A\\g.\nIf, on the other hand, the mixed conditions were instead \\[u(0)=u_l \\quad \\text{and} \\quad\\frac{\\mathrm{d} u}{\\mathrm{d} x}(L)=\\tilde{u}_r,\\] then the artificial point will be located at \\(x=x_{N+1}\\) but the same procedure can be done give the matrix system \\(A \\boldsymbol{U}=\\boldsymbol{g}\\) where \\[\\begin{multline*}\n\\underbrace{\\begin{pmatrix}\n    \\beta_1  & \\gamma_1 & 0        & \\dots  & 0 & 0 & 0 \\\\\n    \\alpha_2 & \\beta_2  & \\gamma_2 & \\dots  & 0 & 0 & 0 \\\\\n    0        & \\alpha_3 & \\beta_3  & \\dots  & 0 & 0 & 0 \\\\\n    \\vdots   & \\vdots   & \\vdots   & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n    0        & 0        & 0        & \\dots  & \\beta_{N-2} & \\gamma_{N-2} & 0 \\\\\n    0        & 0        & 0        & \\dots  & \\alpha_{N-1} & \\beta_{N-1} & \\gamma_{N-1} \\\\\n    0        & 0        & 0        & \\dots  & 0 & \\alpha_{N}+\\gamma_N & \\beta_{N}\n\\end{pmatrix}}_{A}\n\\underbrace{\\begin{pmatrix}\n    U_1 \\\\ U_2 \\\\ U_3 \\\\ \\vdots \\\\ U_{N-2} \\\\ U_{N-1} \\\\ U_{N}\n\\end{pmatrix}}_{\\boldsymbol{U}}=\\\\\n\\underbrace{\\begin{pmatrix}\n    f(x_1)-\\alpha_1 u_l \\\\ f(x_2) \\\\ f(x_3) \\\\ \\vdots \\\\ f(x_{N-2}) \\\\ f(x_{N-1}) \\\\ f(x_{N})-2h\\gamma_{N}\\tilde{u}_r\n\\end{pmatrix}}_{\\boldsymbol{g}}.\n\\end{multline*}\\]",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mixed Value Problems</span>"
    ]
  },
  {
    "objectID": "13_MVP.html#finite-difference-method-for-mvps",
    "href": "13_MVP.html#finite-difference-method-for-mvps",
    "title": "9  Mixed Value Problems",
    "section": "",
    "text": "Mixed Value Problem\n\n\n\nConsider the differential equation for a damped harmonic oscillator \\[\\frac{\\mathrm{d}^{2} u}{\\mathrm{d} t^{2}}+0.5\\frac{\\mathrm{d} u}{\\mathrm{d} t}+u=0 \\quad \\text{for} \\quad 0&lt;t&lt;2\\pi\\] with the mixed conditions \\[\\frac{\\mathrm{d} u}{\\mathrm{d} t}(0)=1 \\quad \\text{and} \\quad u(2\\pi)=0.\\] This MVP is to determine the trajectory of the mass if the launching speed at the start is \\(1\\), which is \\(\\frac{\\mathrm{d} u}{\\mathrm{d} t}(0)=1\\), and after \\(2\\pi\\) seconds, the mass reaches its equilibrium state, which is \\(u(2\\pi)=0\\). Notice that there is no restriction on the starting location, only the starting speed, so the mass can start anywhere as long as it is launched with a velocity \\(1\\).  The starting location here happens to be at \\(0.2188\\) but that is no restricted by the mixed conditions as long as the gradient at the start is \\(1\\).",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mixed Value Problems</span>"
    ]
  },
  {
    "objectID": "14_SBVP.html",
    "href": "14_SBVP.html",
    "title": "10  Symmetric Boundary Conditions",
    "section": "",
    "text": "10.1 Finite Difference Method for Symmetric Boundary Value Problems\nThis problem can be tackled in a very similar way to BVPs and MVPs. Consider the differential equation \\[a(x) \\frac{\\mathrm{d}^{2} u}{\\mathrm{d} x^{2}}+b(x) \\frac{\\mathrm{d} u}{\\mathrm{d} x}+c(x) u=f(x) \\quad \\text{with} \\quad -L&lt; x &lt; L.\\] The interval \\([-L,L]\\) will be split into \\(N\\) equally sized sections each of width \\(h=\\frac{2L}{N}\\) and the grid points are labelled \\(x_n=-L+nh\\) for \\(n=0,1,2,\\dots,N\\). This differential equation can be discretised using the centred difference approximation (just as in Section 8.2) to give \\[\\alpha_n U_{n-1}+\\beta_n U_n+\\gamma_n U_{n+1}=f(x_n) \\quad \\text{for} \\quad n=1, 2, \\dots, N-1\\] \\[\\text{where} \\quad \\alpha_n=\\frac{a(x_n)}{h^2}-\\frac{b(x_n)}{2h}, \\quad \\beta_n=-\\frac{2a(x_n)}{h^2}+c(x_n), \\quad \\gamma_n=\\frac{a(x_n)}{h^2}+\\frac{b(x_n)}{2h}.\\] This gives a set of \\(N-1\\) equations in \\(N+1\\) unknowns, namely \\(U_0, U_1, U_2, \\dots, U_N\\). In this case, neither \\(U_0\\) nor \\(U_N\\) are explicitly known, therefore none of the unknowns can be eliminated from the boundary conditions per se.\nSuppose the given conditions are \\[u(-L)=u(L) \\quad \\text{and} \\quad \\frac{\\mathrm{d} u}{\\mathrm{d} x}(L)=pu(L)+q\\] where \\(p\\) and \\(q\\) are some constants. The first condition is the symmetric boundary condition which represents the fact that the value of the unknown solution \\(u\\) at both ends is the same, then \\(U_0=U_N\\), even though neither is explicitly known. The term \\(U_0\\) can be eliminated since determining \\(U_N\\) automatically determines \\(U_0\\), this reduces the number of unknowns to \\(N\\).\nConsider the discretisation at \\(n=1\\), namely \\[\\alpha_1 U_{0}+\\beta_1 U_1+\\gamma_1 U_{2}=f(x_1),\\] since \\(U_0=U_N\\), this can be rewritten in terms of \\(U_N\\) instead as \\[\\beta_1 U_1+\\gamma_1 U_{2}+\\alpha_1 U_{N}=f(x_1).\\]\nThe discretised form of the differential equation at \\(n=N\\) is \\[\\alpha_N U_{N-1}+\\beta_N U_N+\\gamma_N U_{N+1}=f(x_N). \\tag{10.1}\\] Just as in the case with the MVPs, an artificial point \\(U_{N+1}\\) is introduced which is the solution approximated at the point \\(x=x_{N+1}=L+h\\) which is beyond the computational domain.\nTo find an expression for \\(U_{N+1}\\), first consider the second condition \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_N)=\\frac{\\mathrm{d} u}{\\mathrm{d} x}(L) pu(L)+q \\approx pU_N+q.\\] The LHS can be rewritten in terms of its centred differencing approximation as \\[\\frac{\\mathrm{d} u}{\\mathrm{d} x}(x_N) \\approx \\frac{u(x_{N+1})-u(x_{N-1})}{2h} \\approx \\frac{U_{N+1}-U_{N-1}}{2h}.\\]\nCombining these two can give an expression for \\(U_{N+1}\\) as: \\[\\frac{U_{N+1}-U_{N-1}}{2h} \\approx pU_N+q \\quad \\implies \\quad U_{N+1}=U_{N-1}+ 2hpU_N+2hq.\\]\nReplacing this into Equation 10.1 gives \\[(\\alpha_N+\\gamma_N)U_{N-1}+(\\beta_N+2hp\\gamma_N)U_N=f(x_N)-2hq\\gamma_N,\\] thus providing the last equation to complete the set. Finally, this system can be written in matrix form as \\(A\\boldsymbol{U}=\\boldsymbol{g}\\) where \\[\\begin{multline*}\n\\underbrace{\\begin{pmatrix}\n    \\beta_1  & \\gamma_1 & 0        & \\dots  & 0 & 0 & \\alpha_1 \\\\\n    \\alpha_2 & \\beta_2  & \\gamma_2 & \\dots  & 0 & 0 & 0 \\\\\n    0        & \\alpha_3 & \\beta_3  & \\dots  & 0 & 0 & 0 \\\\\n    \\vdots   & \\vdots   & \\vdots   & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n    0        & 0        & 0        & \\dots  & \\beta_{N-2} & \\gamma_{N-2} & 0 \\\\\n    0        & 0        & 0        & \\dots  & \\alpha_{N-1} & \\beta_{N-1} & \\gamma_{N-1} \\\\\n    0        & 0        & 0        & \\dots  & 0 & \\alpha_{N}+\\gamma_N & \\beta_{N}+2hp\\gamma_N\n\\end{pmatrix}}_{A}\n\\underbrace{\\begin{pmatrix}\n    U_1 \\\\ U_2 \\\\ U_3 \\\\ \\vdots \\\\ U_{N-2} \\\\ U_{N-1} \\\\ U_{N}\n\\end{pmatrix}}_{\\boldsymbol{U}}=\\\\\n\\underbrace{\\begin{pmatrix}\n    f(x_1) \\\\ f(x_2) \\\\ f(x_3) \\\\ \\vdots \\\\ f(x_{N-2}) \\\\ f(x_{N-1}) \\\\ f(x_{N})-2hq\\gamma_{N}\n\\end{pmatrix}}_{\\boldsymbol{g}}.\n\\end{multline*}\\] This can then be solved in MATLAB but bearing in mind that \\(U_0=U_N\\) which determines the function \\(U\\) at \\(-L\\) and \\(L\\).",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Symmetric Boundary Conditions</span>"
    ]
  },
  {
    "objectID": "14_SBVP.html#finite-difference-method-for-symmetric-boundary-value-problems",
    "href": "14_SBVP.html#finite-difference-method-for-symmetric-boundary-value-problems",
    "title": "10  Symmetric Boundary Conditions",
    "section": "",
    "text": "Symmetric Boundary Value Problem\n\n\n\nConsider the conduction problem \\[-\\frac{\\mathrm{d}^{2} T}{\\mathrm{d} x^{2}}=40\\sin(x) \\quad \\text{in} \\quad -1&lt; x &lt; 1\\] with the conditions \\[T(-1)=T(1) \\quad \\text{and} \\quad \\frac{\\mathrm{d} T}{\\mathrm{d} x}(1)=\\frac{1}{2}(T(1)-25).\\]",
    "crumbs": [
      "Solving Boundary Value Problems",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Symmetric Boundary Conditions</span>"
    ]
  },
  {
    "objectID": "15_PDE.html",
    "href": "15_PDE.html",
    "title": "11  Heat Equation",
    "section": "",
    "text": "11.1 The Method of Lines for the Heat Equation\nThe outline of the method of lines for the heat equation is as follows:\nIn essence, the Method of Lines has converted a PDE into a set of ODEs using the same techniques as BVPs and will be solved in the same way as IVPs.",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  },
  {
    "objectID": "15_PDE.html#the-method-of-lines-for-the-heat-equation",
    "href": "15_PDE.html#the-method-of-lines-for-the-heat-equation",
    "title": "11  Heat Equation",
    "section": "",
    "text": "Divide the spatial interval \\([0,L]\\) into \\(N_x\\) equally sized sections and label the points as \\(x_0, x_1, x_2, \\dots, x_N\\) where \\(x_n=nh\\) and the spatial interval width is \\(h_x=\\frac{L}{N}\\).\n\n\n\nLeft Hand Side: For each point \\(x_n\\), define the approximation \\(U_n(t) \\approx u(x_n,t)\\). Therefore the left hand side of the heat equation can be written as \\[\\frac{\\partial u}{\\partial t}(x_n,t) \\approx \\frac{\\mathrm{d} U_n}{\\mathrm{d} t}(t)\\] and this holds for \\(n=1, 2, \\dots, N-1\\) since \\(U_0(t) \\approx u(0,t)=u_l(t)\\) and \\(U_N(t) \\approx u(L,t)=u_r(t)\\) are already known from the boundary conditions. Notice that the derivative of \\(U_n\\) is an ordinary derivative since \\(U_n\\) is a function of \\(t\\) only.\nRight Hand Side: Use the finite difference approximation to approximate the spatial derivative in the differential equation. Here, the centred difference approximation for the second derivative will be used, namely \\[\\frac{\\partial^{2} u}{\\partial x^{2}}(x_n,t) \\approx \\frac{U_{n+1}(t)-2U_{n}(t)+U_{n-1}(t)}{h_x^2}.\\] Therefore the right hand side of the heat equation will become \\[\\alpha \\frac{\\partial^{2} u}{\\partial x^{2}}(x_n,t) \\approx \\frac{\\alpha}{h_x^2}\\left[ U_{n-1}(t)-2U_n(t)+U_{n+1}(t) \\right].\\] This holds for \\(n=1, 2, \\dots, N-1\\) bearing in mind, once again, that \\(U_0(t) \\approx u(0,t)=u_l(t)\\) and \\(U_N(t) \\approx u(L,t)=u_r(t)\\) are known beforehand.\nThese can be combined to give the discretised form of the heat equation \\[\\frac{\\mathrm{d} U_n}{\\mathrm{d} t}=\\frac{\\alpha}{h_x^2}\\left[ U_{n-1}-2U_n+U_{n+1} \\right]\\] for all \\(n=1,2,\\dots,N-1\\) where \\(U_n=U_n(t)\\). This means that the partial differential equation has been split into \\(N-1\\) ordinary differential equations.\nThis entire system of \\(N-1\\) equations can now be written in matrix form as \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\[\\begin{multline*}\n\\frac{\\mathrm{d} }{\\mathrm{d} t}\\underbrace{\\begin{pmatrix}\nU_1(t) \\\\\nU_2(t) \\\\\nU_3(t) \\\\\n\\vdots \\\\\nU_{N-3}(t) \\\\\nU_{N-2}(t) \\\\\nU_{N-1}(t) \\\\\n\\end{pmatrix}}_{\\boldsymbol{U}}=\n\\underbrace{\\frac{\\alpha}{h_x^2}\\begin{pmatrix}\n-2 & 1 & 0 & \\dots & 0 & 0 & 0 \\\\\n1 & -2 & 1 & \\dots & 0 & 0 & 0 \\\\\n0 & 1 & -2 & \\dots & 0 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\dots & -2 & 1 & 0 \\\\\n0 & 0 & 0 & \\dots & 1 & -2 & 1 \\\\\n0 & 0 & 0 & \\dots & 0 & 1 & -2 \\\\\n\\end{pmatrix}}_{A}\n\\underbrace{\\begin{pmatrix}\nU_1(t) \\\\\nU_2(t) \\\\\nU_3(t) \\\\\n\\vdots \\\\\nU_{N-3}(t) \\\\\nU_{N-2}(t) \\\\\nU_{N-1}(t) \\\\\n\\end{pmatrix}}_{\\boldsymbol{U}} \\\\\n+\\underbrace{\\frac{\\alpha}{h_x^2}\\begin{pmatrix}\nu_l(t) \\\\\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0 \\\\\n0 \\\\\nu_r(t) \\\\\n\\end{pmatrix}}_{\\boldsymbol{b}}\n\\end{multline*}\\] subject to the initial condition \\[\n\\boldsymbol{U}_0=\\begin{pmatrix}\nU_1(0) \\\\ U_2(0) \\\\ U_3(0) \\\\ \\vdots \\\\ U_{N-3}(0) \\\\ U_{N-2}(0) \\\\ U_{N-1}(0)\n\\end{pmatrix}\\approx\\begin{pmatrix}\nu(x_1,0) \\\\ u(x_2,0) \\\\ u(x_3,0) \\\\ \\vdots \\\\ u(x_{N-3},0) \\\\ u(x_{N-2},0) \\\\ u(x_{N-1},0)\n\\end{pmatrix} =\\begin{pmatrix}\nu_{init}(x_1) \\\\\nu_{init}(x_2) \\\\\nu_{init}(x_3) \\\\\n\\vdots \\\\\nu_{init}(x_{N-3}) \\\\\nu_{init}(x_{N-2}) \\\\\nu_{init}(x_{N-1}) \\\\\n\\end{pmatrix}.\n\\] This system can now be solved using any of the IVP solvers with a temporal stepsize \\(h_t\\).\n\n\n\n\n\n\n\n\nHeat Equation\n\n\n\nConsider an iron rod (of thermal diffusivity \\(\\alpha=2.3\\times 10^{-5}\\)) of length 1 where the middle section of length 0.2 has been heated to a temperature of 1 while the rest is at 0. The ends of the rod have been kept at a constant temperature of 2. This system can be represented by the IBVP \\[\\frac{\\partial u}{\\partial t}=\\alpha\\frac{\\partial^{2} u}{\\partial t^{2}}, \\quad x \\in [0,1], \\quad t&gt;0\\] \\[u(x,0)=u_{init}(x)=\\left\\{\n\\begin{matrix}\n        0 & 0 \\leq x&lt;0.4 \\\\\n        1 & 0.4 \\leq x &lt;0.6 \\\\\n        0 & 0.6 \\leq x \\leq 1 \\\\\n\\end{matrix} \\right. , \\quad u(0,t)=u_l(t)=2, \\quad u(L,t)=u_r(t)=2.\\]\nFirst, divide the interval \\([0,1]\\) into five equal sections (which will be of width \\(h_x=\\frac{1-0}{5}=0.2\\)). \nThis system can be discretised using the centred difference method and written in matrix form as \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\[\n    \\frac{\\mathrm{d} }{\\mathrm{d} t}\\underbrace{\\begin{pmatrix}\n        U_1(t) \\\\\n        U_2(t) \\\\\n        U_3(t) \\\\\n        U_4(t) \\\\\n        U_5(t)\n    \\end{pmatrix}}_{\\boldsymbol{U}}=\n    \\underbrace{\\frac{\\alpha}{h^2}\\begin{pmatrix}\n        -2 & 1 & 0 & 0 & 0 \\\\\n        1 & -2 & 1 & 0 & 0 \\\\\n        0 & 1 & -2 & 1 & 0 \\\\\n        0 & 0 & 1 & -2 & 1 \\\\\n        0 & 0 & 0 & 1 & -2\n    \\end{pmatrix}}_{A}\n    \\underbrace{\\begin{pmatrix}\n        U_1(t) \\\\\n        U_2(t) \\\\\n        U_3(t) \\\\\n        U_4(t) \\\\\n        U_5(t) \\\\\n    \\end{pmatrix}}_{\\boldsymbol{U}}+\n    \\underbrace{\\frac{\\alpha}{h^2}\\begin{pmatrix}\n        u_l(t) \\\\\n        0 \\\\\n        0 \\\\\n        0 \\\\\n        u_r(t) \\\\\n    \\end{pmatrix}}_{\\boldsymbol{b}}\n\\] The differential equation \\[\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\] can be solved using the Euler method with the initial condition \\[\n    \\boldsymbol{U}(0)=\\begin{pmatrix}\n        u_{init}(x_1) \\\\\n        u_{init}(x_2) \\\\\n        u_{init}(x_3) \\\\\n        u_{init}(x_4) \\\\\n        u_{init}(x_5) \\\\\n    \\end{pmatrix}\n\\] subject to a time stepsize \\(h_t\\). Below are the plots of the heat distribution at \\(t=0, 100, 1000\\) for \\(N_x=500\\) (\\(h_x=0.002\\)) and \\(h_t=0.02\\) (\\(N_t=50000\\)).    At the beginning, the temperature at the ends is 2 and the middle section is at a temperature of 1. As time progresses, the heat evens out across the iron bar until eventually, the whole bar will be the same temperature.",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  },
  {
    "objectID": "15_PDE.html#linear-advection-equation",
    "href": "15_PDE.html#linear-advection-equation",
    "title": "11  Heat Equation",
    "section": "11.2 Linear Advection Equation",
    "text": "11.2 Linear Advection Equation\nThe heat equation deals with heat transfer through diffusion throughout a material. Another way in which heat transfer can be achieved by advection (or convection) and this is given by \\[\\frac{\\partial u}{\\partial t}=-v \\frac{\\partial u}{\\partial x} \\quad \\text{with} \\quad 0&lt;x&lt;L \\quad \\text{and} \\quad t&gt;0\\] where \\(u=u(x,t)\\) is the temperature at location \\(x\\) at time \\(t\\) and \\(v\\) is the flow speed.\nThis partial differential equation has two derivatives in total, one in \\(x\\) and one in \\(t\\), this means that two conditions are needed, one spatial and one temporal:\n\n\\(u(x,0)=u_{init}(x)\\) for \\(x \\in [0,L]\\): Initial heat distribution across the rod;\n\\(u(0,t)=u_l(t)\\) for \\(t&gt;0\\): The temperature at the left end of the rod.\n\nConsider the PDE along with the initial condition only, namely \\(u(x,0)=u_{init}(x)\\) for \\(x \\in [0,L]\\). The exact solution to this differential equation is given by \\[u(x,t)=u_{init}(x-vt),\\] this can be verified from the partial differential equation as follows: \\[\\frac{\\partial u}{\\partial t}=-v \\frac{\\partial u}{\\partial x} \\quad \\text{at} \\quad u(x,t)=u_{init}(x-vt)\\] \\[\\begin{align*}\n    &\\text{LHS}=\\frac{\\partial }{\\partial t}u(x,t)=\\frac{\\partial }{\\partial t}\\left( u_{init}(x-vt) \\right)=-vu_{init}'(x-vt)\\\\\n    &\\text{RHS}=\\frac{\\partial }{\\partial x}u(x,t)=\\frac{\\partial }{\\partial x}\\left( u_{init}(x-vt) \\right)=-vu_{init}'(x-vt).\n\\end{align*}\\] This means that if the initial heat profile takes the form of \\(u_{init}(x)\\), then after time \\(t\\), the profile will look exactly the same but shifted to the right by a distance \\(vt\\).\n\nThe “information” moves from left to right so if the finite differences are to be used, the centred differencing approach would not be suitable since the information on the right is not known yet. Therefore the backwards differencing approximation will be the most suitable. This is known as an upwind/upstream scheme (i.e. against the direction of the wind/stream) if \\(v&gt;0\\). Therefore using the convention \\(U_n(t) \\approx u(x_n,t)\\) where \\(x=x_n\\) is the discretisation of the spatial points for \\(n=0,1,2,\\dots,N\\), the backward differencing approximation to the spatial derivative is \\[\\frac{\\partial u}{\\partial x}(x_n,t) \\approx \\frac{\\partial U_n}{\\partial x}=\\frac{U_n-U_{n-1}}{h_x}.\\] Therefore is discretised advection equation is \\[\\frac{\\mathrm{d} U_n}{\\mathrm{d} t}= \\frac{v}{h_x}\\left( U_{n-1}-U_n \\right) \\quad \\text{for} \\quad n=1,2,\\dots,N\\] and this can be solved subject to the initial condition \\[u(x,0)=u_{init}(x)\\] and boundary condition \\[u(0,t)=u_{l}(t)\\] to give the discretised set of equations in the form \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\[\\begin{multline*}\n     \\frac{\\mathrm{d} }{\\mathrm{d} t}\\underbrace{\\begin{pmatrix}\n        U_1(t) \\\\\n        U_2(t) \\\\\n        U_3(t) \\\\\n        \\vdots \\\\\n        U_{N-2}(t) \\\\\n        U_{N-1}(t) \\\\\n        U_N(t)\n    \\end{pmatrix}}_{\\boldsymbol{U}}=\n    \\underbrace{\\frac{v}{h_x}\\begin{pmatrix}\n        -1 & 0 & 0 & \\dots & 0 & 0 & 0 \\\\\n        1 & -1 & 0 & \\dots & 0 & 0 & 0 \\\\\n        0 & 1 & -1 & \\dots & 0 & 0 & 0 \\\\\n        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n        0 & 0 & 0 & \\dots & -1 & 0 & 0 \\\\\n        0 & 0 & 0 & \\dots & 1 & -1 & 0 \\\\\n        0 & 0 & 0 & \\dots & 0 & 1 & -1 \\\\\n    \\end{pmatrix}}_{A}\n    \\underbrace{\\begin{pmatrix}\n        U_1(t) \\\\\n        U_2(t) \\\\\n        U_3(t) \\\\\n        \\vdots \\\\\n        U_{N-2}(t) \\\\\n        U_{N-1}(t) \\\\\n        U_N(t)\n    \\end{pmatrix}}_{\\boldsymbol{U}}\\\\+\n    \\underbrace{\\frac{v}{h_x}\\begin{pmatrix}\n        u_l(t) \\\\\n        0 \\\\\n        0 \\\\\n        \\vdots \\\\\n        0 \\\\\n        0 \\\\\n        0 \\\\\n    \\end{pmatrix}}_{\\boldsymbol{b}}\n\\end{multline*}\\] and the initial condition is \\[\n\\boldsymbol{U}_0=\\begin{pmatrix}\n    U_1(0) \\\\ U_2(0) \\\\ U_3(0) \\\\ \\vdots \\\\ U_{N-3}(0) \\\\ U_{N-2}(0) \\\\ U_{N-1}(0)\n\\end{pmatrix}\\approx\\begin{pmatrix}\n    u(x_1,0) \\\\ u(x_2,0) \\\\ u(x_3,0) \\\\ \\vdots \\\\ u(x_{N-3},0) \\\\ u(x_{N-2},0) \\\\ u(x_{N-1},0)\n\\end{pmatrix} =\\begin{pmatrix}\n    u_{init}(x_1) \\\\\n    u_{init}(x_2) \\\\\n    u_{init}(x_3) \\\\\n    \\vdots \\\\\n    u_{init}(x_{N-3}) \\\\\n    u_{init}(x_{N-2}) \\\\\n    u_{init}(x_{N-1}) \\\\\n\\end{pmatrix}\n\\].",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  },
  {
    "objectID": "15_PDE.html#convection-diffusion-equation",
    "href": "15_PDE.html#convection-diffusion-equation",
    "title": "11  Heat Equation",
    "section": "11.3 Convection-Diffusion Equation",
    "text": "11.3 Convection-Diffusion Equation\nThe heat (or diffusion) equation dictates the spread of heat across a length of material while on the other hand, the advection (or convection) equation dictates the flow of heat in a certain direction. The combination of these two effects gives rise to the Convection-Diffusion Equation which takes the form \\[\\frac{\\partial u}{\\partial t}=\\alpha \\frac{\\partial^{2} u}{\\partial x^{2}}-v \\frac{\\partial u}{\\partial x} \\quad \\text{with} \\quad 0&lt;x&lt;L, \\quad t&gt;0.\\]\nJust as in the heat equation, this partial differential equation has three derivatives in total, two derivatives in \\(x\\) and one derivative in \\(t\\), this means that three conditions are needed, two on \\(x\\) and one on \\(t\\), these will be as follows:\n\n\\(u(x,0)=u_{init}(x)\\) for \\(x \\in [0,L]\\): Initial heat distribution across the rod;\n\\(u(0,t)=u_l(t)\\) for \\(t&gt;0\\): The temperature at the left end of the rod;\n\\(u(L,t)=u_r(t)\\) for \\(t&gt;0\\): The temperature at the right end of the rod.\n\nIn order to discretise this system, a finite difference approximation needs to be chosen first. The centred difference approximation was used for the heat equation and the backwards difference approximation for the advection. Here, the combination of both will be used. Even though this might initially seem like an inconsistency, but in fact, this will allow the system to present a distinct stable advantage as will be seen in the next section.\nThis system can be discretised in exactly the same way as before \\[\\frac{\\mathrm{d} U_n}{\\mathrm{d} t}(t)= \\frac{\\alpha}{h_x^2}\\left[ U_{n-1}(t)-2U_n(t)+U_{n+1}(t) \\right]-\\frac{v}{h_x}\\left[U_n(t)+U_{n-1}(t) \\right] \\quad \\text{for} \\quad n=1,2,\\dots,N-1.\\] This system can be written in the form \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\[\n        A=\\frac{\\alpha}{h_x^2}\\begin{pmatrix}\n            -2 & 1 & 0 & \\dots & 0 & 0 & 0 \\\\\n            1 & -2 & 1 & \\dots & 0 & 0 & 0 \\\\\n            0 & 1 & -2 & \\dots & 0 & 0 & 0 \\\\\n            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n            0 & 0 & 0 & \\dots & -2 & 1 & 0 \\\\\n            0 & 0 & 0 & \\dots & 1 & -2 & 1 \\\\\n            0 & 0 & 0 & \\dots & 0 & 1 & -2 \\\\\n        \\end{pmatrix}+\\frac{v}{h_x}\\begin{pmatrix}\n            -1 & 0 & 0 & \\dots & 0 & 0 & 0 \\\\\n            1 & -1 & 0 & \\dots & 0 & 0 & 0 \\\\\n            0 & 1 & -1 & \\dots & 0 & 0 & 0 \\\\\n            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots \\\\\n            0 & 0 & 0 & \\dots & -1 & 0 & 0 \\\\\n            0 & 0 & 0 & \\dots & 1 & -1 & 0 \\\\\n            0 & 0 & 0 & \\dots & 0 & 1 & -1 \\\\\n        \\end{pmatrix}\n\\] \\[\n        \\boldsymbol{U}\\begin{pmatrix}\n            U_1(t) \\\\\n            U_2(t) \\\\\n            \\vdots \\\\\n            U_{N-2}(t) \\\\\n            U_{N-1}(t) \\\\\n        \\end{pmatrix}, \\quad \\boldsymbol{b}=\\frac{\\alpha}{h_x^2}\\begin{pmatrix}\n            u_l(t) \\\\\n            0 \\\\\n            \\vdots \\\\\n            0 \\\\\n            u_r(t) \\\\\n        \\end{pmatrix}+\\frac{v}{h_x}\\begin{pmatrix}\n            u_l(t) \\\\\n            0 \\\\\n            \\vdots \\\\\n            0 \\\\\n            0 \\\\\n        \\end{pmatrix}.\n\\] and this system can be solved using an Euler iteration subject to the initial condition \\(u(x,0)=u_{init}(x)\\).",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  },
  {
    "objectID": "15_PDE.html#asymptotic-stability",
    "href": "15_PDE.html#asymptotic-stability",
    "title": "11  Heat Equation",
    "section": "11.4 Asymptotic Stability",
    "text": "11.4 Asymptotic Stability\nThe method of lines is essentially a hybrid method that makes use of a combination between a finite difference approximation and the Euler method and is very effective at solving partial differential equations, as seen from solving the heat, advection and convection-diffusion equations. The derivation of the method of lines for the different methods builds on the very same principle and the codes can be adapted quite easily. One main issue that arises here is the choice for the stepsizes for both the spatial and temporal discretisations, i.e. the choice of \\(h_t\\) and \\(h_x\\) respectively. When both methods are combined, there needs to be a restriction on both stepsizes.\nThe first issue that needs to be addressed is the asymptotic stability of the heat equation and the advection equation. For arbitrarily large matrices, it may not be simple to determine if all the eigenvalues are negative since it may be computationally restrictive to do so. However, a result can be used to see if all the eigenvalues are negative without explicitly calculating them.\n\nTheorem 11.1 (Gershgorin Circle Theorem) Let \\(A\\) be an \\(N \\times N\\) given by \\[\nA=\\begin{pmatrix}\n    a_{11} & a_{12} & a_{13} & \\dots  & a_{1N} \\\\\n    a_{21} & a_{22} & a_{23} & \\dots  & a_{2N} \\\\\n    a_{31} & a_{32} & a_{33} & \\dots  & a_{3N} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    a_{N1} & a_{N2} & a_{N3} & \\dots  & a_{NN}\n\\end{pmatrix}.\n\\] On the complex plane, consider \\(N\\) closed discs, each centred at the locations \\(a_{ii}\\) for \\(i=1,2,\\dots,n\\) (the diagonal terms) where the disc centred at \\(a_{ii}\\) has a radius \\(R_i\\) where \\[R_i=\\sum_{j \\neq i}{|a_{ij}|}.\\] Then all the eigenvalues of the matrix \\(A\\) will have to lie in at least one of these discs. In other words, every eigenvalues of \\(A\\) satisfies \\[|\\lambda-a_{ii}| \\leq R_i \\quad \\text{for at least one } \\quad i=1,2,\\dots,n.\\]\n\n\n\n\n\n\n\nGershgorin Circle Theorem Exapmple\n\n\n\nConsider the matrix \\[\nA=\\begin{pmatrix}\n    -1 & 3 & 4 & 2 & -4 \\\\\n    0 & 5 & 4 & 7 & 1 \\\\\n    4 & -2 & 0 & -3 & 0 \\\\\n    6 & -6 & -4 & -6 & -1 \\\\\n    7 & 4 & 7 & 9 & 7\n\\end{pmatrix}.\n\\] Following the steps of the theorem:\n\nIndicate the locations of the diagonal terms (namely \\(-1, 5, 0, -6, 7\\)) on the complex plane.\nFind the radii \\(R_i\\) which are equal to the row sum of the absolute terms without the diagonal terms, in other words, \\[\n\\text{abs}(A)=\\begin{pmatrix}\n1 & 3 & 4 & 2 & 4 \\\\\n0 & 5 & 4 & 7 & 1 \\\\\n4 & 2 & 0 & 3 & 0 \\\\\n6 & 6 & 4 & 6 & 1 \\\\\n7 & 4 & 7 & 9 & 7\n\\end{pmatrix}\n\\begin{matrix}\n\\rightarrow \\\\ \\rightarrow \\\\ \\rightarrow \\\\ \\rightarrow \\\\ \\rightarrow\n\\end{matrix}\n\\begin{matrix}\n3+4+2+4 \\\\ 0+4+7+1 \\\\ 4+2+3+0 \\\\ 6+6+4+1 \\\\ 7+4+7+9\n\\end{matrix}\n\\begin{matrix}\n=13 \\\\ =12 \\\\ =9 \\\\ =17 \\\\ =27\n\\end{matrix}\n\\begin{matrix}\n\\rightarrow R_1 \\\\ \\rightarrow R_2 \\\\ \\rightarrow R_3 \\\\ \\rightarrow R_4 \\\\ \\rightarrow R_5\n\\end{matrix}\n\\]\nDraw a circle around \\(a_{11}=-1\\) with radius \\(R_1=13\\), a circle around \\(a_{22}=5\\) with radius \\(R_2=12\\) and so on.\nAll the eigenvalues of the matrix \\(A\\) must lie in at least one of the circles indicated. Indeed, the following figure shows the diagonal terms each with circles around them with the appropriate radius. The eigenvalues are given in red and the blue circles are those which contain all said eigenvalues. \n\n\n\n\n11.4.1 Stability of the Euler Method for the Advection Equation\nConsider the matrix \\(A_2\\) of size \\(N \\times N\\) from the advection equation \\[\nA_2=\\begin{pmatrix}\n    -1 & 0 & \\dots & 0 & 0 \\\\\n    1 & -1 & \\dots & 0 & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    0 & 0 & \\dots & -1 & 0 \\\\\n    0 & 0 & \\dots & 1 & -1 \\\\\n\\end{pmatrix}.\n\\] Following the steps of the Gershgorin theorem, the centres of all the circles on the complex plane will be located at the diagonal terms, all of which are \\(-1\\). The radii of these circles are the row sums of the matrix \\(A_2\\) without the diagonal terms, which means that all the radii will be 1. The figure below shows the circle that results on the complex plane. Therefore regardless of what the eigenvalues might be, it is known that they will always have negative real parts and therefore the advection matrix forms an asymptotically stable system. \nSince the advection equation is asymptotically stable, a bound for the temporal stepsize needs to be found. Consider the advection equation after the discretisation \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\(A=\\frac{v}{h_x}A_2\\). The Euler method is numerically stable if the time step \\(h_t\\) satisfies \\[|| \\mathcal{I}+h_t A ||_{\\infty} \\leq 1.\\] First calculate \\(\\mathcal{I}+h_t A\\): \\[\n\\mathcal{I}+h_t A=\\mathcal{I}+\\frac{vh_t}{h_x}A_2=\n\\begin{pmatrix}\n    1-\\tilde{v} & 0 & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\tilde{v} & 1-\\tilde{v} & 0 & \\dots & 0 & 0 & 0 \\\\\n    0 & \\tilde{v} & 1-\\tilde{v} & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 0&  \\tilde{v} & 1-\\tilde{v}\n\\end{pmatrix}.\n\\] where \\(\\tilde{v}=\\frac{vh_t}{h_x}\\). Now taking the absolute value of all the terms and taking the row sums gives: \\[\n\\mathrm{abs}\\left( \\mathcal{I}+\\frac{vh_t}{h_x}A_2 \\right)=\n\\begin{pmatrix}\n    |1-\\tilde{v}| & 0 & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\tilde{v} & |1-\\tilde{v}| & 0 & \\dots & 0 & 0 & 0 \\\\\n    0 & \\tilde{v} & |1-\\tilde{v}| & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 0&  \\tilde{v} & |1-\\tilde{v}|\n\\end{pmatrix}\\begin{matrix}\n\\to \\\\ \\to \\\\ \\to \\\\ \\vdots \\\\ \\to\n\\end{matrix}\\begin{matrix}\n|1-\\tilde{v}| \\\\ \\tilde{v}+|1-\\tilde{v}| \\\\ \\tilde{v}+|1-\\tilde{v}| \\\\ \\vdots \\\\ \\tilde{v}+|1-\\tilde{v}|\n\\end{matrix}.\n\\] The row sums of the absolute terms of this matrix are \\[a=|1-\\tilde{v}| \\quad \\text{and} \\quad b=|1-\\tilde{v}|+\\tilde{v}.\\] Since it is assumed that \\(v&gt;0\\), then \\(b&gt;a\\) therefore, \\(|| \\mathcal{I}+h_tA ||_{\\infty}=b=|1-\\tilde{v}|+\\tilde{v}\\). Consider the two cases when \\(1-\\tilde{v}&gt;0\\) and \\(1-\\tilde{v}&lt;0.\\)$ 1. If \\(1-\\tilde{v}&gt;0\\), then \\(0&lt;\\tilde{v}&lt;1\\): \\[|| \\mathcal{I}+h_tA ||_{\\infty}=|1-\\tilde{v}|+\\tilde{v}=1-\\tilde{v}+\\tilde{v}=1.\\] Therefore if \\(1-\\tilde{v}&gt;0\\), then \\(|| \\mathcal{I}+h_tA ||_{\\infty}\\leq 1\\).\n\nIf \\(1-\\tilde{v}&lt;0\\), then \\(\\tilde{v}&gt;1\\): \\[|| \\mathcal{I}+h_tA ||_{\\infty}=|1-\\tilde{v}|+\\tilde{v}=\\tilde{v}-1+\\tilde{v}=2\\tilde{v}-1,\\] therefore in this case, if \\(|| \\mathcal{I}+h_tA ||_{\\infty}\\) needs to be less than or equal to \\(1\\), then \\[|| \\mathcal{I}+h_tA ||_{\\infty}\\leq 1 \\quad \\implies \\quad 2\\tilde{v}-1\\leq 1 \\quad \\implies \\quad\\tilde{v} \\leq 1\\] which contradicts with the assumption that \\(\\tilde{v}&gt;1\\).\n\nTherefore, the Euler method will produce a convergent solution if \\[\\tilde{v} &lt; 1 \\quad \\implies \\quad v\\frac{h_t}{h_x}&lt;1.\\] In terms of number of spatial and temporal points \\(N_x\\) and \\(N_t\\) respectively, this restriction would be \\[v \\frac{t_f-t_0}{L-x_0} \\frac{N_x}{N_t} &lt; 1\\] So for a fixed velocity \\(v\\), if the time step \\(h_t\\) is to be halved, then the spatial step would also need to be halved as well.\n\n\n11.4.2 Stability of the Euler Method for the Heat Equation\nConsider the matrix \\(A_1\\) of size \\(N \\times N\\) from the heat equation \\[\nA_1=\\begin{pmatrix}\n    -2 & 1 & \\dots & 0 & 0 \\\\\n    1 & -2 & \\dots & 0 & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    0 & 0 & \\dots & -2 & 1 \\\\\n    0 & 0 & \\dots & 1 & -2 \\\\\n\\end{pmatrix}.\n\\] The steps of the Gershgorin theorem can be followed to produce the following figure on the complex plane.  Once again, this shows that all the eigenvalues will have negative real parts even though their explicit values are not known.\nTo determine the bound on the stepsize, consider the heat equation after the discretisation, which is \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\(A=\\frac{\\alpha}{h_x^2}A_1\\). The Euler method is numerically stable if the time step \\(h_t\\) satisfies \\[|| \\mathcal{I}+h_t A ||_{\\infty} \\leq 1.\\] First calculate \\(\\mathcal{I}+h_t A\\): \\[\n\\mathcal{I}+h_t A=\\mathcal{I}+\\frac{\\alpha h_t}{h_x^2}A_1=\n\\begin{pmatrix}\n    1-2\\tilde{\\alpha} & \\tilde{\\alpha} & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\tilde{\\alpha} & 1-2\\tilde{\\alpha} & \\tilde{\\alpha} & \\dots & 0 & 0 & 0 \\\\\n    0 & \\tilde{\\alpha} & 1-2\\tilde{\\alpha} & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 0&  \\tilde{\\alpha} & 1-2\\tilde{\\alpha}\n\\end{pmatrix}\n\\] where \\(\\tilde{\\alpha}=\\frac{\\alpha h_t}{h_x^2}\\). Now taking the absolute value of all the terms and taking the row sums gives: \\[\n\\mathrm{abs}\\left( \\mathcal{I}+\\frac{\\alpha h_t}{h_x^2}A_1 \\right)=\n\\begin{pmatrix}\n    |1-2\\tilde{\\alpha}| & \\tilde{\\alpha} & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\tilde{\\alpha} & |1-2\\tilde{\\alpha}| & \\tilde{\\alpha} & \\dots & 0 & 0 & 0 \\\\\n    0 & \\tilde{\\alpha} & |1-2\\tilde{\\alpha}| & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 0&  \\tilde{\\alpha} & |1-2\\tilde{\\alpha}|\n\\end{pmatrix}\\begin{matrix}\n    \\to \\; \\; \\tilde{\\alpha}+|1-2\\tilde{\\alpha}| \\\\ \\to 2\\tilde{\\alpha}+|1-2\\tilde{\\alpha}|\\\\ \\to 2\\tilde{\\alpha}+|1-2\\tilde{\\alpha}|\\\\ \\vdots \\\\ \\to \\; \\; \\tilde{\\alpha}+|1-2\\tilde{\\alpha}|.\n\\end{matrix}\n\\] The row sums of the absolute terms of this matrix are \\[a=\\tilde{\\alpha}+|1-2\\tilde{\\alpha}| \\quad \\text{and} \\quad b=2\\tilde{\\alpha}+|1-2\\tilde{\\alpha}|.\\] Since \\(tilde{\\alpha}&gt;0\\), then \\(b&gt;a\\) and therefore, \\(|| \\mathcal{I}+h_tA ||_{\\infty}=b=2\\tilde{\\alpha}+|1-2\\tilde{\\alpha}|\\). Consider the two cases \\(1-2\\tilde{\\alpha}&gt;0\\) and \\(1-2\\tilde{\\alpha}&lt;0\\).\n\nIf \\(1-2\\tilde{\\alpha}&gt;0\\), then \\(0&lt;\\tilde{\\alpha}&lt;\\frac{1}{2}\\): \\[|| \\mathcal{I}+h_tA ||_{\\infty}=|1-2\\tilde{\\alpha}|+2\\tilde{\\alpha}=1-2\\tilde{\\alpha}+2\\tilde{\\alpha}=1,\\] therefore \\(|| \\mathcal{I}+h_tA ||_{\\infty} \\leq 1\\).\nIf \\(1-2\\tilde{\\alpha}&lt;0\\), then \\(\\tilde{\\alpha}&gt;\\frac{1}{2}\\): \\[|| \\mathcal{I}+h_tA ||_{\\infty}=|1-2\\tilde{\\alpha}|+2\\tilde{\\alpha}=2\\tilde{\\alpha}-1+2\\tilde{\\alpha}=4\\tilde{\\alpha}-1,\\] therefore in this case, if \\(|| \\mathcal{I}+h_tA ||_{\\infty}\\) needs to be less than or equal to \\(1\\), then \\[|| \\mathcal{I}+h_tA ||_{\\infty}\\leq 1 \\quad \\implies \\quad 4\\tilde{\\alpha}-1\\leq 1 \\quad \\implies \\quad\\tilde{\\alpha} \\leq \\frac{1}{2}\\] which contradicts with the assumption that \\(\\tilde{\\alpha}&gt;\\frac{1}{2}\\).\n\nThis means that the Euler method produces a stable convergent solution if \\[\\tilde{\\alpha} &lt; \\frac{1}{2} \\quad \\implies \\quad \\alpha\\frac{h_t}{h_x^2}&lt;\\frac{1}{2}.\\] In terms of number of spatial and temporal points \\(N_x\\) and \\(N_t\\) respectively, this restriction would be \\[2 \\alpha \\frac{t_f-t_0}{(L-x_0)^2} \\frac{N_x^2}{N_t} &lt; 1\\] So for a fixed diffusivity \\(\\alpha\\), if the time step \\(h_t\\) is to be halved, then the spatial step would should be quartered.",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  },
  {
    "objectID": "15_PDE.html#stability-of-the-convection-diffusion-equation",
    "href": "15_PDE.html#stability-of-the-convection-diffusion-equation",
    "title": "11  Heat Equation",
    "section": "11.5 Stability of the Convection-Diffusion Equation",
    "text": "11.5 Stability of the Convection-Diffusion Equation\nNow that it has been established that both the heat and advection equations are asymptotically stable and the stepsize bounds have been found, it is time to combine both cases to tackle the convection-diffusion equation.\nWhen discretised, the convection-diffusion equation can be written as \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where the matrix \\(A\\) is given by \\[\nA=\\frac{\\alpha}{h_x^2}\\begin{pmatrix}\n    -2 & 1 & \\dots & 0 & 0 \\\\\n    1 & -2 & \\dots & 0 & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    0 & 0 & \\dots & -2 & 1 \\\\\n    0 & 0 & \\dots & 1 & -2 \\\\\n\\end{pmatrix}+\\frac{v}{h_x}\\begin{pmatrix}\n    -1 & 0 & \\dots & 0 & 0 \\\\\n    1 & -1 & \\dots & 0 & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    0 & 0 & \\dots & -1 & 0 \\\\\n    0 & 0 & \\dots & 1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nThe Gershgorin theorem can be applied to the matrix \\(A\\) to show that all the eigenvectors have negative real parts. Indeed, \\[\nA=\n\\begin{pmatrix}\n    -2\\hat{\\alpha}-\\hat{v} & \\hat{\\alpha} & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\hat{\\alpha}+\\hat{v} & -2\\hat{\\alpha}-\\hat{v} & \\hat{\\alpha} & \\dots & 0 & 0 & 0 \\\\\n    0 & \\hat{\\alpha}+\\hat{v} & -2\\hat{\\alpha}-\\hat{v} & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 0&  \\hat{\\alpha}+\\hat{v} & -2\\hat{\\alpha}-\\hat{v}\n\\end{pmatrix}.\n\\] where \\(\\hat{\\alpha}=\\frac{\\alpha}{h_x^2}\\) and \\(\\hat{v}=\\frac{v}{h_x}\\). By the Gershgorin theorem, the centres of the circles will be located at the diagonal terms, namely at \\(-2\\hat{\\alpha}-\\hat{v}\\) with the radii \\(\\hat{\\alpha}\\), \\(\\hat{\\alpha}+\\hat{v}\\) and \\(2\\hat{\\alpha}+\\hat{v}\\). The largest radius is \\(2\\hat{\\alpha}+\\hat{v}\\) which means that all the eigenvalues will be negative as shown below. Therefore the convection-diffusion equation is asymptotically stable.\n\nTo find the bound for the stepsizes, consider the convection-diffusion equation after the discretisation \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}+\\boldsymbol{b}\\) where \\(A=\\frac{\\alpha}{h_x^2}A_1+\\frac{v}{h_x}A_2\\). The Euler method is numerically stable if the time step \\(h_t\\) satisfies \\[|| \\mathcal{I}+h_t A ||_{\\infty} \\leq 1.\\] Calculating \\(\\mathcal{I}+h_t A\\): \\[\n\\mathcal{I}+h_t A=\n\\begin{pmatrix}\n    1-2\\tilde{\\alpha}-\\tilde{v} & \\tilde{\\alpha} & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\tilde{\\alpha}+\\tilde{v} & 1-2\\tilde{\\alpha}-\\tilde{v} & \\tilde{\\alpha} & \\dots & 0 & 0 & 0 \\\\\n    0 & \\tilde{\\alpha}+\\tilde{v} & 1-2\\tilde{\\alpha}-\\tilde{v} & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 1-2\\tilde{\\alpha}-\\tilde{v} & \\tilde{\\alpha} & 0\\\\\n    0 & 0 & 0 & \\dots & \\tilde{\\alpha}+\\tilde{v} & 1-2\\tilde{\\alpha}-\\tilde{v} & \\tilde{\\alpha} \\\\\n    0 & 0 & 0 & \\dots & 0&  \\tilde{\\alpha}+\\tilde{v} & 1-2\\tilde{\\alpha}-\\tilde{v}\n\\end{pmatrix}\n\\] where \\(\\tilde{\\alpha}=\\frac{\\alpha h_t}{h_x^2}\\) and \\(\\tilde{v}=\\frac{v h_t}{h_x}\\). Taking the absolute value of all the terms and adding the rows gives \\[\\begin{multline*}\n\\mathrm{abs}\\left( \\mathcal{I}+h_t A \\right)=\n\\begin{pmatrix}\n    |1-2\\tilde{\\alpha}-\\tilde{v}| & \\tilde{\\alpha} & 0 & \\dots & 0 & 0 & 0 \\\\\n    \\tilde{\\alpha}+\\tilde{v} & |1-2\\tilde{\\alpha}-\\tilde{v}| & \\tilde{\\alpha} & \\dots & 0 & 0 & 0 \\\\\n    0 & \\tilde{\\alpha}+\\tilde{v} & |1-2\\tilde{\\alpha}-\\tilde{v}| & \\dots & 0 & 0 & 0 \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\vdots\\\\\n    0 & 0 & 0 & \\dots & 0&  \\tilde{\\alpha}+\\tilde{v} & |1-2\\tilde{\\alpha}-\\tilde{v}|\n\\end{pmatrix}\\\\\\begin{matrix}\n\\to \\; \\; \\; \\; \\; \\; \\; \\; \\tilde{\\alpha} +|1-2\\tilde{\\alpha}-\\tilde{v}| \\\\ \\to 2\\tilde{\\alpha}+\\tilde{v}+|1-2\\tilde{\\alpha}-\\tilde{v}|\\\\ \\to 2\\tilde{\\alpha}+\\tilde{v}+|1-2\\tilde{\\alpha}-\\tilde{v}|\\\\ \\vdots \\\\ \\to \\; \\; \\tilde{\\alpha}+\\tilde{v}+|1-2\\tilde{\\alpha}-\\tilde{v}|\n\\end{matrix}.\n\\end{multline*}\\]\nThe row sums of the absolute terms of this matrix are \\[a=\\tilde{\\alpha}+|1-2\\tilde{\\alpha}-\\tilde{v}|, \\quad b=2\\tilde{\\alpha}+\\tilde{v}+|1-2\\tilde{\\alpha}-\\tilde{v}| \\quad \\text{and} \\quad c=\\tilde{\\alpha}+\\tilde{v}+|1-2\\tilde{\\alpha}-\\tilde{v}|.\\] Since \\(\\tilde{\\alpha}&gt;0\\) and \\(\\tilde{v}&gt;0\\), then \\(b&gt;c&gt;a\\), therefore, \\(|| \\mathcal{I}+h_tA ||_{\\infty}=b=2\\tilde{\\alpha}+\\tilde{v}+|1-2\\tilde{\\alpha}-\\tilde{v}|\\). Consider the two cases \\(1-2\\tilde{\\alpha}-\\tilde{v}&gt;0\\) and \\(1-2\\tilde{\\alpha}-\\tilde{v}&lt;0\\).\n\nIf \\(1-2\\tilde{\\alpha}-\\tilde{v}&gt;0\\), then \\(2\\tilde{\\alpha}+\\tilde{v}&lt;1\\): \\[|| \\mathcal{I}+h_tA ||_{\\infty}=|1-2\\tilde{\\alpha}-\\tilde{v}|+2\\tilde{\\alpha}+\\tilde{v}=1-2\\tilde{\\alpha}-\\tilde{v}+2\\tilde{\\alpha}+\\tilde{v}=1,\\] therefore \\(|| \\mathcal{I}+h_tA ||_{\\infty} \\leq 1\\).\nIf \\(1-2\\tilde{\\alpha}-\\tilde{v}&lt;0\\), then \\(2\\tilde{\\alpha}+\\tilde{v}&gt;1\\): \\[|| \\mathcal{I}+h_tA ||_{\\infty}=|1-2\\tilde{\\alpha}-\\tilde{v}|+2\\tilde{\\alpha}+\\tilde{v}=2\\tilde{\\alpha}+\\tilde{v}-1+2\\tilde{\\alpha}+\\tilde{v}=4\\tilde{\\alpha}+2\\tilde{v}-1,\\] therefore in this case, if \\(|| \\mathcal{I}+h_tA ||_{\\infty}\\) needs to be less than or equal to \\(1\\), then \\[|| \\mathcal{I}+h_tA ||_{\\infty}\\leq 1 \\quad \\implies \\quad 4\\tilde{\\alpha}+2\\tilde{v}-1\\leq 1 \\quad \\implies \\quad 2\\tilde{\\alpha}+\\tilde{v} \\leq 1\\] which contradicts with the assumption that \\(2\\tilde{\\alpha}+\\tilde{v}&gt;1\\).\n\nThis means that the Euler method will produce a stable convergent solution if \\[2\\tilde{\\alpha}+\\tilde{v} &lt; 1 \\quad \\implies \\quad 2\\alpha\\frac{h_t}{h_x^2}+v\\frac{h_t}{h_x}&lt;1.\\] This means that a choice can be made with regards to the bounds of the different components, for instance, the values of \\(h_x\\) and \\(h_t\\) can be chosen such that \\[\\tilde{\\alpha}&lt;\\frac{1}{4} \\quad \\text{and} \\quad \\tilde{v}&lt;\\frac{1}{2} \\quad \\text{or} \\quad \\tilde{\\alpha}&lt;\\frac{1}{3} \\quad \\text{and} \\quad \\tilde{v}&lt;\\frac{1}{3}\\] or any combination thereof provided that the choices satisfy the inequality \\(2\\tilde{\\alpha}+\\tilde{v} &lt; 1\\).\n\n\n\n\n\n\nBound for Convection-Diffusion\n\n\n\nConsider the convection-diffusion equation \\[\n\\frac{\\partial u}{\\partial t}=0.1\\frac{\\partial^{2} u}{\\partial x^{2}}-0.5\\frac{\\partial u}{\\partial x} \\quad\n\\begin{matrix}\n    t \\in [0,10] \\\\ x \\in [-2,2]\n\\end{matrix}\n\\] \\[u(x,0)=u_{init}(x)=10, \\quad u(-2,t)=u_l(t)=1, \\quad u(2,t)=u_r(t)=0.\\] This can be discretised to give \\(\\frac{\\mathrm{d} \\boldsymbol{U}}{\\mathrm{d} t}=A\\boldsymbol{U}\\) where \\[\n\\frac{\\mathrm{d} }{\\mathrm{d} t}\\underbrace{\\begin{pmatrix}\n    U_1(t) \\\\\n    U_2(t) \\\\\n    \\vdots \\\\\n    U_{N-1}(t) \\\\\n    U_N(t)\n\\end{pmatrix}}_{\\boldsymbol{U}}\\] \\[=\n\\underbrace{\\left[\\frac{0.1}{h_x^2}\\begin{pmatrix}\n    -2 & 1 & \\dots & 0 & 0 \\\\\n    1 & -2 & \\dots & 0 & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    0 & 0 & \\dots & -2 & 1 \\\\\n    0 & 0 & \\dots & 1 & -2 \\\\\n\\end{pmatrix}+\\frac{0.5}{h_x}\\begin{pmatrix}\n    -1 & 0 & \\dots & 0 & 0 \\\\\n    1 & -1 & \\dots & 0 & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n    0 & 0 & \\dots & -1 & 0 \\\\\n    0 & 0 & \\dots & 1 & -1 \\\\\n\\end{pmatrix}\\right]}_{A}\n\\underbrace{\\begin{pmatrix}\n    U_1(t) \\\\\n    U_2(t) \\\\\n    \\vdots \\\\\n    U_{N-1}(t) \\\\\n    U_N(t)\n\\end{pmatrix}}_{\\boldsymbol{U}}\n\\] subject to \\[\n    \\boldsymbol{U}(0)=\\begin{pmatrix}\n        u_{init}(x_1) \\\\\n        u_{init}(x_2) \\\\\n        \\vdots \\\\\n        u_{init}(x_{N-1}) \\\\\n        u_{init}(x_{N}) \\\\\n    \\end{pmatrix} \\quad \\text{where} \\quad u_{init}(x)=10.\n\\] As yet, the value of \\(N\\) has not been put forward since the stepsizes need to be established first. For a stable Euler method, the stepsizes \\(h_t\\) and \\(h_x\\) need to satisfy \\[2\\alpha\\frac{h_t}{h_x^2}+v\\frac{h_t}{h_x} &lt; 1 \\quad \\implies \\quad 2\\frac{h_t}{h_x}+5\\frac{h_t}{h_x^2}&lt;10.\\] If \\(h_t=2.5\\times 10^{-5}\\) and \\(h_x=0.02\\) (which corresponds to \\(N_t=40000\\) and \\(N_x=100\\)), then the Euler method will be stable.",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  },
  {
    "objectID": "15_PDE.html#footnotes",
    "href": "15_PDE.html#footnotes",
    "title": "11  Heat Equation",
    "section": "",
    "text": "The thermal diffusivity will always be regarded as a constant and usually takes the form \\(\\alpha=\\frac{k}{\\rho Cp}\\) where \\(k\\) is the thermal conductivity, \\(\\rho\\) is the density of the material and \\(Cp\\) is the specific heat capacity.↩︎",
    "crumbs": [
      "Solving Partial Differential Equations",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Heat Equation</span>"
    ]
  }
]